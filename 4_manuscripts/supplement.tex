
\begin{appendix}

    \renewcommand{\thefigure}{A\arabic{figure}}
    \setcounter{figure}{0}
    
    \renewcommand{\thetable}{A\arabic{table}}
    \setcounter{table}{0}
    
    \renewcommand{\theequation}{A\arabic{equation}}
    \setcounter{equation}{0}

    \singlespacing
%    \appendixwithtoc
    \newpage

    \section{Proofs of main identifiability results}
    For convenience, we restate the core identifiability assumptions from the main text. 
    \vspace{1em}
    
    \noindent Assumptions:
    \begin{itemize}
        \item[(A1)] Consistency of potential outcomes. For all individuals $i$ and for $v \in \{0, 1\}$, we have $I_i^v = I_i$ and $T_i^v = T_i$ when $V_i = v$.
        \item[(A2)] No effect of vaccination on test-negative symptomatic illness ($I = 1$) among the vaccinated. That is, $\Pr(I^0 = 1 | V = 1, X) = \Pr(I^1 = 1 | V = 1, X).$
        \item[(A3)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for test-positive and test-negative illnesses, i.e. 
        $$\beta_2(X) = \beta_1(X), $$
        $$ \text{where } \beta_i(X) = \log \frac{\Pr(I^0 = i, T^0 = 1 | V = 1, X)\Pr(I^0 = 0, T^0 = 1 | V = 0, X)}{\Pr(I^0 = 0, T^0 = 1 | V = 1, X)\Pr(I^0 = i, T^0 = 1| V = 0, X)}.$$
        \item[(A4)] Overlap of vaccination among test-positives and test-negatives. Define $\mathcal{S}_i(v)$ as the support of the law of $(I^v = i, T^v = 1, V = v, X)$, then for $v$ in $\{0,1\}$, then it must be that $\mathcal{S}_2(1) \subseteq \mathcal{S}_2(0)$ and $\mathcal{S}_2(v) \subseteq \mathcal{S}_1(v).$
        \item[(A5)] No direct effect of vaccination on test-seeking behavior among the vaccinated. That is, for $i$ in $\{1,2\}$, $\Pr[T^1 = 1 | I^1 = i, V = 1, X] = \Pr[T^0 = 1 | I^0 = i, V = 1, X].$
        % \item[(A5)] Equivalent effects of vaccination on test-seeking behavior for test-positive and test-negative illnesses. That is, for $v \in \{0, 1\}$, $\Pr(T^v = 1 | I^v = 2, V = 1, X) = \Pr(T^v = 1 | I^v = 1, V = 1, X)$ and $\Pr(T^v = 1 | I^v = 2, V = 0, X) = \Pr(T^v = 1 | I^v = 1, V = 0, X)$.
    \end{itemize}

    In Proposition \ref{prop1}, we establish that, under consistency alone, the causal risk ratio among the vaccinated, $\Psi_{RRV}$, is equivalent to two expressions involving the (unobserved) treatment-free potential outcome $I^0 = 2$, one based on the outcome itself and one based on the so-called generalized propensity score. In Theorem \ref{theorem1}, we show that, if we add assumptions A2-A4, we can use the observed incidence of the test-negative illness $I = 1$ as a proxy for expressions involved the treatment-free potential outcome, $I^0 = 2$, and therefore identify $\Psi_{RRV}$ if we had access to all symptomatic infections in the underlying cohort. Then in Corollary \ref{corollary1} we show that adding assumption A5 allows us to identify $\Psi_{RRV}$ in the underlying cohort if we only observe infections among those who seek tests. Finally, in Corollary \ref{corollary2} we show $\Psi_{RRV}$ is still identifiable under the sampling design of the TND, due to the invariance of the odds ratio. 
    
    \newpage
    \begin{proposition}\label{prop1}
    Under assumption A1, the causal risk ratio among the vaccinated, 
    \begin{equation*}
        \Psi \equiv \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)},
    \end{equation*}
    is equivalent to 
    \begin{equation}
        \Psi^0_{om} \equiv \dfrac{\Pr(I = 2 | V = 1)}{E\left[\Pr(I = 2 | V = 0, X) \exp\{\alpha^0_2(X)\}\Big| V = 1 \right]}
    \end{equation}
    and 
    \begin{equation}
        \Psi^0_{ipw} \equiv \dfrac{E\{V \mathbbm 1(I = 2)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2) \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)}\right\}}
    \end{equation}
    where 
    \begin{equation*}
        \pi^0_i(X) = \Pr(V = 1 | I^0 = i, X)
    \end{equation*}
    is the generalized propensity score and 
    \begin{equation*}
        \beta^0_i(X) = \log \dfrac{\Pr(I^0 = i | V = 1, X)\Pr(I^0 = 0 | V = 0, X)}{\Pr(I^0 = 0 | V = 1, X)\Pr(I^0 = i | V = 0, X)}
    \end{equation*}
    is the odds ratio comparing the treatment-free potential outcome in the vaccinated and unvaccinated groups with 
    \begin{equation*}
        \eta^0(X) = \log \dfrac{\Pr(I^0 = 0 | V = 0, X)}{\Pr(I^0 = 0 | V = 1, X)}
    \end{equation*}
    and 
    \begin{equation*}
        \alpha^0_i(X) = \log \dfrac{\Pr(I^0 = i | V = 1, X)}{\Pr(I^0 = i | V = 0, X)}
    \end{equation*}
    such that
    \begin{equation*}
        \beta^0_i(X) = \eta^0(X) + \alpha^0_i(X).
    \end{equation*}
    \end{proposition}
    
    \begin{proof}
    For the first expression, we have 
    \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)} \\
        &= \dfrac{\Pr(I^1=2|V=1)}{E[E\{\mathbbm 1 (I^0 = 2) | V = 1, X\} | V = 1]} \\
        &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\int \mathbbm 1 (I^0 = 2) \cdot f(I^0 = i | V = 1, X) di \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\int \mathbbm 1 (I^0 = 2) \cdot f(I^0 = i | V = 0, X) \dfrac{f(I^0 = i | V = 1, X)}{f(I^0 = i | V = 0, X)}di \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{ \Pr(I^0 = 2 | V = 0, X) \dfrac{\Pr(I^0 = 2 | V = 1, X)}{\Pr(I^0 = 2 | V = 0, X)} \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I=2|V=1)}{E\left[ \Pr(I = 2 | V = 0, X) \exp\{\alpha^0_2(X)\} \mid  V = 1\right]}.
    \end{align*}
    The first line restates the definition. The second uses the law of iterated expectation. The third applies the definition of conditional expectation. The fourth multiplies the density by one. The fifth evaluates the integral and the sixth applies consistency and the definition of $\alpha_i(X)$
    
    For the second, we have 
    \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)} \\
        &= \dfrac{E\left\{\dfrac{V}{\Pr(V = 1)} \mathbbm 1 (I^1 = 2)\right\}}{E\left\{\dfrac{V}{\Pr(V=1)}\mathbbm 1 (I^0 = 2)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2)\}}{E\left\{\mathbbm 1 (I^0 = 2) E(V | I^0 = 2, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2)\}}{E\left\{\mathbbm 1 (I^0 = 2) \pi^0_2(X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2)\}}{E\left\{\mathbbm 1 (I^0 = 2) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}E(1-V|I^0 = 2, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}}.
    \end{align*}
    The first line restates the definition. The second uses the definition of conditional expectation. The third applies the law of iterated expectation. The fourth applies the definition of the generalized propensity score $\pi_i(X)$. The fifth multiplies by one. The sixth reverses the law of iterated expectations and applies consistency. 
    \end{proof}
    \newpage
    
    \begin{theorem}\label{theorem1}
    Under assumptions A1 - A4, the causal risk ratio among the vaccinated, 
    \begin{equation*}
        \Psi \equiv \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)},
    \end{equation*}
    is identified in the population underlying the test-negative design by 
    \begin{equation}
        \Psi_{om} \equiv \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha_1(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}}
    \end{equation}
    and 
    \begin{equation}
        \Psi_{ipw} \equiv \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2) \dfrac{\pi_1(X)}{1 - \pi_1(X)}\right\}}
    \end{equation}
    where $\alpha_i(X) = \log \dfrac{\Pr(I =i | V = 1, X)}{\Pr(I =i | V = 0, X)}$ and $\pi_i(X) = \Pr(V = 1 | I = i, X)$ are both observables. 
    \end{theorem}
    
    \begin{proof}
    For the first expression, note that by Assumption A3 and exclusivity of test-negative and test-positive illnesses
    \begin{align*}
        \dfrac{\Pr(I^0 = 2 | V = 1, X)}{\Pr(I^0 = 2 | V = 0, X)} &= \dfrac{\Pr(I^0 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
        &= \dfrac{\Pr(I^1 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
        &= \dfrac{\Pr(I = 1 | V = 1, X)}{\Pr(I = 1 | V = 0, X)} \\
    \end{align*}
    where the first line is by Assumption A3 and exclusivity of test-negative and test-positive illnesses. The second line is by Assumption A2. And the last line applies consistency. This implies
    \begin{equation*}
        \alpha_2^0(X) = \alpha_1(X)
    \end{equation*}
    and, therefore, combining with the derivation of Proposition \ref{prop1}, we have
    \begin{align*}
        \Psi &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha^0_2(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}} \\
        &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha_1(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}}.
    \end{align*}
    
    For the second expression, note first that, by the invariance of odds ratios, Assumptions A2 and A3 imply
    \begin{equation*}
        \dfrac{\Pr(V = 1 | I^0 = 2, X)}{\Pr(V = 0 | I^0 = 2, X)} = \dfrac{\Pr(V = 1 | I = 1, X)}{\Pr(V = 0 | I = 1, X)}
    \end{equation*}
    and by consequence 
    \begin{equation*}
        \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)} = \dfrac{\pi_1(X)}{1 - \pi_1(X)}.
    \end{equation*}
    Thus, again, continuing the derivation in Theorem 1, we have 
    \begin{align*}
        \Psi &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi_1(X)}{1-\pi_1(X)}\right\}}.
    \end{align*}
    \end{proof}
    \newpage
    
    \begin{corollary}\label{corollary1}
    If we add assumption A5, that is that direct effects of vaccination on testing behavior are the same for test-positive and test-negative illnesses, then $\Psi_{om}$ and $\Psi_{ipw}$ are equivalent to
    \begin{equation}
        \Psi^\dagger_{om} \equiv \dfrac{\Pr(I = 2, T = 1 | V = 1)}{E\left\{ \exp\{\alpha^\dagger_1(X)\} \Pr(I = 2, T = 1 | V = 0, X) \Big| V = 1 \right\}}
    \end{equation}
    and 
    \begin{equation}
        \Psi^\dagger_{ipw} \equiv \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2, T = 1) \dfrac{\pi^\dagger_1(X)}{1 - \pi^\dagger_1(X)}\right\}}
    \end{equation}
    where $\alpha^\dagger_1(X) = \dfrac{\Pr(I = 1, T = 1 | V = 1, X)}{\Pr(I = 1, T = 1 | V = 0, X)}$ and $\pi^\dagger_1(X) = \Pr(V = 1| I = 1, T = 1, X)$.
    \end{corollary}
    
    \begin{proof}
        Define $\Psi^\dagger$ as 
        \begin{equation*}
            \Psi^\dagger \equiv  \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{\Pr(I^0=2, T^0 = 1|V=1)},
        \end{equation*}
        and note that
        \begin{align*}
            \Psi^\dagger &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{\Pr(I^0=2, T^0 = 1|V=1)} \\
            &= \dfrac{\Pr(I^1 = 2 | V = 1)\Pr(T^1 = 1 | I^1 = 2, V = 1)}{\Pr(I^0=2|V=1)\Pr(T^0 = 1 | I^0 = 2, V = 1)} \\
            & = \dfrac{\Pr(I^1 = 2 | V = 1)}{\Pr(I^0 = 2 | V = 1)} \\
            &= \Psi.
        \end{align*}
        The first line follows by definition, the second factors the joint probability, and the third applies applies assumption A3. 
    
        As $\Psi^\dagger = \Psi$, it now suffices to show that $\Psi^\dagger_{om}$ and $\Psi^\dagger_{ipw}$ identify $\Psi^\dagger$. Following the same steps as the derivations in Theorems 1 and 2, for the first expression we have 
        \begin{align*}
        \Psi^\dagger &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{\Pr(I^0=2, T^0 = 1|V=1)} \\
        &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{E[E\{\mathbbm 1 (I^0 = 2, T^0 = 1) | V = 1, X\} | V = 1]} \\
        &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{E\left\{\int \mathbbm 1 (I^0= 2, T^0 = 1) \cdot f(I^0 = i, T^0 = 1 | V = 1, X) di \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{E\left\{\int \mathbbm 1 (I^0= 2, T^0 = 1) \cdot f(I^0 = i, T^0 = 1 | V = 0, X) \dfrac{f(I^0 = i, T^0 = 1 | V = 1, X)}{f(I^0 = i, T^0 = 1 | V = 0, X)}di \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{E\left\{ \Pr(I^0 = 2, T^0 = 1 | V = 0, X) \dfrac{\Pr(I^0 = 2, T^0 = 1 | V = 1, X)}{\Pr(I^0 = 2, T^0 = 1 | V = 0, X)} \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{E\left\{ \Pr(I^0 = 2, T^0 = 1 | V = 0, X) \dfrac{\Pr(I^0 = 1, T^0 = 1 | V = 1, X)}{\Pr(I^0 = 1, T^0 = 1 | V = 0, X)} \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{E\left\{ \Pr(I^0 = 2, T^0 = 1 | V = 0, X) \dfrac{\Pr(I^1 = 1, T^1 = 1 | V = 1, X)}{\Pr(I^0 = 1, T^0 = 1 | V = 0, X)} \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I=2, T = 1|V=1)}{E\left\{ \Pr(I = 2, T = 1 | V = 0, X) \dfrac{\Pr(I = 1, T = 1 | V = 1, X)}{\Pr(I = 1, T = 1 | V = 0, X)} \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I = 2, T = 1 | V = 1)}{E\left\{ \exp\{\alpha^\dagger_1(X)\} \Pr(I = 2, T = 1 | V = 0, X) \Big| V = 1 \right\}}.
        \end{align*}
    
    For the second expression, note first that, by the invariance of odds ratios, Assumptions A2, A3, and A5 imply
    \begin{equation*}
        \dfrac{\Pr(V = 1 | I^0 = 2, T^0 = 1, X)}{\Pr(V = 0 | I^0 = 2,  T^0 = 1, X)} = \dfrac{\Pr(V = 1 | I = 1, T = 1, X)}{\Pr(V = 0 | I = 1,  T = 1, X)}
    \end{equation*}
    and by consequence 
    \begin{equation*}
        \dfrac{\pi^{0\dagger}_2(X)}{1 - \pi^{0\dagger}_2(X)} = \dfrac{\pi^\dagger_1(X)}{1 - \pi^\dagger_1(X)}.
    \end{equation*}
    Thus we have
    \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2, T^1 = 1|V=1)}{\Pr(I^0=2, T^0 = 1|V=1)} \\
        &= \dfrac{E\{\dfrac{V}{\Pr(V=1)} \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\dfrac{V}{\Pr(V=1)}\mathbbm 1 (I^0 = 2, T^0 = 1)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) E(V | I^0 = 2, T^0 = 1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) \pi^{0\dagger}_2(X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) \dfrac{\pi^{0\dagger}_2(X)}{1-\pi^{0\dagger}_2(X)}E(1-V |I^0 = 2, T^0 = 1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T = 1) \dfrac{\pi^{0\dagger}_2(X)}{1-\pi^{0\dagger}_2(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T = 1) \dfrac{\pi^\dagger_1(X)}{1-\pi^\dagger_1(X)}\right\}}.
    \end{align*}
    \end{proof}
    \newpage 
    \begin{corollary}\label{corollary2}
    Under assumptions A1 - A5 and the biased sampling design of the test-negative study, $\Psi^\dagger_{om}$ and $\Psi^\dagger_{ipw}$, and therefore by previous results $\Psi_{om}$, $\Psi_{ipw}$, and $\Psi$, are identified by
    \begin{equation}
        \Psi_{om}^* = \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \exp\{\alpha^*_1(X)\} \Pr(I^* = 1 | S = 1, V = 0, X) \Big| S = 1, V = 1 \right\}}
    \end{equation}
    and 
    \begin{equation}
        \Psi_{ipw}^* = \dfrac{E\{VI^*|S =1\}}{E\left\{ (1 - V) I^* \dfrac{\pi^*(X)}{1 - \pi^*(X)} \bigg| S = 1\right\}}
    \end{equation}
    where $\alpha^*_1(X) = \dfrac{\Pr(I^* = 0 | S = 1, V = 1, X)}{\Pr(I^* = 0| S = 1, V = 0, X)}$ and $\pi^*(X) = \Pr(V = 1| S = 1, I^* = 0, X)$.
    \end{corollary}
    
    \begin{proof}
    For the first expression we have,
        \begin{align*}
        \Psi^\dagger_{om} &= \dfrac{\Pr(I=2, T = 1|V=1)}{E\left\{ \dfrac{\Pr(I = 1, T = 1 | V = 1, X)}{\Pr(I = 1, T = 1 | V = 0, X)}\Pr(I = 2, T = 1 | V = 0, X) \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I = 2, T = 1 | V = 1)}{\int \dfrac{\Pr(I = 1, T = 1 | V = 1, x)}{\Pr(I = 1, T = 1 | V = 0, x)} \Pr(I = 2, T = 1 | V = 0, x) f(x | V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)\Pr(T = 1 | V = 1)}{\int \dfrac{\Pr(I = 1 | T = 1, V = 1, x)}{\Pr(I = 1 | T = 1, V = 0, x)} \Pr(I = 2 | T = 1, V = 0, x) \Pr(T = 1 | V = 1, x) f(x | V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)\Pr(T = 1 | V = 1)}{\int \dfrac{\Pr(I = 1 | T = 1, V = 1, x)}{\Pr(I = 1 | T = 1, V = 0, x)} \Pr(I = 2 | T = 1, V = 0, x) \Pr(T = 1 | V = 1) f(x | T =1, V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{\int \dfrac{\Pr(I = 1 | T = 1, V = 1, x)}{\Pr(I = 1 | T = 1, V = 0, x)} \Pr(I = 2 | T = 1, V = 0, x) f(x | T =1, V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{E\left\{ \dfrac{\Pr(I = 1 | T = 1, V = 1, X)}{\Pr(I = 1 | T = 1, V = 0, X)} \Pr(I = 2 | T = 1, V = 0, X) \bigg| T = 1, V = 1\right\}} \\
        &= \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I^* = 0 | S = 1, V = 1, X)}{\Pr(I^* = 0 | S = 1, V = 0, X)} \Pr(I^* = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} \\
        &= \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \exp\{\alpha^*_1(X)\} \Pr(I^* = 1 | S = 1, V = 0, X) \Big| S = 1, V = 1 \right\}}.
    \end{align*}
    The first line restates the definition of $\Psi^\dagger_{om}$. The second applies the definition of conditional expectation. The third factors the joint probabilities. The fourth applies Bayes theorem, i.e. 
    \begin{equation*}
        f(x | V = 1) = \dfrac{f(x | T = 1, V = 1)\Pr(T = 1 | V = 1)}{\Pr(T = 1 | V = 1, x)}.
    \end{equation*}
    The fifth cancels the $\Pr(T = 1 |V = 1)$ terms in the numerator and denominator. The sixth applies the definition of conditional expectation. The seventh applies the sampling design $S = \mathbbm 1 (T = 1, I \neq 0)$ and the perfect test $I^* = \mathbbm 1(I = 2)$.
    
    For the second expression we have,
    \begin{align*}
        \Psi^\dagger_{ipw} &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2, T = 1) \dfrac{\pi^\dagger_1(X)}{1 - \pi^\dagger_1(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2) | T = 1\} \Pr(T = 1)}{E\left\{ (1 - V) \mathbbm 1(I = 2) \dfrac{\pi^\dagger_1(X)}{1 - \pi^\dagger_1(X)} \bigg| T = 1\right\} \Pr(T = 1)} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2) | T = 1\}}{E\left\{ (1 - V) \mathbbm 1(I = 2) \dfrac{\pi^\dagger_1(X)}{1 - \pi^\dagger_1(X)} \bigg| T = 1\right\}} \\
        &= \dfrac{E\{VI^*|S =1\}}{E\left\{ (1 - V) I^* \dfrac{\pi^*(X)}{1 - \pi^*(X)} \bigg| S = 1\right\}}.
    \end{align*}
    The first line restates the definition of $\Psi^\dagger_{ipw}$. The second applies the law of iterated expectations. The third cancels the $\Pr(T = 1)$ terms in the numerator and denominator. The fourth applies the sampling design $S = \mathbbm 1 (T = 1, I \neq 0)$ and the perfect test $I^* = \mathbbm 1(I = 2)$.
    
    \end{proof}
    \newpage
    \section{Estimation}
    \subsection{Plug-in}
    Corollary \ref{corollary2} suggests two plug-in estimators for the causal risk ratio among the vaccinated.  An estimator based on modeling the outcome
    \begin{equation}
        \widehat{\Psi}_{om}^* = \dfrac{\sum_{i=1}^n V_i I^*_i}{\sum_{i=1}^n V_i \mu_0(X_i)\dfrac{1 - \mu^*_1(X_i)}{1 - \mu^*_0(X_i)}},
    \end{equation}
    and an inverse probability weighting estimator
    \begin{equation}
        \widehat{\Psi}_{ipw}^* = \dfrac{\sum_{i=1}^n V_i I^*_i}{\sum_{i=1}^n (1 - V_i) I^*_i \dfrac{\pi^*(X_i)}{1 - \pi^*(X_i)}},
    \end{equation}
    where 
    \begin{align*}
        \pi^*(X) &= \Pr(V=1\mid S=1, I^*=0, X) \\
        \mu^*_v(X) &= \Pr(I^*=1\mid S=1, V=v, X).
    \end{align*}

\subsection{Doubly robust}
We consider a 
\begin{equation}\label{eqn:dr_estimator}
\widehat{\Psi}_{dr}^* = \dfrac{\sum_{i=1}^n V_i I^*_i}{\sum_{i=1}^n (1 - V_i)\dfrac{\pi^*_1(X_i)}{1 - \pi^*_1(X_i)} \{I_i - \mu^*_0(X_i) \} + V_i \mu^*_0(X_i)\dfrac{1 - \mu^*_1(X_i)}{1 - \mu^*_0(X_i)}}
\end{equation}

To prove the doubly-robustness of $\widehat{\Psi}^*_{dr}$, it suffices to show that 
\begin{align*}
    E\bigg[ (1 - V)\dfrac{\pi^*_1(X)}{1 - \pi^*_1(X)} \{I - \mu^*_0(X) \} + V \mu^*_0(X)\dfrac{1 - \mu^*_1(X)}{1 - \mu^*_0(X)}\mid S=1\bigg] = \Pi
\end{align*}
if (1) $\dot\mu^*_v(\cdot)=\mu^*_v(\cdot)$ a.s. for $v=0,1$, or (2) $\dot \pi^*(\cdot)=\pi^*(\cdot)$ and $\dot\mu_0(\cdot)=\mu_0(\cdot)$  a.s.

    \begin{enumerate}
        \item If $\dot\mu_v(\cdot)=\mu_v(\cdot)$ a.s. for $v=0,1$, then
        \begin{align*}
            &E\bigg[ (1-V)\{I^* -  \mu_0(X)\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
            =& E\bigg[ \{I^* -  \mu_0(X)\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} \mid V=0, S=1\bigg]Pr(V=0\mid  S=1) + \\
            &\qquad E\bigg[E\{V(1-I^*)\mid X\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid  S=1\bigg]\\
            =& E\bigg[E \{I^* -  \mu_0(X)\mid V=0, S=1, X\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} \mid V=0, S=1\bigg]Pr(V=0\mid  S=1) + \\
            &\qquad E\bigg[\pi(X)\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid  S=1\bigg]\\
            &= 0 + \Pi = \Pi
        \end{align*}
    \item If $\dot\pi(\cdot)=\pi(\cdot)$ and $\dot\mu_0(\cdot)=\mu_0(\cdot)$ a.s., then 
\begin{align*}
    &E\bigg[ (1-V)\{I^* -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    =& E\bigg[ \{1 - \pi(X)\}\{\mu_0(X) -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \pi(X)\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    &= \Pi
\end{align*}


    \end{enumerate}
    \subsection{Efficient influence function}
    Let $\Psi(P)$ be the target parameter under true law of the observed data $P$. From the identifiability results in Section 1, we have that 
% $$\Psi(P) = \dfrac{E(I^*\mid S=1, V=1)}{E\left\{\dfrac{\Pr(I^*=1\mid S=1, V=0, X)\Pr(V=1\mid S=1, I^*=0, X)}{\Pr(V=0\mid S=1, I^*=0, X)}\mid S=1, V=1\right\}}.$$ 
$$\Psi(P) = E\left\{\dfrac{\Pr(I^*=1\mid S=1, V=0, X)\Pr(I^*=0\mid S=1, V=1, X)}{\Pr(I^*=0\mid S=1, V=0, X)}\mid S=1, V=1\right\}.$$ 
For convenience, let
\begin{align*}
    \pi(X) &= \Pr(V=1\mid S=1, I^*=0, X) \\
    \mu^*_v(X) &= \Pr(I^*=1\mid S=1, V=v, X).
\end{align*}
Define $P_t$ as a parametric submodel indexed by $t \in [0,1]$ such that
$$P_t = t \widetilde{P} + (1 - t)P$$
where $\widetilde{P}$ is smoothed parametric estimate of $P$ and note that $P_0 = P$. To find the influence function we will use the fact that if we perturb the target in direction of a point mass $\widetilde{o} = (\widetilde{i}^*, \widetilde{s}, \widetilde{v}, \widetilde{x})$ of $\widetilde{P}$
$$ \chi(P, \widetilde{o}) = \frac{d}{dt} \Psi(P_t)\bigg\vert_{t=0}$$
where the right-hand side is the so-called the G\^{a}teaux derivative. Note that 
\begin{align*}
    \Psi(P_t) &= \int \dfrac{\int i^* f_t(i^* | s=1,v=0,x)di^* \int (1-i^*) f_t(i^* | s=1, v=1, x)di^*}{\int(1 - i^*) f_t(i^* | s=1, v=0, x)di^*}f_t(x|s=1,v=1)dx \\
    &= \int \dfrac{1}{f_{S,V}(1, 1)} \dfrac{\int i^* f_t(i^*, s=1,v=0,x)di^* \int (1 - i^*)  f_t(i^*, s=1, v=1, x)dv}{\int(1 - i^*)  f_t(i^*, s=1, v=0, x)dv}dx 
\end{align*}
\begin{align*}
    \Psi(P_t) &= \int (1 - v)i^*\dfrac{\int v f_t(v | s=1, i^*=0, x)dv}{\int (1-v) f_t(v | s=1, i^*=0, x)dv} f(i^*, s=1, v, x) di^* dv dx \\
    &= \int (1 - v)i^*\dfrac{\int v f_t(v, s=1, i^*=0, x)dv}{\int (1-v) f_t(v, s=1, i^*=0, x)dv} f(i^*, s=1, v, x) di^* dv dx
\end{align*}

Then the pathwise derivative of $\Psi_t$ wrt $t$ is
\begin{align*}
   &\frac{d}{dt} \Psi(P_t)\bigg\vert_{t=0} \\
   &= \int \dfrac{1}{f_{S,V}(1, 1)} \frac{d}{dt} \left\{\dfrac{\int i^* f_t(i^*, s=1,v=0,x)di^* \int (1 - i^*)  f_t(i^*, s=1, v=1, x)dv}{\int(1 - i^*)  f_t(i^*, s=1, v=0, x)dv} \right\} \bigg\vert_{t=0}dx \\
   &= \int \dfrac{1}{f_{S,V}(1, 1)}\bigg\{ \dfrac{1 - \mu^*_1(x)}{1-\mu^*_0(x)} \dfrac{f(x, s=1, v=1)}{f(x, s=1, v=0)}\int i^* \frac{d}{dt} f_t(i^*, s=1,v=0,x) \bigg\vert_{t=0} di^* \\
   &\qquad + \dfrac{\mu^*_0(x)}{1 - \mu^*_0(x)} \int (1-i^*) \frac{d}{dt}f_t(i^*, s=1, v=1, x) \bigg\vert_{t=0} di^* \\ 
   &\qquad - \dfrac{\mu^*_0(x)\{1 - \mu^*_1(x)\}}{\{1 - \mu^*_0(x)\}^2} \dfrac{f(x, s=1, v=1)}{f(x, s=1, v =0)} \int (1-i^*) \frac{d}{dt}f_t(i^*, s=1, v=0, x) \bigg\vert_{t=0} dv \\
   % &\qquad + \dfrac{\mu^*_0(x)\pi(x)}{1-\pi(x)} \frac{d}{dt} f_t(x, s=1, v=1) \bigg\vert_{t=0} \\
   % &\qquad - \dfrac{\mu^*_0(x)\pi(x)}{1-\pi(x)} \dfrac{f(x, s=1, v=1)}{f(x, s=1, v=0)} \frac{d}{dt} f_t(x, s=1, v=0) \bigg\vert_{t=0} \bigg\}dx \\
   &= \int \dfrac{1}{f_{S,V}(1, 1)}\bigg\{  \dfrac{1 - \mu^*_1(x)}{1-\mu^*_0(x)} \dfrac{f(x, s=1, v=1)}{f(x, s=1, v=0)}\int i^* \{ \mathbbm 1_{\widetilde i^*, \widetilde s, \widetilde v, \widetilde x}(i^*, 1, 0, x) - f(i^*, s=1,v=0,x) \} di^* \\
   &\qquad + \dfrac{\mu^*_0(x)}{1-\mu^*_0(x)} \int (1 - i^*) \{\mathbbm 1_{\widetilde i^*, \widetilde s, \widetilde v, \widetilde x}(i^*, 1, 1, x) - f(i^*, s=1, v=1, x) \} di^* \\ 
   &\qquad -\dfrac{\mu^*_0(x)\{1 - \mu^*_1(x)\}}{\{1 - \mu^*_0(x)\}^2} \dfrac{f(x, s=1, v=1)}{f(x, s=1, v =0)} \int (1 - i^*) \{\mathbbm 1_{\widetilde i^*, \widetilde s, \widetilde v, \widetilde x}(i^*, 1, 0, x) - f(i^*, s=1, v=0, x) \} di^*  \\
   % & \qquad + \dfrac{\mu^*_0(x)\pi(x)}{1-\pi(x)} \left\{\mathbbm 1_{\widetilde s, \widetilde v, \widetilde x}(1,1,x) - f_t(x, s=1, v=1) \right\} \\
   % & \qquad - \dfrac{\mu^*_0(x)\pi(x)}{1-\pi(x)} \dfrac{f(x, s=1, v=1)}{f(x, s=1, v=0)} \left\{\mathbbm 1_{\widetilde s, \widetilde v, \widetilde x}(1,0,x) - f_t(x, s=1, v=0) \right\}  \bigg\}dx\\
    %&= \int \dfrac{1}{f_{S,V}(1, 1)}\bigg\{ \dfrac{\pi(\widetilde x)}{1-\pi(\widetilde x)} \widetilde i^* \widetilde s(1 - \widetilde v) - \Psi(P) + \dfrac{\mu^*_0(\widetilde x)}{1-\pi(\widetilde x)}(1-\widetilde i^*) \widetilde s \widetilde v  - \Psi(P) - \dfrac{\mu^*_0(x)\pi(x)}{\{1 - \pi(x)\}^2} (1-\widetilde i^*) \widetilde s (1-\widetilde v) - \Psi(P) +  \bigg\}dx \\ 
   &= \int \dfrac{1}{f_{S,V}(1, 1)} \dfrac{\mu^*_0(x)\{1 - \mu^*_1(x)\}}{1 - \mu^*_0(x)}f(x, s=1, v=1) \bigg\{\dfrac{\widetilde{i}^*\widetilde{s}(1-\widetilde{v})\mathbbm 1_{\widetilde{x}}(x)}{\mu^*_0(x)} + \dfrac{(1-\widetilde{i}^*)\widetilde{s}\widetilde{v}\mathbbm 1_{\widetilde{x}}(x)}{1 - \mu^*_1(x)} \\
   &\qquad - \dfrac{(1-\widetilde{i}^*)\widetilde{s}(1-\widetilde{v})\mathbbm 1_{\widetilde{x}}(x)}{\{1 - \mu^*_0(x)\}}\bigg\} dx - \Psi(P) \\
   &=\dfrac{\widetilde s}{f_{S,V}(1, 1)}  \bigg\{(1 - \widetilde v)\widetilde i^*\dfrac{1 - \mu^*_1(\widetilde x)}{1 - \mu^*_0(\widetilde x)} + \widetilde v (1 - \widetilde i^*) \dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)} -  (1 - \widetilde v) (1 - \widetilde i^*)\dfrac{\mu^*_0(\widetilde x)\{1 - \mu^*_1(\widetilde x)\}}{\{1 - \mu^*_0(\widetilde x)\}^2}  \bigg\}  - \Psi(P) \\
   &=\dfrac{\widetilde s}{f_{S,V}(1, 1)}  \bigg\{(1 - \widetilde v)\widetilde i^*\dfrac{1 - \mu^*_1(\widetilde x)}{1 - \mu^*_0(\widetilde x)} + \widetilde v \left\{\dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)} - \widetilde i^* \dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)}\right\} \\
   &\qquad -  (1 - \widetilde v) \left[\dfrac{\mu^*_0(\widetilde x)\{1 - \mu^*_1(\widetilde x)\}}{\{1 - \mu^*_0(\widetilde x)\}^2} - \widetilde i^*\dfrac{\mu^*_0(\widetilde x)\{1 - \mu^*_1(\widetilde x)\}}{\{1 - \mu^*_0(\widetilde x)\}^2}\right]  \bigg\}  - \Psi(P) \\
   &=\dfrac{\widetilde s}{f_{S,V}(1, 1)}  \bigg\{(1 - \widetilde v) \left[\widetilde i^* - \dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)}  + \widetilde i^*\dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)}\right]\dfrac{1 - \mu^*_1(\widetilde x)}{1 - \mu^*_0(\widetilde x)}\\
   &\qquad   + \widetilde v \left[\dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)} - \widetilde i^* \dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)}\right] \bigg\}  - \Psi(P) \\
   &=\dfrac{\widetilde s}{f_{S,V}(1, 1)}  \bigg\{(1 - \widetilde v) \left[ \dfrac{\widetilde i^* - \widetilde i^* \mu^*_0(\widetilde x) - \mu^*_0(\widetilde x) + \widetilde i^* \mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)}\right]\dfrac{1 - \mu^*_1(\widetilde x)}{1 - \mu^*_0(\widetilde x)} \\
   &\qquad + \widetilde v \left[\dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)} - \widetilde i^* \dfrac{\mu^*_0(\widetilde x)}{1 - \mu^*_0(\widetilde x)}\right]   \bigg\}  - \Psi(P) \\
   &=\dfrac{\widetilde s}{f_{S,V}(1, 1)}  \bigg\{(1 - \widetilde v) \{\widetilde i^* - \mu^*_0(\widetilde x)\}\dfrac{1 - \mu^*_1(\widetilde x)}{1 - \mu^*_0(\widetilde x)} + \widetilde v \mu^*_0(\widetilde x) \dfrac{1 - \mu^*_1(\widetilde x)}{1 - \mu^*_0(\widetilde x)}  \bigg\}  - \Psi(P) 
\end{align*}
    \newpage
    
    \section{Example data generation mechanisms satisfying equi-confounding}

    \newpage
    
    \section{Additional results}
    \subsection{Example mechanisms where key assumptions are violated}
    
    \begin{figure}[p]
    \centering
    \begin{subfigure}{0.8\linewidth}
        \centering
        \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
            \tikzstyle{every state}=[
              draw = none,
              fill = none
            ]
            \node[state] (x) {$X$};
            \node[state] (v) [right of=x] {$V$};
            \node[state] (i) [right of=v] {$I_2$};
            \node[state] (t) [right of=i] {$T$};
            \node[state] (is) [right of=t] {$I^*$};
            \node[state] (i1) [below of=i] {$I_1$};
            \node[state] (u) [below of=x] {$U$};
            \node[state] (v1) [below of=v] {$V_1$};
   
            \path[->] (x) edge node {} (v);
            \path[->] (x) edge [out=45, in=135] node {} (i);
    
            \path[->] (v) edge node {} (i);
            
            \path[->] (i) edge node {} (t);
            \path[->] (i1) edge node {} (t);
            \path[->] (i1) edge node {} (is);

            \path[->] (x) edge [out=45, in=135] node {} (t);
    
            \path[->] (t) edge node {} (is);
            \path[->] (v1) edge node {} (i1);

            \path[->] (i) edge [out=45, in=135] node {} (is);
            \path[->] (i1) edge [dashed] node {} (i);

    
            \path[->] (u) edge node {} (v1);
            \path[->] (u) edge node {} (v);
            % \path[->] (u) edge [line width=2pt] node {} (i);
            % \path[->] (u) edge [line width=2pt] node {} (t);
            % \path[->] (u) edge [line width=2pt] node {} (i1);
            \end{tikzpicture}
        \caption{Correlated vaccination behavior.}
        \end{subfigure}
        \begin{subfigure}{0.8\linewidth}
            \centering
            \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.25cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
                \tikzstyle{every state}=[
                  draw = none,
                  fill = none
                ]
                \node[state] (x) {$X$};
                \node[state] (v) [right of=x] {$V$};
                \node[state] (i) [right of=v] {$I_2$};
                \node[state] (t) [right of=i] {$T$};
                \node[state] (is) [right of=t] {$I^*$};
                \node[state] (i1) [below of=i] {$I_1$};
                \node[state] (u) [below of=v] {$U$};
       
                \path[->] (x) edge node {} (v);
                \path[->] (x) edge [out=45, in=135] node {} (i);
        
                \path[->] (v) edge node {} (i);
                
                \path[->] (i) edge node {} (t);
                \path[->] (i1) edge node {} (t);
                \path[->] (i1) edge node {} (is);
    
                \path[->] (x) edge [out=45, in=135] node {} (t);
        
                \path[->] (t) edge node {} (is);
    
                \path[->] (i) edge [out=45, in=135] node {} (is);    
        
                \path[->] (u) edge node {} (v);
                \path[->] (u) edge node {} (i);
                \path[->] (u) edge node {} (t);
                \path[->] (u) edge node {} (i1);
                \end{tikzpicture}
            \caption{$I_1$ and $I_2$ are not mutually exclusive.}
            \end{subfigure}
            \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
            \tikzstyle{every state}=[
              draw = none,
              fill = none
            ]
            \node[state] (x) {$X$};
            \node[state] (v) [right of=x] {$V$};
            \node[state] (i) [right of=v] {$I_2$};
            \node[state] (t) [right of=i] {$T$};
            \node[state] (is) [right of=t] {$I^*$};
            \node[state] (i1) [below of=i] {$I_1$};
            \node[state] (u) [below of=x] {$U$};
            \node[state] (v1) [below of=v] {$V_1$};
   
            \path[->] (x) edge node {} (v);
            \path[->] (x) edge [out=45, in=135] node {} (i);
    
            \path[->] (v) edge node {} (i);
            
            \path[->] (i) edge node {} (t);
            \path[->] (i1) edge node {} (t);
            \path[->] (i1) edge node {} (is);

            \path[->] (x) edge [out=45, in=135] node {} (t);
    
            \path[->] (t) edge node {} (is);
            \path[->] (v1) edge node {} (i1);

            \path[->] (i) edge [out=45, in=135] node {} (is);
            \path[->] (i1) edge [dashed] node {} (i);

    
            \path[->] (u) edge node {} (v1);
            \path[->] (u) edge node {} (v);
            % \path[->] (u) edge [line width=2pt] node {} (i);
            % \path[->] (u) edge [line width=2pt] node {} (t);
            % \path[->] (u) edge [line width=2pt] node {} (i1);
            \end{tikzpicture}
        \caption{Correlated vaccination behavior.}
        \end{subfigure}
    % \begin{subfigure}{0.8\linewidth}
    %     \centering
    %     \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.25cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
    %         \tikzstyle{every state}=[
    %           draw = none,
    %           fill = none
    %         ]
    %         \node[state] (x) {$X$};
    %         \node[state] (v) [right of=x] {$V$};
    %         \node[state] (i) [right of=v] {$\mathbbm{1}(I = 2)$};
    %         \node[state] (t) [right of=i] {$T$};
    %         \node[state] (is) [right of=t] {$I^*$};
    %         \node[state] (i1) [below of=i] {$\mathbbm{1}(I = 1)$};
    %         \node[state] (u) [below of=v] {$U$};
    
    %         \path[->] (x) edge node {} (v);
    %         \path[->] (x) edge [out=45, in=135] node {} (i);
    
    %         \path[->] (v) edge node {} (i);
            
    %         \path[->] (i) edge node {} (t);
    %         \path[->] (i1) edge node {} (t);
    %         \path[->] (x) edge [out=45, in=135] node {} (t);
    
    %         \path[->] (t) edge node {} (is);
    
    %         \path[->] (i) edge [out=45, in=135] node {} (is);
    
    
    %         \path[->] (i1) edge node {} (i);
    
    
    %         \path[->] (u) edge node {} (x);
    %         \path[->] (u) edge node {} (v);
    %         \path[->] (u) edge node {} (i);
    %         \path[->] (u) edge node {} (t);
    %         \path[->] (u) edge node {} (i1);
            
    %         \end{tikzpicture}
    %     \caption{Splitting $I$ node to show how $I=1$ is a negative outcome control. Solid line is deterministic relationship as $I = 1$ and $I = 2$ are mutually exclusive.}
    %     \end{subfigure}
    %     \begin{subfigure}{0.8\linewidth}
    %         \centering
    %         \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.25cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
    %             \tikzstyle{every state}=[
    %               draw = none,
    %               fill = none
    %             ]
    %             \node[state] (x) {$X$};
    %             \node[state] (v) [right of=x] {$V$};
    %             \node[state] (i) [right of=v] {$I$};
    %             \node[state] (z) [right of=i] {$Z$};

    %             \node[state] (t) [right of=z] {$T$};
    %             \node[state] (is) [right of=t] {$I^*$};
    %             \node[state] (u) [below of=v] {$U$};
        
    %             \path[->] (x) edge node {} (v);
    %             \path[->] (x) edge [out=45, in=135] node {} (i);
    %             \path[->] (x) edge [out=45, in=135] node {} (t);
                
    %             \path[->] (v) edge node {} (i);
                
    %             \path[->] (i) edge node {} (z);
    %             \path[->] (i) edge [out=45, in=135] node {} (is);
        
    %             \path[->] (t) edge node {} (is);
        
    %             \path[->] (u) edge node {} (x);
    %             \path[->] (u) edge node {} (v);
    %             \path[->] (u) edge node {} (i);
    %             \path[->] (u) edge node {} (t);
                
    %             \end{tikzpicture}
    %         \caption{Causal directed-acyclic graph for the test-negative design}
    %     \end{subfigure}
    \caption{A}\label{fig:dags}
\end{figure}
\clearpage
\subsection{What if test-positive and test-negative infections are not mutually exclusive?}
Define $I_1$ as an indicator of a symptomatic test-negative infection and $I_2$ as an indicator of a symptomatic test-positive infection, where now we allow for the possibility that $\Pr[I_1 = 1, I_2 = 1] > 0$, but still assume that the symptom screen is effective such that, for any individual $i$, $\mathbbm 1(I_{i,1} + I_{i,2}) = 1$ when $S_i = 1$. We use the following modified assumption set 
\begin{itemize}
    \item[(A1$^\ddagger$)] Consistency of potential outcomes. For all individuals $i$ and for $v \in \{0, 1\}$, we have $I_{2i}^v = I_{2i}$, $I_{1i}^v = I_{1i}$, and $T_i^v = T_i$ when $V_i = v$.
    \item[(A2$^\ddagger$)] No effect of vaccination on test-negative symptomatic illness  among the vaccinated. That is, $\Pr(I_1^0 = 1 | V = 1, X) = \Pr(I_1^1 = 1 | V = 1, X).$
    \item[(A3$^\ddagger$)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for test-positive and test-negative illnesses, i.e. 
        $$\beta_2(X) = \beta_1(X), $$
        $$ \text{where } \beta_i(X) = \log \frac{\Pr(I^0_i = 1, T^0 = 1 | V = 1, X)\Pr(I^0_i = 0, T^0 = 1 | V = 0, X)}{\Pr(I^0_i = 0, T^0 = 1 | V = 1, X)\Pr(I^0_i = 1, T^0 = 1| V = 0, X)}.$$
    \item[(A4$^\ddagger$)] Overlap of vaccination among test-positives and test-negatives. Define $\mathcal{S}_i(v)$ as the support of the law of $(I^v_i, T^v = 1, V = v, X)$, then for $v$ in $\{0,1\}$, then it must be that $\mathcal{S}_2(1) \subseteq \mathcal{S}_2(0)$ and $\mathcal{S}_2(v) \subseteq \mathcal{S}_1(v).$
    \item[(A5$^\ddagger$)] No direct effect of vaccination on test-seeking behavior among the vaccinated. That is, for $i$ in $\{1,2\}$, $\Pr[T^1 = 1 | I^1_i = 1, V = 1, X] = \Pr[T^0 = 1 | I^0_i = 1, V = 1, X].$
\end{itemize}
Here, we show that, under assumptions A1$^\ddagger$-A5$^\ddagger$, the causal odds ratio among the vaccinated remains identified in the test-negative design.

\begin{theorem}
    If the test-negative and test-positive illnesses are not mutually exclusive, then, under assumptions A1$^\ddagger$-A5$^\ddagger$, the causal odds ratio among the vaccinated, 
    \begin{equation*}
        \Phi_{ORV} \equiv \dfrac{\Pr(I^1_2=1|V=1)\Pr(I^1_2=0|V=1)}{\Pr(I^0_2=0|V=1)\Pr(I^0_2=1|V=1)},
    \end{equation*}
    is identified by $\Psi^*_{om}$ and $\Psi^*_{ipw}$ (as defined in Corollary \ref{corollary2}) in the test-negative design.
    \end{theorem}
    
    \begin{proof}
    \begin{align*}
        \Psi_{om}^* &= \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I^* = 0 | S = 1, V = 1, X)}{\Pr(I^* = 0 | S = 1, V = 0, X)} \Pr(I^* = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} \\
        &= \dfrac{\Pr(I_2 = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I_1 = 1, I_2 = 0  | S = 1, V = 1, X)}{\Pr(I_1 = 1, I_2 = 0 | S = 1, V = 0, X)} \Pr(I_2 = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} 
    \end{align*}
    \begin{align*}
        \Phi_{ORV} &= \dfrac{\Pr(I^1_2=1|V=1)\Pr(I^0_2=0|V=1)}{\Pr(I^1_2=0|V=1)\Pr(I^0_2=1|V=1)} \\
        &=\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)\Pr(I^0_2=0, T^0 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)\Pr(I^0_2=1, T^0 = 1|V=1)} \\
        &=\dfrac{\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)}}{E\left\{\dfrac{\Pr(I^0_2=1, T^0 =1|V=1, X)}{\Pr(I^0_2=0, T^0 = 1|V=1, X)}\bigg| V = 1\right\}} \\
        &=\dfrac{\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)}}{E\left\{\dfrac{\dfrac{\Pr(I^0_1=1, T^0 = 1|V=1, X)}{\Pr(I^0_1=0, T^0 = 1|V=1, X)}}{\dfrac{\Pr(I^0_1=1, T^0 = 1|V=0, X)}{\Pr(I^0_1=0, T^0 = 1|V=0, X)}}\dfrac{\Pr(I^0_2=1, T^0 = 1|V=0, X)}{\Pr(I^0_2=0, T^0 = 1|V=0, X)}\bigg| V = 1\right\}} \\
        &=\dfrac{\dfrac{\Pr(I^1_2=1| T^1 = 1, V=1)}{\Pr(I^1_2=0 | T^1 = 1, V=1)}}{E\left\{\dfrac{\dfrac{\Pr(I^0_1=1 | T^0 = 1, V=1, X)}{\Pr(I^0_1=0 | T^0 = 1, V=1, X)}}{\dfrac{\Pr(I^0_1=1 | T^0 = 1, V=0, X)}{\Pr(I^0_1=0 | T^0 = 1, V=0, X)}}\dfrac{\Pr(I^0_2=1 | T^0 = 1, V=0, X)}{\Pr(I^0_2=0 | T^0 = 1,   V=0, X)}\bigg| V = 1\right\}} \\
        &=\dfrac{\dfrac{\Pr(I^1_2=1|V=1)}{\Pr(I^1_2=0|V=1)}}{E\left\{\dfrac{\Pr(I^1_1=1|V=1, X)\Pr(I^0_1=0|V=0, X)}{\Pr(I^1_1=0|V=1, X)\Pr(I^0_1=1|V=0, X)}\dfrac{\Pr(I^0_2=1|V=0, X)}{\Pr(I^0_2=0|V=0, X)}\bigg| V = 1\right\}} \\
        &=\dfrac{\dfrac{\Pr(I_2=1|V=1)}{\Pr(I_2=0|V=1)}}{E\left\{\dfrac{\Pr(I_1=1|V=1, X)\Pr(I_1=0|V=0, X)}{\Pr(I_1=0|V=1, X)\Pr(I_1=1|V=0, X)}\dfrac{\Pr(I_2=1|V=0, X)}{\Pr(I_2=0|V=0, X)}\bigg| V = 1\right\}}
    \end{align*}
    For the first expression, note that by Assumption A3 and exclusivity of test-negative and test-positive illnesses
    \begin{align*}
        \dfrac{\Pr(I^0 = 2 | V = 1, X)}{\Pr(I^0 = 2 | V = 0, X)} &= \dfrac{\Pr(I^0 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
        &= \dfrac{\Pr(I^1 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
        &= \dfrac{\Pr(I = 1 | V = 1, X)}{\Pr(I = 1 | V = 0, X)} \\
    \end{align*}
    where the first line is by Assumption A3 and exclusivity of test-negative and test-positive illnesses. The second line is by Assumption A2. And the last line applies consistency. This implies
    \begin{equation*}
        \alpha_2^0(X) = \alpha_1(X)
    \end{equation*}
    and, therefore, combining with the derivation of Theorem 1, we have
    \begin{align*}
        \Psi &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha^0_2(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}} \\
        &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha_1(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}}.
    \end{align*}
    
    For the second expression, note first that, by the invariance of odds ratios, Assumptions A2 and A3 imply
    \begin{equation*}
        \dfrac{\Pr(V = 1 | I^0 = 2, X)}{\Pr(V = 0 | I^0 = 2, X)} = \dfrac{\Pr(V = 1 | I = 1, X)}{\Pr(V = 0 | I = 1, X)}
    \end{equation*}
    and by consequence 
    \begin{equation*}
        \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)} = \dfrac{\pi_1(X)}{1 - \pi_1(X)}.
    \end{equation*}
    Thus, again, continuing the derivation in Theorem 1, we have 
    \begin{align*}
        \Psi &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi_1(X)}{1-\pi_1(X)}\right\}}.
    \end{align*}
    \end{proof}
    \newpage
    
\begin{equation*}
    \Psi = \dfrac{\Pr(I^1_2 = 1 | V = 1)}{\Pr(I^0_2 = 1| V = 1)}
\end{equation*}
Option 1: If tests for $I=1$ and $I=2$ exist 
\begin{equation*}
    \Psi_{ORV} = \dfrac{\Pr(I^1_2 = 1 | V = 1)\Pr(I^0_2 = 0| V = 1)}{\Pr(I^1_2 = 0| V = 1)\Pr(I^0_2 = 1| V = 1)}
\end{equation*}
% \begin{align*}
%     \Pr[I^* = 1 | S = 1, X, V] &= \Pr[I_2 = 1 | S = 1, X, V]\\
%     \Pr[I^* = 0 | S = 1, X, V] &= \Pr[I_1 = 1 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% \begin{align*}
%     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] \\
%     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] \\
%     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
%     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V]  - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% \begin{align*}
%     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] \\
%     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] \\
%     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I_2 = 0 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
%     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I_2      = 1 | S = 1, X, V]  - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% % \begin{align*}
% %     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] + \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
% %     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% % \end{align*}
% % and 
% % \begin{align*}
% %     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] \\
% %     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] 
% % \end{align*}
% In this case, Assumption (4) is 
% $$OR_2(X) = OR_1(X), $$
% $$ \text{where } OR_i(X) = \frac{\Pr[I^0_i = 1, T^0 = 1 | V = 1, X]\Pr[I^0_i = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_i = 0, T^0 = 1 | V = 1, X]\Pr[I^0_i = 1, T^0 = 1| V = 0, X]}.$$
% First, we show that, when $I_1$ and $I_2$ are not mutually exclusive the causal risk ratio is not identified. Following from our previous proof, we have that 
% \begin{align*}
%     \psi_{rrv}(X) &= \phi(X) \times \dfrac{\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 0, X]}\\
%     \Pr[I_2^0 = 2, T^0 = 1 | V = 1, X] &= \dfrac{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1 | V = 0, X]}{\Pr[I_1 = 0, T = 1 | V = 1, X] \Pr[I_1 = 1, T = 1 | V = 0, X]}\\
%     & \quad \quad \times  \dfrac{\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 0, X]} \Pr[I_2 = 1, T = 1 | V = 0, X]
% \end{align*}

% Here we show that, under these conditions, the conditional odds ratio in the TND identifies the conditional causal odds ratio, i.e.
% \begin{equation*}
%      \psi_{orv}(X) &\equiv \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]}. 
% \end{equation*}

% \begin{align*}
%         \psi_{orv}(X) &= \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I^0_1 = 0, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 1, T^0 = 1 | V = 0, X]}{\Pr[I^0_1 = 1, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 0, T^0 = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I_2 = 0, T = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I_2 = 1, T = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0, T = 1 | V = 1, X]\Pr[I_1 = 1, T = 1 | V = 0, X]}{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1 | I \neq 0, T = 1, V = 1, X]\Pr[I_2 =0 |  I \neq 0, T = 1, V = 0, X]}{\Pr[I_2 = 0 |  I \neq 0, T = 1, V = 1, X]\Pr[I_2 = 1 |  I \neq 0, T = 1, V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0 |  I \neq 0, T = 1, V = 1, X]\Pr[I_1 = 1 |  I \neq 0, T = 1, V = 0, X]}{\Pr[I_1 = 1 |  I \neq 0, T = 1, V = 1, X]\Pr[I_1 = 0 |  I \neq 0, T = 1, V = 0, X]}\\
%         &=  \dfrac{\Pr[I_2 = 1 | S = 1, V = 1, X]\Pr[I_2 =0 |  S = 1, V = 0, X]}{\Pr[I_2 = 0 |  S = 1, V = 1, X]\Pr[I_2 = 1 | S = 1, V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0 |  S = 1, V = 1, X]\Pr[I_1 = 1 |  S = 1, V = 0, X]}{\Pr[I_1 = 1 |  S = 1, V = 1, X]\Pr[I_1 = 0 |  S = 1, V = 0, X]}\\
%         &=  \dfrac{\Pr[I^* = 1 | S = 1, V = 1, X]}{\Pr[I^* = 0|  S = 1, V = 1, X]} \dfrac{\Pr[I^* = 0 |  S = 1, V = 0, X]}{\Pr[I^* = 1 | S = 1, V = 0, X]}\\
% \end{align*}  
    
% Note that in this case, Assumption (4) becomes 
% \begin{itemize}
%      \item[(A4)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for all symptomatic illness regardless if $I_1$ or $I_2$ is cause, i.e. 
%     $$OR_2(X) = OR_1(X), $$
%     $$ \text{where } OR_i(X) = \frac{\Pr[I^0_i = 1, T^0 = 1 | V = 1, X]\Pr[I^0_i = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_i = 0, T^0 = 1 | V = 1, X]\Pr[I^0_i = 1, T^0 = 1| V = 0, X]}.$$
% \end{itemize}
% Consider the conditional odds ratio for the effect of vaccination among the vaccinated, i.e.
%     \begin{align*}
%         \psi_{orv}(X) &\equiv \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I^0_1 = 1, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_1 = 0, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 1, T^0 = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I_2 = 1, T = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I_2 = 0, T = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1 | V = 0, X]}{\Pr[I_1 = 0, T = 1 | V = 1, X]\Pr[I_1 = 1, T = 1| V = 0, X]}
%     \end{align*}        
% Under the consistency assumption (A1) the numerator is equal to $\Pr[I = 2, T = 1 | V = 1, X]$. Focusing on the denominator, under equi-confounding (A3)
% \begin{equation*}
% \Pr[I^0 = 2, T^0 = 1  | V = 1, X] = \frac{\Pr[I^0 = 1, T^0 = 1  | V = 1, X]}{\Pr[I^0 = 1, T^0 = 1  | V = 0, X]}\Pr[I^0 = 2, T^0 = 1 | V = 0, X]
% \end{equation*}
% and then by (A1) and (A2) with (A4) ensuring overlap
%     \begin{equation*}
%      \Pr[I^0 = 2, T^0 = 1  | V = 1, X] = \frac{\Pr[I = 1, T = 1  | V = 1, X]}{\Pr[I = 1, T = 1  | V = 0, X]}\Pr[I = 2, T = 1 | V = 0, X]
%     \end{equation*}
% Plugging back into the expression for $\psi_{rrv}(X)$, we find the following identifying expression 
%     \begin{equation*}
%          \phi(X) \equiv \dfrac{\dfrac{\Pr[I = 2, T = 1 | V = 1, X]}{\Pr[I = 1, T = 1 | V = 1, X]}}{\dfrac{\Pr[I = 2, T = 1 | V = 0, X]}{\Pr[I = 1, T = 1 | V = 0, X]}}
%     \end{equation*}
% which is the ratio of the odds of symptomatic infection with the vaccine pathogen versus symptomatic infection with another pathogen in the vaccinated and unvaccinated. It is also strictly written in terms of the observables. A key insight is that $\frac{\Pr[I = 1, T =1  | V = 0, X]}{\Pr[I = 1, T = 1 | V = 1, X]}$ acts as a proxy for $\frac{\Pr[I^0 = 2, T =1  | V = 0, X]}{\Pr[I^0 = 2, T = 1 | V = 1, X]}$ essentially a ``parallel trend'' for $I=2$ in absence of vaccination.

% Recall that, in a test-negative study, we only observe test results among the symptomatic and tested, i.e. samples $\{(X_i, V_i, S_i = 1, I^*_i) : i = 1, \ldots, n\}$ where $S = \mathbb{I}(I \neq 0, T = 1)$. However, we can show that 
%     \begin{align*}
%          \phi(X) &= \dfrac{\dfrac{\Pr[I^* = 1 | S = 1, V = 1, X]}{\Pr[I^* = 0 | S = 1, V = 1, X]}}{\dfrac{\Pr[I^* = 1 | S = 1, V = 0, X]}{\Pr[I^* = 0 | S = 1, V = 0, X]}}
%     \end{align*}    
% which is the odds ratio comparing odds of testing positive for vaccinated and unvaccinated among the tested only.
\newpage 
\subsection{What if there is a direct effect of vaccination on testing behavior?}
Unlike in a placebo-controlled trial, participants in a test-negative design are generally aware of their vaccination status. It is therefore possible that this knowledge could affect their testing behavior in a number of ways which would violate Assumption A5. For instance, some individuals may be less likely to get tested when vaccinated because they feel more protected or perceive the risk of illness to be lower. Here, we consider identifiability under a weaker assumption A5*:
\begin{itemize}
    \item[(A5*)] Equivalent effects of vaccination on test-seeking behavior for test-positive and test-negative illnesses among the vaccinated. That is, 
    \begin{equation}
    \dfrac{\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)} = \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}.
\end{equation}
\end{itemize}
This assumption allows for vaccination to affect testing behavior provided it does so similarly for test-positive and test-negative illnesses. Together with assumption A3 it implies
\begin{equation*}
    \dfrac{\Pr(T = 1 | I = 1, V = 1, X)}{\Pr(T = 1 | I = 1, V = 0, X)} = \dfrac{\Pr(T = 1 | I = 2, V = 1, X)}{\Pr(T = 1 | I = 2, V = 0, X)}.
\end{equation*}
It may be plausible given that participants may not know which infection they have prior to receiving a test. Note that, it would still be violated if vaccination reduced severity or altered symptoms of infection, and therefore willingness to seek a test, for $I=2$ but not $I=1$. In Corollary 3, we show that under this assumption we can still identify the causal risk ratio among the vaccinated in the test-negative study. 
\vspace{1em}

% \begin{equation}
%     \dfrac{\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)} = \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}
% \end{equation}

% \begin{align*}
%     \Pr&(T^1 = 1 | I^1 = 1, V = 1, X) \\
%     &= \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}\Pr(T^0 = 1 | I^0 = 1, V = 1, X) \\
%     &= \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)\Pr(T^0 = 1 | I^0 = 1, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)\Pr(T^0 = 1 | I^0 = 2, V = 0, X)} \\
%      &= \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 0, X)} \\
% \end{align*}

% \begin{align*}
%     \Pr&(T^0 = 1 | I^0 = 1, V = 1, X) \\
%     &= \dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)} \Pr(T^1 = 1 | I^1 = 1, V = 1, X)\\
%     &= \dfrac{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)\Pr(T^0 = 1 | I^0 = 2, V = 0, X)\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)} \\
%      1 &= \dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 0, X)\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)} \\
% \end{align*}
    

\begin{corollary}
    Under assumptions A1-A4 and alternative assumption A5* that direct effects of vaccination on testing behavior are the same for test-positive and test-negative illnesses, $\Psi$ is identified by $\Psi^\dagger_{om}$ and $\Psi^\dagger_{ipw}$ (as defined in Corollary \ref{corollary1}) in the full cohort as well as $\Psi^*_{om}$ and $\Psi^*_{ipw}$ (as defined in Corollary \ref{corollary2}) in the test-negative design.
\end{corollary}
    
    \begin{proof}
        Define $\gamma(X)$ as 
        \begin{equation*}
            \gamma(X) \equiv \dfrac{\Pr(T = 1 | I = 1, V = 1, X)}{\Pr(T = 1 | I = 1, V = 0, X)} \dfrac{\Pr(T = 1 | I = 2, V = 0, X)}{\Pr(T = 1 | I = 2, V = 1, X)},
        \end{equation*}
        and note that by Assumption A5*, $\gamma(X) = 1$. First, we show that $\Psi$ is identified by $\Psi^\dagger_{om}$ and therefore by Corollary 2 by $\Psi^*_{om}$. We have
        \begin{align*}
            \Psi &= \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)} \\
            &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\Pr(I^0=2|V=1, X) | V= 1 \right\}} \\
            &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\dfrac{\Pr(I^0=1|V=1, X)}{\Pr(I^0=1|V=0, X)}\Pr(I^0=2|V=0, X) \bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\dfrac{\Pr(I^1=1|V=1, X)}{\Pr(I^0=1|V=0, X)}\Pr(I^0=2|V=0, X) \bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I=2|V=1)}{E\left\{\dfrac{\Pr(I=1|V=1, X)}{\Pr(I=1|V=0, X)}\Pr(I=2|V=0, X) \gamma(X) \bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I=2|V=1)}{E\left\{\dfrac{\Pr(I=1, T=1|V=1, X)}{\Pr(I=1, T = 1|V=0, X)} \dfrac{\Pr(I=2, T=1|V=0, X)}{\Pr(T^ = 1 | I = 2, V = 1, X)}\bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I=2, T=1 |V=1)}{E\left\{\dfrac{\Pr(I=1, T=1|V=1, X)}{\Pr(I=1, T= 1|V=0, X)} \Pr(I=2, T=1|V=0, X) \bigg| V= 1 \right\}} \\
            &= \Psi^\dagger_{om}.
        \end{align*}
        The first line follows by definition. The second applies the law of iterated expectations. The third applies assumption A3. The fourth applies assumption A2. The fifth applies new assumption A5*. The sixth replaces the product of conditionals with the joint distribution. The seventh reverses the law of iterated expectations. 

        Next, we show that $\Psi$ is identified by $\Psi^\dagger_{ipw}$ and therefore by Corollary 2 by $\Psi^*_{ipw}$. Note that, by the invariance of the odds ratio, 
        \begin{equation*}
            \gamma(X) = \dfrac{\Pr(V = 1 | I = 1, T = 1, X)}{\Pr(V = 0 | I = 1, T = 1, X)} \dfrac{\Pr(V = 0 | I = 2, T = 1, X)}{\Pr(V = 1 | I = 2, T = 1, X)},
        \end{equation*}
        We have
        \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)} \\
        &= \dfrac{E\left\{\dfrac{V}{\Pr(V=1)} \mathbbm 1 (I^1 = 2)\right\}}{E\left\{\dfrac{V}{\Pr(V=1)}\mathbbm 1 (I^0 = 2)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) E(V | I^0 = 2, T^0 = 1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) \pi^{0\dagger}_2(X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) \dfrac{\pi^{0\dagger}_2(X)}{1-\pi^{0\dagger}_2(X)}E(1-V|I^0 = 2, T^0 = 1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T = 1) \dfrac{\pi^{0\dagger}_2(X)}{1-\pi^{0\dagger}_2(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T = 1) \dfrac{\pi^\dagger_1(X)}{1-\pi^\dagger_1(X)}\right\}}.
    \end{align*}
    \end{proof}

\newpage
\subsection{What if symptom screen is imperfect?}
Here we mean what if some non
\newpage

\subsection{What if test is imperfect?}
In the main text, we assumed the availability of a perfect test such that $I^*_i = \mathbbm 1 (I_i = 2, T_i = 1)$ for all individuals. In the absence of such a test, here we describe the potential for bias due to misclassification of case status which has also been described previously \cite{sullivan_theoretical_2016}. Because most studies employ real-time reverse-transcription polymerase chain reaction (RT-PCR), false positives are unlikely as most RT-PCR tests have specificity approaching 100\%. Therefore, misclassification of test-positives is probably rare, perhaps due to sample contamination or data entry errors. False negatives may be be more likely as test sensitivity is generally lower due to the fact that (1) some of those infected with test-positive illness may not shed detectable virus or viral RNA; (2) some may seek care after shedding has ceased; or (3) sample quality may be compromised due to swab quality or inadequate storage. If sample collection is sufficient such that the source of the test-negative infection can be identified via RT-PCR, then sensitivity with respect to the test-positive illness can be improved by limiting to test-negative controls with an identified cause. In the case that misclassification is nondifferential with respect to vaccination, bias is likely to be minimal due to high specificity. However, care should be taken as test errors must be independent of potential confounders and effect modifiers for the bias to be strictly towards the null. 

\subsection{What about immortal time bias?}

\subsection{Relationship to vaccination mechanisms}
% Define the following:
% \begin{description}
%     \item[$\lambda$:] Force of infection 
%     \item[$\pi$:] Probability of symptoms given infection
%     \item[$P$:] Total population 
%     \item[$v$:] Proportion vaccinated
%     \item[$\mu$:] Probability of care-seeking given symptoms
%     \item[$\theta(t):$] Hazard ratio for infection given 
% \end{description}
% The rate of ascertaining test-positive, vaccinated persons is
% $$
% \Lambda_{V I}(t)=\lambda_I \pi_I \mu_V[(1-\varphi) e^{-\lambda_I t} + \varphi \theta e^{-\theta \lambda_I t}] v P .
% $$

% $$
% \Lambda_{U I}(t)=\lambda_I \pi_I \mu_U e^{-\lambda_I t}(1-v) P .
% $$

% Test-negative vaccinated and unvaccinated persons are ascertained at the rates
% $$
% \Lambda_{V N}(t)=\lambda_N \pi_N \mu_V v P
% $$
% and
% $$
% \Lambda_{U N}(t)=\lambda_N \pi_N \mu_U(1-v) P,
% $$
% respectively, assuming vaccination does not affect susceptibility to the test-negative conditions.

% Test-negative studies typically measure the odds ratio of vaccination among the test-positive and test-negative subjects, similar to the exposure odds ratio in case-control studies, using cumulative cases $(C)$. For the test-positive outcome,
% $$
% \begin{gathered}
% C_{V I}(t)=\pi_I \mu_V\left[(1-\varphi)\left(1-e^{-\lambda_I t}\right)+\varphi\left(1-e^{-\theta \lambda_I t}\right)\right] v P \\
% C_{U I}(t)=\pi_I \mu_U\left(1-e^{-\lambda_I t}\right)(1-v) P .
% \end{gathered}
% $$

% Under the assumption that test-negative infections are not immunizing, cumulative cases are proportional to the incidence rate and study duration:
% $$
% \begin{gathered}
% C_{V N}(t)=\lambda_N \pi_N \mu_V v P t \\
% C_{U N}(t)=\lambda_N \pi_N \mu_U(1-v) P t .
% \end{gathered}
% $$

% We consider the case of immunizing test-negative outcomes in Web Appendix 1. Using the vaccine-exposure odds ratio measured from cumulative cases,
% $$
% \begin{aligned}
% 1-O R^C(t) & =1-\frac{C_{V I}(t) C_{U N}(t)}{C_{U I}(t) C_{V N}(t)} \\
% & =1-\frac{(1-\varphi)\left(1-e^{-\lambda_I t}\right)+\varphi\left(1-e^{-\theta \lambda_I t}\right)}{1-e^{-\lambda_I t}} \\
% & =\varphi\left(1-\frac{1-e^{-\theta \lambda_I t}}{1-e^{-\lambda_I t}}\right) .
% \end{aligned}
% $$

% $\varphi$
\end{appendix}

