
\begin{appendix}

    \renewcommand{\thefigure}{A\arabic{figure}}
    \setcounter{figure}{0}
    
    \renewcommand{\thetable}{A\arabic{table}}
    \setcounter{table}{0}
    
    \renewcommand{\theequation}{A\arabic{equation}}
    \setcounter{equation}{0}

    \singlespacing
%    \appendixwithtoc
    \newpage

    \section{Proofs of main identifiability results} \label{sec:proofs}
    For convenience, we restate the core identifiability assumptions from the main text. 
    \begin{enumerate}[label=\upshape(A\arabic*), ref=A\arabic*]
    \item\label{app_ass1} Consistency of potential outcomes. For all individuals $i$ and for $v \in \{0, 1\}$, we have $I_i^v = I_i$ and $T_i^v = T_i$ when $V_i = v$. 
    \item\label{app_ass2} No effect of vaccination on testing negative and symptomatic among the vaccinated. That is, $\Pr[I^0 = 1, T^0=1 | V = 1, X] = \Pr[I^1 = 1, T^1=1 | V = 1, X].$
    \item\label{app_ass3} Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for test-positive and test-negative illnesses, i.e. 
    $$OR_2(X) = OR_1(X), $$
    $$ \text{where } OR_i(X) = \frac{\Pr[I^0 = i, T^0 = 1 | V = 1, X]\Pr[I^0 = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0 = 0, T^0 = 1 | V = 1, X]\Pr[I^0 = i, T^0 = 1| V = 0, X]}.$$
    \item\label{app_ass4} Overlap of vaccination among test-positives and test-negatives. Define $\mathcal{S}_i(v)$ as the support of the law of $(I^0 = i, T^0 = 1, V = v, X)$, then for $v$ in $\{0,1\}$, then it must be that $\mathcal{S}_2(1) \subseteq \mathcal{S}_2(0)$ and $\mathcal{S}_2(v) \subseteq \mathcal{S}_1(v).$
    %\item[(A5)] No direct effect of vaccination on test-seeking behavior among the vaccinated. That is, for $i$ in $\{1,2\}$, $\Pr[T^1 = 1 | I^1 = i, V = 1, X] = \Pr[T^0 = 1 | I^0 = i, V = 1, X].$
\end{enumerate}

    %In Proposition \ref{prop1}, we established that, under Assumption \ref{app_ass1} alone, the causal risk ratio for medically-attended illness among the vaccinated, $\Psi_{RRV}$, is equivalent to two expressions involving the (unobserved) treatment-free potential outcome $\mathbbm 1(I^0 = 2, T^0=1)$, one based on the outcome itself and one based on the so-called generalized propensity score. In Lemma \ref{lemma1}, we show that, if we add Assumptions \ref{app_ass2} - \ref{app_ass4}, we can use the observed ratio of testing negative among the vaccinated and unvaccinated as a proxy for expressions involved the treatment-free potential outcome and therefore identify $\Psi_{RRV}$ when we have access to unbiased samples from the underlying target population. Then in Lemma \ref{lemma2} we show $\Psi_{RRV}$ is still identifiable despite the biased sampling design of the test-negative design, due to the invariance of the odds ratio, thus completing our proof. 
    
    \newpage
    \subsection{Proof of Proposition \ref{prop1}} \label{sec:proof1}
    
    \begin{proof}
    We begin by showing that, under Assumption \ref{app_ass1} alone, the causal risk ratio for medically-attended illness among the vaccinated, $\Psi_{RRV}$, 
    \begin{equation*}
        \Psi \equiv \dfrac{\Pr(I^1=2, T^1=1|V=1)}{\Pr(I^0=2, T^0=1|V=1)},
    \end{equation*}
    is equivalent to two expressions involving only the (unobserved) treatment-free potential outcome, $\mathbbm 1(I^0 = 2, T^0=1)$, namely 
    \begin{equation}
        \Psi^0_{om} \equiv \dfrac{\Pr(I = 2, T = 1 | V = 1)}{E\left[\Pr(I = 2, T = 1 | V = 0, X) \exp\{\alpha^0_2(X)\}\Big| V = 1 \right]}
    \end{equation}
    and 
    \begin{equation}
        \Psi^0_{ipw} \equiv \dfrac{E\{V \mathbbm 1(I = 2, T = 1)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2, T = 1) \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)}\right\}}
    \end{equation}
    where 
    \begin{equation*}
        \pi^0_i(X) \equiv \Pr(V = 1 | I^0 = i, T^0 = 1,  X)
    \end{equation*}
    is the extended propensity score and 
     \begin{equation*}
        \alpha^0_i(X) \equiv \log \dfrac{\Pr(I^0 = i, T^0 = 1 | V = 1, X)}{\Pr(I = i, T=1 | V = 0, X)}
    \end{equation*}
    
    For the first expression, we have 
    \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2, T^1=1|V=1)}{\Pr(I^0=2, T^0=1|V=1)} \\
        &= \dfrac{\Pr(I^1=2,T^1=1|V=1)}{E[E\{\mathbbm 1 (I^0 = 2, T^0=1) | V = 1, X\} | V = 1]} \\
        &= \dfrac{\Pr(I^1=2,T^1=1|V=1)}{E\left\{\int \mathbbm 1 (I^0 = 2, T^0=1) f(I^0 = i, T^0=1 | V = 1, X) di \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2,T^1=1|V=1)}{E\left\{\int \mathbbm 1 (I^0 = 2, T^0=1) f(I^0 = i, T^0=1 | V = 0, X) \dfrac{f(I^0 = i,T^0=1 | V = 1, X)}{f(I^0 = i, T^0=1| V = 0, X)}di \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I^1=2,T^1=1|V=1)}{E\left\{ \Pr(I^0 = 2, T^0=1 | V = 0, X) \dfrac{\Pr(I^0 = 2, T^0=1 | V = 1, X)}{\Pr(I^0 = 2, T^0=1 | V = 0, X)} \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I=2,T=1|V=1)}{E\left[ \Pr(I = 2, T=1 | V = 0, X) \exp\{\alpha^0_2(X)\} \mid  V = 1\right]}.
    \end{align*}
    The first line restates the definition. The second uses the law of iterated expectation. The third applies the definition of conditional expectation. The fourth multiplies the density by one. The fifth evaluates the integral and the sixth applies consistency and the definition of $\alpha_i(X)$
    
    For the second, we have 
    \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2, T^1=1|V=1)}{\Pr(I^0=2, T^0=1|V=1)} \\
        &= \dfrac{E\left\{\dfrac{V}{\Pr(V = 1)} \mathbbm 1 (I^1 = 2, T^1=1)\right\}}{E\left\{\dfrac{V}{\Pr(V=1)}\mathbbm 1 (I^0 = 2,T^0=1)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1=1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0=1) E(V | I^0 = 2, T^0=1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1=1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0=1) \pi^0_2(X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1=1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0=1) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}E(1-V|I^0 = 2, T^0=1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T=1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T=1) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}}.
    \end{align*}
    The first line restates the definition. The second uses the definition of conditional expectation. The third applies the law of iterated expectation and cancels the $\Pr(V=1)$ terms in the numerator and denominator. The fourth applies the definition of the generalized propensity score $\pi_i(X)$. The fifth multiplies by one. The sixth reverses the law of iterated expectations and applies consistency. 

    We now show that, under assumptions \ref{app_ass1} - \ref{app_ass4}, $\Psi^0_{om}$ and $\Psi^0_{om}$, and therefore by extension $\Psi$, is identified by 
    \begin{equation}
        \Psi_{om} \equiv \dfrac{\Pr(I = 2, T = 1 | V = 1)}{E\left\{\Pr(I = 2, T = 1 | V = 0, X) \exp\{\alpha_1(X)\} \Big| V = 1 \right\}}
    \end{equation}
    and 
    \begin{equation}
        \Psi_{ipw} \equiv \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2, T = 1) \dfrac{\pi_1(X)}{1 - \pi_1(X)}\right\}}
    \end{equation}
    under cohort sampling $O_{cohort}$ and where $\alpha_1(X) = \log \dfrac{\Pr(I = 1, T = 1 | V = 1, X)}{\Pr(I = 1, T = 1 | V = 0, X)}$ and $\pi_1(X) = \Pr(V = 1| I = 1, T = 1, X)$ are both written in terms of observable quantities.
    
    For the first expression, note that 
    \begin{align*}
        \dfrac{\Pr(I^0 = 2, T^0=1 | V = 1, X)}{\Pr(I^0 = 2, T^0=1 | V = 0, X)} &= \dfrac{\Pr(I^0 = 1, T^0=1 | V = 1, X)}{\Pr(I^0 = 1, T^0=1 | V = 0, X)} \\
        &= \dfrac{\Pr(I^1 = 1, T^1=1 | V = 1, X)}{\Pr(I^0 = 1, T^0=1 | V = 0, X)} \\
        &= \dfrac{\Pr(I = 1, T = 1 | V = 1, X)}{\Pr(I = 1, T = 1 | V = 0, X)} \\
    \end{align*}
    where the first line is by Assumption \ref{app_ass3} and mutual exclusivity of test-negative and test-positive illnesses. The second line is by Assumption \ref{app_ass2}. And the last line applies Assumption \ref{app_ass1}. This implies
    \begin{equation*}
        \alpha_2^0(X) = \alpha_1(X)
    \end{equation*}
    and, therefore, combining with the derivation of Proposition \ref{prop1}, we have
    \begin{align*}
        \Psi &= \dfrac{\Pr(I = 2, T = 1 | V = 1)}{E\left\{ \exp\{\alpha^0_2(X)\} \Pr(I = 2, T = 1 | V = 0, X) \Big| V = 1 \right\}} \\
        &= \dfrac{\Pr(I = 2, T = 1 | V = 1)}{E\left\{ \exp\{\alpha_1(X)\} \Pr(I = 2, T = 1 | V = 0, X) \Big| V = 1 \right\}}.
    \end{align*}
    
    For the second expression, note first that, by the invariance of odds ratios, Assumptions \ref{app_ass2} and \ref{app_ass3} imply
    \begin{equation*}
        \dfrac{\Pr(V = 1 | I^0 = 2, T^0=1, X)}{\Pr(V = 0 | I^0 = 2, T^0=1, X)} = \dfrac{\Pr(V = 1 | I = 1, T=1, X)}{\Pr(V = 0 | I = 1, T=1, X)}
    \end{equation*}
    and by consequence 
    \begin{equation*}
        \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)} = \dfrac{\pi_1(X)}{1 - \pi_1(X)}.
    \end{equation*}
    Thus, again, continuing the derivation in Proposition \ref{prop1}, we have 
    \begin{align*}
        \Psi &= \dfrac{E\{V \mathbbm 1 (I = 2, T=1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T=1) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T=1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T=1) \dfrac{\pi_1(X)}{1-\pi_1(X)}\right\}}.
    \end{align*}
    \end{proof}
    \newpage

    \subsection{Proof of Proposition \ref{prop2}}\label{sec:proof2}
    % \begin{lemma}\label{lemma2}
    % Under selection $S = \mathbbm 1(I\neq 0, T = 1)$, $\Psi_{om}$ and $\Psi_{ipw}$ are equivalent to 
    % \begin{equation}
    %     \Psi_{om}^* = \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{  \Pr(I^* = 1 | S = 1, V = 0, X) \exp\{\alpha^*_1(X)\}\Big| S = 1, V = 1 \right\}}
    % \end{equation}
    % and 
    % \begin{equation}
    %     \Psi_{ipw}^* = \dfrac{E\{VI^*|S =1\}}{E\left\{ (1 - V) I^* \dfrac{\pi^*_0(X)}{1 - \pi^*_0(X)} \bigg| S = 1\right\}}
    % \end{equation}
    % where $\alpha^*_1(X) = \log \dfrac{\Pr(I^* = 0 | S = 1, V = 1, X)}{\Pr(I^* = 0| S = 1, V = 0, X)}$ and $\pi^*_0(X) = \Pr(V = 1| S = 1, I^* = 0, X)$. 
    % \end{lemma}
    
    \begin{proof}
    For the first expression we have,
        \begin{align*}
        \Psi_{om} &= \dfrac{\Pr(I=2, T = 1|V=1)}{E\left\{ \dfrac{\Pr(I = 1, T = 1 | V = 1, X)}{\Pr(I = 1, T = 1 | V = 0, X)}\Pr(I = 2, T = 1 | V = 0, X) \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(I = 2, T = 1 | V = 1)}{\int \dfrac{\Pr(I = 1, T = 1 | V = 1, x)}{\Pr(I = 1, T = 1 | V = 0, x)} \Pr(I = 2, T = 1 | V = 0, x) f(x | V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)\Pr(T = 1 | V = 1)}{\int \dfrac{\Pr(I = 1 | T = 1, V = 1, x)}{\Pr(I = 1 | T = 1, V = 0, x)} \Pr(I = 2 | T = 1, V = 0, x) \Pr(T = 1 | V = 1, x) f(x | V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)\Pr(T = 1 | V = 1)}{\int \dfrac{\Pr(I = 1 | T = 1, V = 1, x)}{\Pr(I = 1 | T = 1, V = 0, x)} \Pr(I = 2 | T = 1, V = 0, x) \Pr(T = 1 | V = 1) f(x | T =1, V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{\int \dfrac{\Pr(I = 1 | T = 1, V = 1, x)}{\Pr(I = 1 | T = 1, V = 0, x)} \Pr(I = 2 | T = 1, V = 0, x) f(x | T =1, V = 1) dx} \\
        &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{E\left\{ \dfrac{\Pr(I = 1 | T = 1, V = 1, X)}{\Pr(I = 1 | T = 1, V = 0, X)} \Pr(I = 2 | T = 1, V = 0, X) \bigg| T = 1, V = 1\right\}} \\
        &= \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I^* = 0 | S = 1, V = 1, X)}{\Pr(I^* = 0 | S = 1, V = 0, X)} \Pr(I^* = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} \\
        &= \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \Pr(I^* = 1 | S = 1, V = 0, X) \exp\{\alpha^*_1(X)\}\Big| S = 1, V = 1 \right\}}.
    \end{align*}
    The first line restates the definition of $\Psi_{om}$. The second applies the definition of conditional expectation. The third factors the joint probabilities. The fourth applies Bayes theorem, i.e. 
    \begin{equation*}
        f(x | V = 1) = \dfrac{f(x | T = 1, V = 1)\Pr(T = 1 | V = 1)}{\Pr(T = 1 | V = 1, x)}.
    \end{equation*}
    The fifth cancels the $\Pr(T = 1 |V = 1)$ terms in the numerator and denominator. The sixth applies the definition of conditional expectation. The seventh applies the sampling design $S = \mathbbm 1 (T = 1, I \neq 0)$ and the perfect test $I^* = \mathbbm 1(I = 2)$.
    
    For the second expression we have,
    \begin{align*}
        \Psi_{ipw} &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{ (1 - V) \mathbbm 1(I = 2, T = 1) \dfrac{\pi_1(X)}{1 - \pi_1(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2) | T = 1\} \Pr(T = 1)}{E\left\{ (1 - V) \mathbbm 1(I = 2) \dfrac{\pi_1(X)}{1 - \pi_1(X)} \bigg| T = 1\right\} \Pr(T = 1)} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2) | T = 1\}}{E\left\{ (1 - V) \mathbbm 1(I = 2) \dfrac{\pi_1(X)}{1 - \pi_1(X)} \bigg| T = 1\right\}} \\
        &= \dfrac{E\{VI^*|S =1\}}{E\left\{ (1 - V) I^* \dfrac{\pi^*_0(X)}{1 - \pi^*_0(X)} \bigg| S = 1\right\}}.
    \end{align*}
    The first line restates the definition of $\Psi_{ipw}$. The second applies the law of iterated expectations. The third cancels the $\Pr(T = 1)$ terms in the numerator and denominator. The fourth applies the sampling design $S = \mathbbm 1 (T = 1, I \neq 0)$ and the perfect test $I^* = \mathbbm 1(I = 2, T=1)$.
    \end{proof}
    
    \newpage
    \section{Plug-in Estimation}
    The identified expressions for $\Psi^*_{om}$ and $\Psi^*_{ipw}$ in Proposition \ref{prop2} suggest two plug-in estimators for the causal risk ratio among the vaccinated $\Psi$.  An estimator based on modeling the outcome
    \begin{equation}
        \widehat{\Psi}_{om}^* = \dfrac{\sum_{i=1}^n V_i I^*_i}{\sum_{i=1}^n V_i \mu_0(X_i)\dfrac{1 - \mu_1(X_i)}{1 - \mu_0(X_i)}},
    \end{equation}
    and an inverse probability weighting estimator
    \begin{equation}
        \widehat{\Psi}_{ipw}^* = \dfrac{\sum_{i=1}^n V_i I^*_i}{\sum_{i=1}^n (1 - V_i) I^*_i \dfrac{\pi(X_i)}{1 - \pi(X_i)}},
    \end{equation}
    where 
    \begin{align*}
        \pi(X) &= \Pr(V=1\mid S=1, I^*=0, X) \\
        \mu_v(X) &= \Pr(I^*=1\mid S=1, V=v, X)
    \end{align*}
    are nuisance functions. Here, we discuss inference for $\widehat{\Psi}_{om}^*$ and $\widehat{\Psi}_{ipw}^*$ under correctly specified models for $\pi(X)$ or $\mu_v(X)$. 
    \newpage
    \section{Efficiency Theory}\label{sec:eif}
    In this section, we start by deriving the efficient influence function (EIF) for $\Psi$. We then obtain an estimator of $\Psi$ based on the EIF and establish the conditions under which the estimator is $\sqrt{n}$-consistent and asymptotically normal. We then evaluate the robustness of the estimator to misspecification of the nuisance functions. 
    
    \subsection{The efficient influence function}
    To derive the efficient influence function (EIF) for $\Psi$, we use the point-mass approach suggested in \textcite{hines_demystifying_2022}. The 
    \begin{lemma}
        The efficient influence function for $\Psi^*$ is given by 
        \begin{align*}
         \chi(P, O) &= \dfrac{1}{\sigma \Delta} \Bigg(S V I^* - \Psi(P)  \times \bigg[S(1 - V)\{I^* - \mu_0(X)\}\dfrac{\pi_0(X)\{1 - \mu_1(X)\}}{\{1 - \pi_0(X)\}\{1 - \mu_0(X)\}^2}\\
        &\qquad + S V\{1-I^*\}\dfrac{\mu_0(X)}{1 - \mu_0(X)}\bigg] \Bigg)
    \end{align*}
    where 
    \begin{align*}
        \pi_i(X) &\equiv \Pr(V = 1 | S = 1, I^* = i, X), \\
        \mu_v(X) &\equiv \Pr(I^* = 1 | S = 1, V = v, X), \\
        \sigma &\equiv \Pr(S = 1, V = 1),
    \end{align*}
    and 
    \begin{equation*}
        \Delta \equiv E\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right].
    \end{equation*}
    \end{lemma}

    \begin{proof}
    Let $\Psi(P)$ be the target parameter under true law of the observed data $P$. To derive the EIF, we assume we have access to iid observations $O \equiv (X, V, S, I^*)$ but show below that the resulting estimator is still identifiable under TND sampling. From the identifiability results in Lemma \ref{lemma1}, we have that 
    % $$\Psi(P) = \dfrac{E(I^*\mid S=1, V=1)}{E\left\{\dfrac{\Pr(I^*=1\mid S=1, V=0, X)\Pr(V=1\mid S=1, I^*=0, X)}{\Pr(V=0\mid S=1, I^*=0, X)}\mid S=1, V=1\right\}}.$$ 
    $$\Psi(P) = \dfrac{E_P(I^* = 1 | S = 1, V = 1)}{E_P\left\{\dfrac{E_P(I^*\mid S=1, V=0, X)E_P(1 - I^*\mid S=1, V=1, X)}{E_P(1 - I^*\mid S=1, V=0, X)} \bigg| S=1, V=1\right\}}.$$ 
    
    Define $P_t$ as a parametric submodel indexed by $t \in [0,1]$ such that
    $$P_t = t \widetilde{P} + (1 - t)P$$
    where $\widetilde{P}$ is smoothed parametric estimate of $P$ and note that $P_0 = P$. To find the influence function we will use the fact that if we perturb the target in direction of a point mass $\widetilde{o} = (\widetilde{i}^*, \widetilde{s}, \widetilde{v}, \widetilde{x})$ of $\widetilde{P}$
    $$ \chi(P, \widetilde{o}) = \frac{d}{dt} \Psi(P_t)\bigg\vert_{t=0}$$
    where the right-hand side is the so-called the G\^{a}teaux derivative. Note that
    \begin{align*}
        \dfrac{d}{dt}E_{P_t}(I^*\mid S=1, V=1)\Big\rvert_{t=0} &= \int i^* \dfrac{d}{dt}f_{P_t}(i^*\mid S=1, V=1)\Big\rvert_{t=0} \,di^*\\
        &= \dfrac{\mathbbm 1_{\tilde s,\tilde v}(1, 1)}{f_{S, V}(1, 1)}\{\tilde i^* - E_P(I^*\mid S=1, V=1)\}\\
        &= \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde i^* - E_P(I^*\mid S=1, V=1)\},
    \end{align*}
    and similarly
    \begin{align*}
        \dfrac{d}{dt}E_{P_t}(I^*\mid S=1, V=0, X=x)\Big\rvert_{t=0} &= \dfrac{\tilde s(1-\tilde v)\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 0, x)}\{\tilde i^* - \mu_0(X)\}\\
         \dfrac{d}{dt}E_{P_t}(1-I^*\mid S=1, V=0, X=x)\Big\rvert_{t=0} &= -\dfrac{\tilde s(1-\tilde v)\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 0, x)}\{\tilde i^* -\mu_0(X)\}\\
         \dfrac{d}{dt}E_{P_t}(1-I^*\mid S=1, V=1, X=x)\Big\rvert_{t=0} &= -\dfrac{\tilde s\tilde v\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 1, x)}\{\tilde i^* - \mu_1(X)\}.
    \end{align*}
We also have
$$\dfrac{d}{dt}f_{P_t}(x\mid S=1, V=1)\Big\vert_{t=0} = \dfrac{\tilde s \tilde v}{f_{S, V}(1, 1)}\{\mathbbm 1_{\tilde x}(x)-f_X(x\mid S=1, V=1)\}.$$
Then, applying the chain rule, the pathwise derivative of $\Psi_t$ wrt $t$ is
\begin{align*}
   &\frac{d}{dt} \Psi(P_t)\bigg\vert_{t=0} \\
   &= \frac{d}{dt} \left[\dfrac{E_P(I^* = 1 | S = 1, V = 1)}{E_P\left\{\dfrac{E_P(I^*\mid S=1, V=0, X)E_P(1 - I^*\mid S=1, V=1, X)}{E_P(1 - I^*\mid S=1, V=0, X)} \bigg| S=1, V=1\right\}}\right] \vast\vert_{t=0}  \\
   &= \dfrac{\dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde i^* - E_P(I^*\mid S=1, V=1)\}}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{E_P(I^*\mid S=1, V=1)}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]^2} \\
   &\qquad \times \bigg( \dfrac{\tilde s(1 - \tilde v)}{f_{S, V}(1, 0)}\{\tilde i^* - \mu_0(\tilde x)\}\dfrac{f_X(\tilde x\mid S=1, V=1)\{1-\mu_1(\tilde x)\}}{f_X(\tilde x\mid S=1, V=0)\{1-\mu_0(\tilde x)\}} \\
   & \qquad \qquad - \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde i^* - \mu_1(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} + \dfrac{\tilde s(1 - \tilde v)}{f_{S, V}(1, 0)}\{\tilde i^* - \mu_0(\tilde x)\} \\
   &\qquad \qquad \times \dfrac{f_X(\tilde x\mid S=1, V=1)\{1-\mu_1(\tilde x)\}\mu_0(\tilde x)}{f_X(\tilde x\mid S=1, V=0)\{1-\mu_0(\tilde x)\}^2}+\dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\dfrac{\mu_0(\tilde x)\{1-\mu_1(\tilde x)\}}{1-\mu_0(\tilde x)} \\
   & \qquad \qquad - \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]\bigg) 
\end{align*}
Collecting terms, and noting that $\sigma = f_{S,V}(1, 1)$ and
$$
\dfrac{f_X(\tilde x\mid S=1, V=1)}{f_X(\tilde x\mid S=1, V=0)f_{S,V}(1, 0)} = \dfrac{\pi(\tilde x)}{\{1 - \pi(\tilde x)\} \sigma}
$$
we have
\begin{align*}    
    &= \dfrac{\dfrac{\tilde s\tilde v}{\sigma}\{\tilde i^* - E_P(I^*\mid S=1, V=1)\}}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{E_P(I^*\mid S=1, V=1)}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]^2} \\
    &\qquad \times \bigg( \dfrac{\tilde s(1 - \tilde v)}{\sigma}\{\tilde i^* - \mu_0(\tilde x)\}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}} - \dfrac{\tilde s\tilde v}{\sigma}\{\tilde i^* - \mu_1(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \\ 
    &\qquad \qquad + \dfrac{\tilde s( 1 -\tilde v)}{\sigma}\{\tilde i^* - \mu_0(\tilde x)\}\dfrac{\pi(\tilde x)\{1 - \mu_1(\tilde x)\}\mu_0(\tilde x)}{\{1 - \pi(\tilde x)\}\{1 - \mu_0(\tilde x)\}^2} + \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)\{1 - \mu_1(\tilde x)\}}{1 - \mu_0(\tilde x)} \\
    &\qquad \qquad - \dfrac{\tilde s\tilde v}{\sigma}E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]\bigg)
\end{align*}
The result can be further simplified as
\begin{align*}
    &= \dfrac{\tilde s\tilde v\tilde i^*}{\sigma E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{E_P(I^*\mid S=1, V=1)}{E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)}\bigg| S=1, V=1\right]^2} \\
    &\qquad \times \bigg[ \dfrac{\tilde s(1 - \tilde v)}{\sigma}\{\tilde i^* - \mu_0(\tilde x)\}\dfrac{\pi(\tilde x)\{1 - \mu_1(\tilde x)\}}{\{1 - \pi(\tilde x)\}\{1 - \mu_0(\tilde x)\}^2} + \dfrac{\tilde s \tilde v}{\sigma}\{1-\tilde i^*\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)}\bigg]
\end{align*}
Therefore the efficient influence function for $\Psi$ is 
\begin{align*}
     \chi(P, O) &= \dfrac{S V I^*}{\sigma E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{\Psi(P)}{E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)}\bigg| S=1, V=1\right]} \\
    &\qquad \times \bigg[ \dfrac{S(1 - V)}{\sigma}\{I^* - \mu_0(X)\}\dfrac{\pi(X)\{1 - \mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \dfrac{S V}{\sigma}\{1-I^*\}\dfrac{\mu_0(X)}{1 - \mu_0(X)}\bigg] 
\end{align*}
    \end{proof}

    
% \begin{align*}
%     &\dfrac{\tilde s (1 - \tilde v)}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}}\left[\{\tilde i^* - \mu_0(\tilde x)\}\dfrac{1-\mu_0(\tilde x)}{1-\mu_0(\tilde x)} + \{\tilde i^* - \mu_0(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1-\mu_0(\tilde x)}\right]\\
%     &=  \dfrac{\tilde s (1 - \tilde v)}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}}\bigg[\{\tilde i^* - \mu_0(\tilde x) - \tilde i^* \mu_0(\tilde x) + \mu_0(\tilde x)^2\}\dfrac{1}{1-\mu_0(\tilde x)} \\
%     &\qquad + \{\tilde i^* \mu_0(\tilde x) - \mu_0(\tilde x)^2\}\dfrac{1}{1-\mu_0(\tilde x)}\bigg] \\
%     &= \dfrac{\tilde s (1 - \tilde v)}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}^2}\{\tilde i^* - \mu_0(\tilde x)\}
% \end{align*}

% \begin{align*}
%     &\dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)\{1 - \mu_1(\tilde x)\}}{1 - \mu_0(\tilde x)} - \dfrac{\tilde s\tilde v}{\sigma}\{\tilde i^* - \mu_1(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \\
%     &= \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \bigg\{1 - \mu_1(\tilde x) - \tilde i + \mu_1(\tilde x)\bigg\} \\
%     &= \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \{1 - \tilde i\}
% \end{align*}

% \begin{align*}
%     \dfrac{\tilde s}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}^2}\{\tilde i^* - \mu_0(\tilde x)\} - \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}^2}\{\tilde i^* - \mu_0(\tilde x)\} + \dfrac{\tilde s \tilde v}{\sigma} \dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} - \dfrac{\tilde s \tilde v}{\sigma} \tilde i^* \dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)}
% \end{align*}


\subsection{An estimator based on the efficient influence function}
As discussed in \textcite{hines_demystifying_2022}, an estimator for $\Psi$ that removes plug-in bias may be found as the solution to the following estimating equation 
\begin{equation*}
    0 = \dfrac{1}{n} \sum_{i=1}^n \chi(P_n, O_i).
\end{equation*}
For convenience, first define 
\begin{equation*}
    \Delta = E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right].
\end{equation*}
We solve 
\begin{align*}
    &\dfrac{1}{\Delta \sigma} \bigg(\dfrac{1}{n} \sum_{i=1}^n  S_i V_i I_i^* - \dfrac{1}{n} \sum_{i=1}^n \Psi(P_n) \bigg[  S_i (1 - V_i)\{I_i^* - \mu_0(X_i)\}\dfrac{\pi(X_i)\{1 - \mu_1(X_i)\}}{\{1 - \pi(X_i)\}\{1 - \mu_0(X_i)\}^2} \\
    &\qquad +  S_i V_i\{1-I_i^*\}\dfrac{\mu_0(X_i)}{1 - \mu_0(X_i)}\bigg]\bigg) = 0
\end{align*}
for $\Psi(P_n)$ and obtain the estimator
\begin{equation*}
    \widehat{\Psi}_{dr} = \dfrac{\sum_{i=1}^n   S_i V_i I_i^*}{\sum_{i=1}^n S_i (1 - V_i)\{I_i^* - \mu_0(X_i)\}\dfrac{\pi(X_i)\{1 - \mu_1(X_i)\}}{\{1 - \pi(X_i)\}\{1 - \mu_0(X_i)\}^2} +  S_i V_i\{1-I_i^*\}\dfrac{\mu_0(X_i)}{1 - \mu_0(X_i)}}.
\end{equation*}

Now consider the case that we have $m$ observations from $O_{tnd} \equiv (X, V, S=1, I^*)$, i.e. the sampling design of the test-negative study. Note that summands in numerator and denominator are zero when $S_i=0$ such that $\widehat{\Psi}_{dr} = \widehat{\Psi}^*_{dr}$, where
\begin{equation*}
    \widehat{\Psi}^*_{dr} = \dfrac{\sum_{i=1}^{m}  V_i I_i^*}{\sum_{i=1}^m(1 - V_i)\{I_i^* - \mu_0(X_i)\}\dfrac{\pi(X_i)\{1 - \mu_1(X_i)\}}{\{1 - \pi(X_i)\}\{1 - \mu_0(X_i)\}^2} + V_i\{1-I_i^*\}\dfrac{\mu_0(X_i)}{1 - \mu_0(X_i)}}.
\end{equation*}
and $\widehat{\Psi}^*_{dr}$ is estimable from using $O_{tnd}$.
% Using the plug-in results above an initial estimator of $\Psi$ is
% \begin{equation*}
%         \widehat{\Psi}_{0} = \dfrac{\sum_{i=1}^n V_i I^*_i}{\sum_{i=1}^n V_i \mu_0(X_i)\dfrac{1 - \mu_1(X_i)}{1 - \mu_0(X_i)}},
%     \end{equation*}
% As defined in \textcite{hines_demystifying_2022}, the one-step corrected estimator is given by
% \begin{equation*}
%     \widehat{\Psi}_{1} = \widehat{\Psi}_{0} + \dfrac{1}{n} \sum_{i=1}^n \chi(P_n, O_i) 
% \end{equation*}
% which is equal to
% \begin{align*}
%     \widehat{\Psi}_{1} &= \widehat{\Psi}_{0} + \dfrac{ \sum_{i=1}^n V_i I_i^*}{ \sum_{i=1}^n V_i \mu_0(X_i) \dfrac{\{1 - \mu_1(X_i)\}}{1 - \mu_0(X_i)}} - \dfrac{E_P(I^*\mid S=1, V=1)}{\left[E_P\left\{\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)}\bigg| S=1, V=1\right\}\right]^2} \\
%     &\qquad \times \bigg[ \dfrac{S(1 - V)}{\sigma}\{I^* - \mu_0(X)\}\dfrac{\pi(X)\{1 - \mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \dfrac{S V}{\sigma}\{1-I^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg]
% \end{align*}
% Specify the propensity score model $\pi(x;\alpha)$ and the outcome regression model $\mu_v(x;\beta)$. The nuisance parameters may be estimated by solving estimating equations
%     \begin{align*}
%         \sum_{j=1}^n X_j\{V_j - \pi(X_j;\alpha)\} &= 0;\\
%         \sum_{j=1}^n \begin{pmatrix}
%             X_j\\V_j
%         \end{pmatrix}\{I^*_j - \mu_{V_j}(X_j;\beta)\}=0.
%     \end{align*}
%     Denote the resulting estimators as $\hat\alpha$, $\hat\beta$.
%     \item Let $\hat \Pi$ be an estimator for $\Pi\equiv \int \dfrac{\mu_0(x)\{1 - \mu_1(x)\}}{1 - \mu_0(x)}\pi(x)f_s(x)dx$, for example,
%     $$\hat\Pi = \dfrac{1}{n}\sum_{j=1}^n \dfrac{\mu_0(X_j;\hat\beta)\{1 - \mu_1(X_j;\hat\beta)\}}{1 - \mu_0(X_j;\hat\beta)}\pi(X_j;\hat \alpha).$$

%     The initial estimator of $\Psi$ is
%     $$\hat\Psi_0 = \dfrac{\dfrac{1}{n}\sum_{j=1}^n I^*_jV_j}{\hat\Pi}.$$
%     \item The one-step corrected estimator is
%     \begin{align*}
%         \hat\Psi_1 &= \hat\Psi_0 +\dfrac{1}{n}\sum_{i=1}^n \bigg\{\dfrac{V_jI^*_j}{\hat\Pi}-\\&\qquad \dfrac{\dfrac{1}{n}\sum_{k=1}^n I^*_kV_k}{\hat\Pi^2}\bigg[S_j(1 - V_j)\{I^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + S_jV_j\{1-I^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]\bigg\}\\
%         &= 2\hat\Psi_0-\dfrac{\hat\Psi_0}{\hat\Pi}\dfrac{1}{n}\sum_{j=1}^n\bigg[S_j(1 - V_j)\{I^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + S_jV_j\{1-I^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]
%     \end{align*}

\subsection{Proof of double robustness}\label{sec:dr}
To prove the unique double robustness property of $\widehat{\Psi}^*_{dr}$, it suffices to show that 
\begin{align*}
    E\bigg[(1 - V_i)\{I_i^* - \mu^\dagger_0(X_i)\}\dfrac{\pi^\dagger(X_i)\{1 - \mu^\dagger_1(X_i)\}}{\{1 - \pi^\dagger(X_i)\}\{1 - \mu^\dagger_0(X_i)\}^2} + V_i\{1-I_i^*\}\dfrac{\mu^\dagger_0(X_i)}{\{1 - \mu^\dagger_0(X_i)\}}\bigg| S=1\bigg] = \dfrac{\Delta}{\sigma}
\end{align*}
where $\Delta$ is defined as previously, if either:
\begin{enumerate}
    \item $\mu_1^\dagger(X) = \mu_1(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$
    \item $\pi^\dagger(X) = \pi(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$
\end{enumerate}
hold.
\vspace{2em}

\begin{proof}
    Note that, by the invariance of the odds ratio, we have that
    \begin{equation*}
        \dfrac{1 - \mu_1(X)}{1 - \mu_0(X)} = \dfrac{\pi_0(X)}{1 - \pi_0(X)}.
    \end{equation*}
    First, suppose that $\mu_1^\dagger(X) = \mu_1(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$. Then we have that 
    \begin{align*}
            &E\bigg[(1 - V)\{I^* - \mu^\dagger_0(X)\}\dfrac{\pi^\dagger(X)\{1 - \mu^\dagger_1(X)\}}{\{1 - \pi^\dagger(X)\}\{1 - \mu^\dagger_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu^\dagger_0(X)}{\{1 - \mu^\dagger_0(X)\}}\bigg| S=1\bigg]\\
            &= E\bigg[(1 - V)\{I^* - \mu_0(X)\}\dfrac{\pi^\dagger(X)\{1 - \mu_1(X)\}}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg| S=1\bigg] \\
            &= E\bigg[\dfrac{(1 - V)\pi(X)}{1 - \pi(X)}\{I^* - \mu_0(X)\}\dfrac{\pi^\dagger(X)}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}} + V\{1-I^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg| S=1\bigg] \\
            &= E\bigg[(1 - V)\dfrac{\pi(X)}{1 - \pi(X)}\dfrac{\pi^\dagger(X)}{1 - \pi^\dagger(X)} \dfrac{I^*}{1 - \mu_0(X)} - (1 - V)\dfrac{\pi(X)}{1 - \pi(X)}\dfrac{\pi^\dagger(X)}{1 - \pi^\dagger(X)} \dfrac{\mu_0(X)}{1 - \mu_0(X)} \\
            &\qquad +  V\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}} - I^*V \dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}} \bigg| S=1\bigg] \\
            %&= E\bigg[E(1 - V | S=1, I^*, X)\{I^* - \mu_0(X)\}\dfrac{\pi^\dagger(X)\{1 - \mu_1(X)\}}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg| S=1\bigg] \\
            &= E\bigg[(1 - V) \dfrac{\pi(X)}{1 - \pi(X)}\dfrac{\dot\pi(X)}{1 - \dot\pi(X)}\bigg| S=1, I^*=1\bigg]\Pr(I^*=1\mid  S=1) \\
            &\qquad - E\bigg[\dfrac{(1 - V)\pi(X)}{1 - \pi(X)}\dfrac{\pi^\dagger(X)\mu_0(X)}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}} + \dfrac{\mu_0(X)}{1-\mu_0(X)}\bigg| S = 1, I^* = 0 \bigg]\Pr(I^*=0\mid  S=1) \\
            &\qquad + E\bigg[\{1-I^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\bigg| S = 1, V = 1\bigg] \\
            &= E\bigg[E \{I^*-  \mu_0(X)\mid S=1, V=0, X\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} \mid S=1, V = 0\bigg] \\
            &\qquad \times \Pr(V=0\mid  S=1) + E\bigg[\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid  S=1, V = 1\bigg] \Pr(V =1 \mid S = 1)\\
            &= 0 + \Pi = \Pi
        \end{align*}
    Second, suppose that $\pi^\dagger(X) = \pi(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$. 
\end{proof}
    \begin{enumerate}
        \item If $\dot\mu_v(\cdot)=\mu_v(\cdot)$ a.s. for $v=0,1$, then
        
    \item If $\dot\pi(\cdot)=\pi(\cdot)$ and $\dot\mu_0(\cdot)=\mu_0(\cdot)$ a.s., then 
\begin{align*}
    &E\bigg[ (1-V)\{I^* -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    =& E\bigg[ \{1 - \pi(X)\}\{\mu_0(X) -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \pi(X)\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    &= \Delta
\end{align*}


    \end{enumerate}
    
    \newpage
    \section{Identification and direct effects of vaccination}
    In the main text, we present results for the causal effect of vaccination on \textit{medically-attended illness} which we define as
    \begin{equation*}
        
    \end{equation*}
    
    
    
    
    \section{Additional results}
    \subsection{Example mechanisms where key assumptions are violated}
    
    \begin{figure}[p]
    \centering
    \begin{subfigure}{0.8\linewidth}
        \centering
        \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
            \tikzstyle{every state}=[
              draw = none,
              fill = none
            ]
            \node[state] (x) {$X$};
            \node[state] (v) [right of=x] {$V$};
            \node[state] (i) [right of=v] {$I_2$};
            \node[state] (t) [right of=i] {$T$};
            \node[state] (is) [right of=t] {$I^*$};
            \node[state] (i1) [below of=i] {$I_1$};
            \node[state] (u) [below of=x] {$U$};
            \node[state] (v1) [below of=v] {$V_1$};
   
            \path[->] (x) edge node {} (v);
            \path[->] (x) edge [out=45, in=135] node {} (i);
    
            \path[->] (v) edge node {} (i);
            
            \path[->] (i) edge node {} (t);
            \path[->] (i1) edge node {} (t);
            \path[->] (i1) edge node {} (is);

            \path[->] (x) edge [out=45, in=135] node {} (t);
    
            \path[->] (t) edge node {} (is);
            \path[->] (v1) edge node {} (i1);

            \path[->] (i) edge [out=45, in=135] node {} (is);
            \path[->] (i1) edge [dashed] node {} (i);

    
            \path[->] (u) edge node {} (v1);
            \path[->] (u) edge node {} (v);
            % \path[->] (u) edge [line width=2pt] node {} (i);
            % \path[->] (u) edge [line width=2pt] node {} (t);
            % \path[->] (u) edge [line width=2pt] node {} (i1);
            \end{tikzpicture}
        \caption{Correlated vaccination behavior.}
        \end{subfigure}
        \begin{subfigure}{0.8\linewidth}
            \centering
            \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.25cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
                \tikzstyle{every state}=[
                  draw = none,
                  fill = none
                ]
                \node[state] (x) {$X$};
                \node[state] (v) [right of=x] {$V$};
                \node[state] (i) [right of=v] {$I_2$};
                \node[state] (t) [right of=i] {$T$};
                \node[state] (is) [right of=t] {$I^*$};
                \node[state] (i1) [below of=i] {$I_1$};
                \node[state] (u) [below of=v] {$U$};
       
                \path[->] (x) edge node {} (v);
                \path[->] (x) edge [out=45, in=135] node {} (i);
        
                \path[->] (v) edge node {} (i);
                
                \path[->] (i) edge node {} (t);
                \path[->] (i1) edge node {} (t);
                \path[->] (i1) edge node {} (is);
    
                \path[->] (x) edge [out=45, in=135] node {} (t);
        
                \path[->] (t) edge node {} (is);
    
                \path[->] (i) edge [out=45, in=135] node {} (is);    
        
                \path[->] (u) edge node {} (v);
                \path[->] (u) edge node {} (i);
                \path[->] (u) edge node {} (t);
                \path[->] (u) edge node {} (i1);
                \end{tikzpicture}
            \caption{$I_1$ and $I_2$ are not mutually exclusive.}
            \end{subfigure}
            \begin{subfigure}{0.8\linewidth}
            \centering
            \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
            \tikzstyle{every state}=[
              draw = none,
              fill = none
            ]
            \node[state] (x) {$X$};
            \node[state] (v) [right of=x] {$V$};
            \node[state] (i) [right of=v] {$I_2$};
            \node[state] (t) [right of=i] {$T$};
            \node[state] (is) [right of=t] {$I^*$};
            \node[state] (i1) [below of=i] {$I_1$};
            \node[state] (u) [below of=x] {$U$};
            \node[state] (v1) [below of=v] {$V_1$};
   
            \path[->] (x) edge node {} (v);
            \path[->] (x) edge [out=45, in=135] node {} (i);
    
            \path[->] (v) edge node {} (i);
            
            \path[->] (i) edge node {} (t);
            \path[->] (i1) edge node {} (t);
            \path[->] (i1) edge node {} (is);

            \path[->] (x) edge [out=45, in=135] node {} (t);
    
            \path[->] (t) edge node {} (is);
            \path[->] (v1) edge node {} (i1);

            \path[->] (i) edge [out=45, in=135] node {} (is);
            \path[->] (i1) edge [dashed] node {} (i);

    
            \path[->] (u) edge node {} (v1);
            \path[->] (u) edge node {} (v);
            % \path[->] (u) edge [line width=2pt] node {} (i);
            % \path[->] (u) edge [line width=2pt] node {} (t);
            % \path[->] (u) edge [line width=2pt] node {} (i1);
            \end{tikzpicture}
        \caption{Correlated vaccination behavior.}
        \end{subfigure}
    % \begin{subfigure}{0.8\linewidth}
    %     \centering
    %     \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.25cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
    %         \tikzstyle{every state}=[
    %           draw = none,
    %           fill = none
    %         ]
    %         \node[state] (x) {$X$};
    %         \node[state] (v) [right of=x] {$V$};
    %         \node[state] (i) [right of=v] {$\mathbbm{1}(I = 2)$};
    %         \node[state] (t) [right of=i] {$T$};
    %         \node[state] (is) [right of=t] {$I^*$};
    %         \node[state] (i1) [below of=i] {$\mathbbm{1}(I = 1)$};
    %         \node[state] (u) [below of=v] {$U$};
    
    %         \path[->] (x) edge node {} (v);
    %         \path[->] (x) edge [out=45, in=135] node {} (i);
    
    %         \path[->] (v) edge node {} (i);
            
    %         \path[->] (i) edge node {} (t);
    %         \path[->] (i1) edge node {} (t);
    %         \path[->] (x) edge [out=45, in=135] node {} (t);
    
    %         \path[->] (t) edge node {} (is);
    
    %         \path[->] (i) edge [out=45, in=135] node {} (is);
    
    
    %         \path[->] (i1) edge node {} (i);
    
    
    %         \path[->] (u) edge node {} (x);
    %         \path[->] (u) edge node {} (v);
    %         \path[->] (u) edge node {} (i);
    %         \path[->] (u) edge node {} (t);
    %         \path[->] (u) edge node {} (i1);
            
    %         \end{tikzpicture}
    %     \caption{Splitting $I$ node to show how $I=1$ is a negative outcome control. Solid line is deterministic relationship as $I = 1$ and $I = 2$ are mutually exclusive.}
    %     \end{subfigure}
    %     \begin{subfigure}{0.8\linewidth}
    %         \centering
    %         \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.25cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
    %             \tikzstyle{every state}=[
    %               draw = none,
    %               fill = none
    %             ]
    %             \node[state] (x) {$X$};
    %             \node[state] (v) [right of=x] {$V$};
    %             \node[state] (i) [right of=v] {$I$};
    %             \node[state] (z) [right of=i] {$Z$};

    %             \node[state] (t) [right of=z] {$T$};
    %             \node[state] (is) [right of=t] {$I^*$};
    %             \node[state] (u) [below of=v] {$U$};
        
    %             \path[->] (x) edge node {} (v);
    %             \path[->] (x) edge [out=45, in=135] node {} (i);
    %             \path[->] (x) edge [out=45, in=135] node {} (t);
                
    %             \path[->] (v) edge node {} (i);
                
    %             \path[->] (i) edge node {} (z);
    %             \path[->] (i) edge [out=45, in=135] node {} (is);
        
    %             \path[->] (t) edge node {} (is);
        
    %             \path[->] (u) edge node {} (x);
    %             \path[->] (u) edge node {} (v);
    %             \path[->] (u) edge node {} (i);
    %             \path[->] (u) edge node {} (t);
                
    %             \end{tikzpicture}
    %         \caption{Causal directed-acyclic graph for the test-negative design}
    %     \end{subfigure}
    %\caption{A}\label{fig:dags}
\end{figure}
\clearpage
\subsection{What if test-positive and test-negative infections are not mutually exclusive?}
Define $I_1$ as an indicator of a symptomatic test-negative infection and $I_2$ as an indicator of a symptomatic test-positive infection. We allow for the possibility that $\Pr(I_1 = 1, I_2 = 1) > 0$, but still assume that the symptom screen is effective such that, for any individual $i$, $\mathbbm 1(I_{i,1} + I_{i,2}) = 1$ when $S_i = 1$. We use the following modified assumption set:
\begin{itemize}
    \item[(A1$^\ddagger$)] Consistency of potential outcomes. For all individuals $i$ and for $v \in \{0, 1\}$, we have $I_{2i}^v = I_{2i}$, $I_{1i}^v = I_{1i}$, and $T_i^v = T_i$ when $V_i = v$.
    \item[(A2$^\ddagger$)] No effect of vaccination on test-negative symptomatic illness  among the vaccinated. That is, $\Pr(I_1^0 = 1 | V = 1, X) = \Pr(I_1^1 = 1 | V = 1, X).$
    \item[(A3$^\ddagger$)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for test-positive and test-negative illnesses, i.e. 
        $$\beta_2(X) = \beta_1(X), $$
        $$ \text{where } \beta_i(X) = \log \frac{\Pr(I^0_i = 1, T^0 = 1 | V = 1, X)\Pr(I^0_i = 0, T^0 = 1 | V = 0, X)}{\Pr(I^0_i = 0, T^0 = 1 | V = 1, X)\Pr(I^0_i = 1, T^0 = 1| V = 0, X)}.$$
    \item[(A4$^\ddagger$)] Overlap of vaccination among test-positives and test-negatives. Define $\mathcal{S}_i(v)$ as the support of the law of $(I^v_i, T^v = 1, V = v, X)$, then for $v$ in $\{0,1\}$, then it must be that $\mathcal{S}_2(1) \subseteq \mathcal{S}_2(0)$ and $\mathcal{S}_2(v) \subseteq \mathcal{S}_1(v).$
    \item[(A5$^\ddagger$)] No direct effect of vaccination on test-seeking behavior among the vaccinated. That is, for $i$ in $\{1,2\}$, $\Pr[T^1 = 1 | I^1_i = 1, V = 1, X] = \Pr[T^0 = 1 | I^0_i = 1, V = 1, X].$
\end{itemize}

Here, we show that, under assumptions A1$^\ddagger$-A5$^\ddagger$, the causal odds ratio among the vaccinated remains identified in the test-negative design.

\begin{theorem}
    If the test-negative and test-positive illnesses are not mutually exclusive, then, under assumptions A1$^\ddagger$-A5$^\ddagger$, the causal odds ratio among the vaccinated, 
    \begin{equation*}
        \Phi_{ORV} \equiv \dfrac{\Pr(I^1_2=1, T = 1|V=1)\Pr(I^1_2=0, T = 1|V=1)}{\Pr(I^0_2=0, T = 1|V=1)\Pr(I^0_2=1, T = 1|V=1)},
    \end{equation*}
    is identified by $\Psi^*_{om}$ and $\Psi^*_{ipw}$ (as defined in Lemma \ref{lemma2}) in the test-negative design.
    \end{theorem}
    
    % \begin{proof}
    % \begin{align*}
    %     \Psi_{om}^* &= \dfrac{\Pr(I^* = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I^* = 0 | S = 1, V = 1, X)}{\Pr(I^* = 0 | S = 1, V = 0, X)} \Pr(I^* = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} \\
    %     &= \dfrac{\Pr(I_2 = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I_1 = 1, I_2 = 0  | S = 1, V = 1, X)}{\Pr(I_1 = 1, I_2 = 0 | S = 1, V = 0, X)} \Pr(I_2 = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} 
    % \end{align*}
    % \begin{align*}
    %     \Phi_{ORV} &= \dfrac{\Pr(I^1_2=1|V=1)\Pr(I^0_2=0|V=1)}{\Pr(I^1_2=0|V=1)\Pr(I^0_2=1|V=1)} \\
    %     &=\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)\Pr(I^0_2=0, T^0 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)\Pr(I^0_2=1, T^0 = 1|V=1)} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)}}{E\left\{\dfrac{\Pr(I^0_2=1, T^0 =1|V=1, X)}{\Pr(I^0_2=0, T^0 = 1|V=1, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)}}{E\left\{\dfrac{\dfrac{\Pr(I^0_1=1, T^0 = 1|V=1, X)}{\Pr(I^0_1=0, T^0 = 1|V=1, X)}}{\dfrac{\Pr(I^0_1=1, T^0 = 1|V=0, X)}{\Pr(I^0_1=0, T^0 = 1|V=0, X)}}\dfrac{\Pr(I^0_2=1, T^0 = 1|V=0, X)}{\Pr(I^0_2=0, T^0 = 1|V=0, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1| T^1 = 1, V=1)}{\Pr(I^1_2=0 | T^1 = 1, V=1)}}{E\left\{\dfrac{\dfrac{\Pr(I^0_1=1 | T^0 = 1, V=1, X)}{\Pr(I^0_1=0 | T^0 = 1, V=1, X)}}{\dfrac{\Pr(I^0_1=1 | T^0 = 1, V=0, X)}{\Pr(I^0_1=0 | T^0 = 1, V=0, X)}}\dfrac{\Pr(I^0_2=1 | T^0 = 1, V=0, X)}{\Pr(I^0_2=0 | T^0 = 1,   V=0, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1|V=1)}{\Pr(I^1_2=0|V=1)}}{E\left\{\dfrac{\Pr(I^1_1=1|V=1, X)\Pr(I^0_1=0|V=0, X)}{\Pr(I^1_1=0|V=1, X)\Pr(I^0_1=1|V=0, X)}\dfrac{\Pr(I^0_2=1|V=0, X)}{\Pr(I^0_2=0|V=0, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I_2=1|V=1)}{\Pr(I_2=0|V=1)}}{E\left\{\dfrac{\Pr(I_1=1|V=1, X)\Pr(I_1=0|V=0, X)}{\Pr(I_1=0|V=1, X)\Pr(I_1=1|V=0, X)}\dfrac{\Pr(I_2=1|V=0, X)}{\Pr(I_2=0|V=0, X)}\bigg| V = 1\right\}}
    % \end{align*}
    % For the first expression, note that by Assumption A3 and exclusivity of test-negative and test-positive illnesses
    % \begin{align*}
    %     \dfrac{\Pr(I^0 = 2 | V = 1, X)}{\Pr(I^0 = 2 | V = 0, X)} &= \dfrac{\Pr(I^0 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
    %     &= \dfrac{\Pr(I^1 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
    %     &= \dfrac{\Pr(I = 1 | V = 1, X)}{\Pr(I = 1 | V = 0, X)} \\
    % \end{align*}
    % where the first line is by Assumption A3 and exclusivity of test-negative and test-positive illnesses. The second line is by Assumption A2. And the last line applies consistency. This implies
    % \begin{equation*}
    %     \alpha_2^0(X) = \alpha_1(X)
    % \end{equation*}
    % and, therefore, combining with the derivation of Theorem 1, we have
    % \begin{align*}
    %     \Psi &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha^0_2(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}} \\
    %     &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha_1(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}}.
    % \end{align*}
    
    % For the second expression, note first that, by the invariance of odds ratios, Assumptions A2 and A3 imply
    % \begin{equation*}
    %     \dfrac{\Pr(V = 1 | I^0 = 2, X)}{\Pr(V = 0 | I^0 = 2, X)} = \dfrac{\Pr(V = 1 | I = 1, X)}{\Pr(V = 0 | I = 1, X)}
    % \end{equation*}
    % and by consequence 
    % \begin{equation*}
    %     \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)} = \dfrac{\pi_1(X)}{1 - \pi_1(X)}.
    % \end{equation*}
    % Thus, again, continuing the derivation in Theorem 1, we have 
    % \begin{align*}
    %     \Psi &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}} \\
    %     &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi_1(X)}{1-\pi_1(X)}\right\}}.
    % \end{align*}
    % \end{proof}
    \newpage

% \begin{align*}
%     \Pr[I^* = 1 | S = 1, X, V] &= \Pr[I_2 = 1 | S = 1, X, V]\\
%     \Pr[I^* = 0 | S = 1, X, V] &= \Pr[I_1 = 1 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% \begin{align*}
%     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] \\
%     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] \\
%     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
%     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V]  - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% \begin{align*}
%     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] \\
%     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] \\
%     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I_2 = 0 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
%     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I_2      = 1 | S = 1, X, V]  - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% % \begin{align*}
% %     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] + \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
% %     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% % \end{align*}
% % and 
% % \begin{align*}
% %     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[I^* = 1 | S = 1, X, V] \\
% %     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[I^* = 0 | S = 1, X, V] 
% % \end{align*}
% In this case, Assumption (4) is 
% $$OR_2(X) = OR_1(X), $$
% $$ \text{where } OR_i(X) = \frac{\Pr[I^0_i = 1, T^0 = 1 | V = 1, X]\Pr[I^0_i = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_i = 0, T^0 = 1 | V = 1, X]\Pr[I^0_i = 1, T^0 = 1| V = 0, X]}.$$
% First, we show that, when $I_1$ and $I_2$ are not mutually exclusive the causal risk ratio is not identified. Following from our previous proof, we have that 
% \begin{align*}
%     \psi_{rrv}(X) &= \phi(X) \times \dfrac{\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 0, X]}\\
%     \Pr[I_2^0 = 2, T^0 = 1 | V = 1, X] &= \dfrac{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1 | V = 0, X]}{\Pr[I_1 = 0, T = 1 | V = 1, X] \Pr[I_1 = 1, T = 1 | V = 0, X]}\\
%     & \quad \quad \times  \dfrac{\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 0, X]} \Pr[I_2 = 1, T = 1 | V = 0, X]
% \end{align*}

% Here we show that, under these conditions, the conditional odds ratio in the TND identifies the conditional causal odds ratio, i.e.
% \begin{equation*}
%      \psi_{orv}(X) &\equiv \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]}. 
% \end{equation*}

% \begin{align*}
%         \psi_{orv}(X) &= \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I^0_1 = 0, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 1, T^0 = 1 | V = 0, X]}{\Pr[I^0_1 = 1, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 0, T^0 = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I_2 = 0, T = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I_2 = 1, T = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0, T = 1 | V = 1, X]\Pr[I_1 = 1, T = 1 | V = 0, X]}{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1 | I \neq 0, T = 1, V = 1, X]\Pr[I_2 =0 |  I \neq 0, T = 1, V = 0, X]}{\Pr[I_2 = 0 |  I \neq 0, T = 1, V = 1, X]\Pr[I_2 = 1 |  I \neq 0, T = 1, V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0 |  I \neq 0, T = 1, V = 1, X]\Pr[I_1 = 1 |  I \neq 0, T = 1, V = 0, X]}{\Pr[I_1 = 1 |  I \neq 0, T = 1, V = 1, X]\Pr[I_1 = 0 |  I \neq 0, T = 1, V = 0, X]}\\
%         &=  \dfrac{\Pr[I_2 = 1 | S = 1, V = 1, X]\Pr[I_2 =0 |  S = 1, V = 0, X]}{\Pr[I_2 = 0 |  S = 1, V = 1, X]\Pr[I_2 = 1 | S = 1, V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0 |  S = 1, V = 1, X]\Pr[I_1 = 1 |  S = 1, V = 0, X]}{\Pr[I_1 = 1 |  S = 1, V = 1, X]\Pr[I_1 = 0 |  S = 1, V = 0, X]}\\
%         &=  \dfrac{\Pr[I^* = 1 | S = 1, V = 1, X]}{\Pr[I^* = 0|  S = 1, V = 1, X]} \dfrac{\Pr[I^* = 0 |  S = 1, V = 0, X]}{\Pr[I^* = 1 | S = 1, V = 0, X]}\\
% \end{align*}  
    
% Note that in this case, Assumption (4) becomes 
% \begin{itemize}
%      \item[(A4)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for all symptomatic illness regardless if $I_1$ or $I_2$ is cause, i.e. 
%     $$OR_2(X) = OR_1(X), $$
%     $$ \text{where } OR_i(X) = \frac{\Pr[I^0_i = 1, T^0 = 1 | V = 1, X]\Pr[I^0_i = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_i = 0, T^0 = 1 | V = 1, X]\Pr[I^0_i = 1, T^0 = 1| V = 0, X]}.$$
% \end{itemize}
% Consider the conditional odds ratio for the effect of vaccination among the vaccinated, i.e.
%     \begin{align*}
%         \psi_{orv}(X) &\equiv \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I^0_1 = 1, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_1 = 0, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 1, T^0 = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I_2 = 1, T = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I_2 = 0, T = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1 | V = 0, X]}{\Pr[I_1 = 0, T = 1 | V = 1, X]\Pr[I_1 = 1, T = 1| V = 0, X]}
%     \end{align*}        
% Under the consistency assumption (A1) the numerator is equal to $\Pr[I = 2, T = 1 | V = 1, X]$. Focusing on the denominator, under equi-confounding (A3)
% \begin{equation*}
% \Pr[I^0 = 2, T^0 = 1  | V = 1, X] = \frac{\Pr[I^0 = 1, T^0 = 1  | V = 1, X]}{\Pr[I^0 = 1, T^0 = 1  | V = 0, X]}\Pr[I^0 = 2, T^0 = 1 | V = 0, X]
% \end{equation*}
% and then by (A1) and (A2) with (A4) ensuring overlap
%     \begin{equation*}
%      \Pr[I^0 = 2, T^0 = 1  | V = 1, X] = \frac{\Pr[I = 1, T = 1  | V = 1, X]}{\Pr[I = 1, T = 1  | V = 0, X]}\Pr[I = 2, T = 1 | V = 0, X]
%     \end{equation*}
% Plugging back into the expression for $\psi_{rrv}(X)$, we find the following identifying expression 
%     \begin{equation*}
%          \phi(X) \equiv \dfrac{\dfrac{\Pr[I = 2, T = 1 | V = 1, X]}{\Pr[I = 1, T = 1 | V = 1, X]}}{\dfrac{\Pr[I = 2, T = 1 | V = 0, X]}{\Pr[I = 1, T = 1 | V = 0, X]}}
%     \end{equation*}
% which is the ratio of the odds of symptomatic infection with the vaccine pathogen versus symptomatic infection with another pathogen in the vaccinated and unvaccinated. It is also strictly written in terms of the observables. A key insight is that $\frac{\Pr[I = 1, T =1  | V = 0, X]}{\Pr[I = 1, T = 1 | V = 1, X]}$ acts as a proxy for $\frac{\Pr[I^0 = 2, T =1  | V = 0, X]}{\Pr[I^0 = 2, T = 1 | V = 1, X]}$ essentially a ``parallel trend'' for $I=2$ in absence of vaccination.

% Recall that, in a test-negative study, we only observe test results among the symptomatic and tested, i.e. samples $\{(X_i, V_i, S_i = 1, I^*_i) : i = 1, \ldots, n\}$ where $S = \mathbb{I}(I \neq 0, T = 1)$. However, we can show that 
%     \begin{align*}
%          \phi(X) &= \dfrac{\dfrac{\Pr[I^* = 1 | S = 1, V = 1, X]}{\Pr[I^* = 0 | S = 1, V = 1, X]}}{\dfrac{\Pr[I^* = 1 | S = 1, V = 0, X]}{\Pr[I^* = 0 | S = 1, V = 0, X]}}
%     \end{align*}    
% which is the odds ratio comparing odds of testing positive for vaccinated and unvaccinated among the tested only.
\newpage 
\subsection{What if there is a direct effect of vaccination on testing behavior?}\label{sec:de_testing}
Unlike in a placebo-controlled trial, participants in a test-negative design are generally aware of their vaccination status. It is therefore possible that this knowledge could affect their testing behavior in a number of ways which would violate Assumption A5. For instance, some individuals may be less likely to get tested when vaccinated because they feel more protected or perceive the risk of illness to be lower. Here, we consider identifiability under a weaker assumption A5*:
\begin{itemize}
    \item[(A5*)] Equivalent effects of vaccination on test-seeking behavior for test-positive and test-negative illnesses among the vaccinated. That is, 
    \begin{equation}
    \dfrac{\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)} = \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}.
\end{equation}
\end{itemize}
This assumption allows for vaccination to affect testing behavior provided it does so similarly for test-positive and test-negative illnesses. Together with assumption A3 it implies
\begin{equation*}
    \dfrac{\Pr(T = 1 | I = 1, V = 1, X)}{\Pr(T = 1 | I = 1, V = 0, X)} = \dfrac{\Pr(T = 1 | I = 2, V = 1, X)}{\Pr(T = 1 | I = 2, V = 0, X)}.
\end{equation*}
It may be plausible given that participants may not know which infection they have prior to receiving a test. Note that, it would still be violated if vaccination reduced severity or altered symptoms of infection, and therefore willingness to seek a test, for $I=2$ but not $I=1$. In Corollary 3, we show that under this assumption we can still identify the causal risk ratio among the vaccinated in the test-negative study. 
\vspace{1em}

% \begin{equation}
%     \dfrac{\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)} = \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}
% \end{equation}

% \begin{align*}
%     \Pr&(T^1 = 1 | I^1 = 1, V = 1, X) \\
%     &= \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}\Pr(T^0 = 1 | I^0 = 1, V = 1, X) \\
%     &= \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)\Pr(T^0 = 1 | I^0 = 1, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)\Pr(T^0 = 1 | I^0 = 2, V = 0, X)} \\
%      &= \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)}{\Pr(T^0 = 1 | I^0 = 2, V = 0, X)} \\
% \end{align*}

% \begin{align*}
%     \Pr&(T^0 = 1 | I^0 = 1, V = 1, X) \\
%     &= \dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)} \Pr(T^1 = 1 | I^1 = 1, V = 1, X)\\
%     &= \dfrac{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)\Pr(T^0 = 1 | I^0 = 2, V = 0, X)\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)} \\
%      1 &= \dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 0, X)\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)\Pr(T^0 = 1 | I^0 = 1, V = 0, X)} \\
% \end{align*}
    

\begin{corollary}
    Under assumptions A1-A4 and alternative assumption A5* that direct effects of vaccination on testing behavior are the same for test-positive and test-negative illnesses, $\Psi$ is identified by $\Psi^\dagger_{om}$ and $\Psi^\dagger_{ipw}$ (as defined in Lemma \ref{lemma2}) in the full cohort as well as $\Psi^*_{om}$ and $\Psi^*_{ipw}$ (as defined in Lemma \ref{lemma2}) in the test-negative design.
\end{corollary}
    
    \begin{proof}
        Define $\gamma(X)$ as 
        \begin{equation*}
            \gamma(X) \equiv \dfrac{\Pr(T = 1 | I = 1, V = 1, X)}{\Pr(T = 1 | I = 1, V = 0, X)} \dfrac{\Pr(T = 1 | I = 2, V = 0, X)}{\Pr(T = 1 | I = 2, V = 1, X)},
        \end{equation*}
        and note that by Assumption A5*, $\gamma(X) = 1$. First, we show that $\Psi$ is identified by $\Psi^\dagger_{om}$ and therefore by Corollary 2 by $\Psi^*_{om}$. We have
        \begin{align*}
            \Psi &= \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)} \\
            &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\Pr(I^0=2|V=1, X) | V= 1 \right\}} \\
            &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\dfrac{\Pr(I^0=1|V=1, X)}{\Pr(I^0=1|V=0, X)}\Pr(I^0=2|V=0, X) \bigg| V= 1 \right\}} \text{\textcolor{red}{(I don't think this is correct?)}}\\
            &= \dfrac{\Pr(I^1=2|V=1)}{E\left\{\dfrac{\Pr(I^1=1|V=1, X)}{\Pr(I^0=1|V=0, X)}\Pr(I^0=2|V=0, X) \bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I=2|V=1)}{E\left\{\dfrac{\Pr(I=1|V=1, X)}{\Pr(I=1|V=0, X)}\Pr(I=2|V=0, X) \gamma(X) \bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I=2|V=1)}{E\left\{\dfrac{\Pr(I=1, T=1|V=1, X)}{\Pr(I=1, T = 1|V=0, X)} \dfrac{\Pr(I=2, T=1|V=0, X)}{\Pr(T^ = 1 | I = 2, V = 1, X)}\bigg| V= 1 \right\}} \\
            &= \dfrac{\Pr(I=2, T=1 |V=1)}{E\left\{\dfrac{\Pr(I=1, T=1|V=1, X)}{\Pr(I=1, T= 1|V=0, X)} \Pr(I=2, T=1|V=0, X) \bigg| V= 1 \right\}} \\
            &= \Psi^\dagger_{om}.
        \end{align*}
        The first line follows by definition. The second applies the law of iterated expectations. The third applies assumption A3. The fourth applies assumption A2. The fifth applies new assumption A5*. The sixth replaces the product of conditionals with the joint distribution. The seventh reverses the law of iterated expectations. 

        Next, we show that $\Psi$ is identified by $\Psi^\dagger_{ipw}$ and therefore by Corollary 2 by $\Psi^*_{ipw}$. Note that, by the invariance of the odds ratio, 
        \begin{equation*}
            \gamma(X) = \dfrac{\Pr(V = 1 | I = 1, T = 1, X)}{\Pr(V = 0 | I = 1, T = 1, X)} \dfrac{\Pr(V = 0 | I = 2, T = 1, X)}{\Pr(V = 1 | I = 2, T = 1, X)},
        \end{equation*}
        We have
        \begin{align*}
        \Psi &= \dfrac{\Pr(I^1=2|V=1)}{\Pr(I^0=2|V=1)} \\
        &= \dfrac{E\left\{\dfrac{V}{\Pr(V=1)} \mathbbm 1 (I^1 = 2)\right\}}{E\left\{\dfrac{V}{\Pr(V=1)}\mathbbm 1 (I^0 = 2)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) E(V | I^0 = 2, T^0 = 1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) \pi^{0\dagger}_2(X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I^1 = 2, T^1 = 1)\}}{E\left\{\mathbbm 1 (I^0 = 2, T^0 = 1) \dfrac{\pi^{0\dagger}_2(X)}{1-\pi^{0\dagger}_2(X)}E(1-V|I^0 = 2, T^0 = 1, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T = 1) \dfrac{\pi^{0\dagger}_2(X)}{1-\pi^{0\dagger}_2(X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (I = 2, T = 1)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2, T = 1) \dfrac{\pi^\dagger_1(X)}{1-\pi^\dagger_1(X)}\right\}}.
    \end{align*}
    \end{proof}

\newpage

\subsection{What if the test is imperfect?} \label{sec:testing}
In the main text, we assumed the availability of a perfect test such that $I^*_i = \mathbbm 1 (I_i = 2, T_i = 1)$ for all individuals. In the absence of such a test, here we describe the potential for bias due to misclassification of case status which has also been described previously \cite{sullivan_theoretical_2016}. Because most studies employ real-time reverse-transcription polymerase chain reaction (RT-PCR), false positives are unlikely as most RT-PCR tests have specificity approaching 100\%. Therefore, misclassification of test-positives is probably rare, perhaps due to sample contamination or data entry errors. False negatives may be be more likely as test sensitivity is generally lower due to the fact that (1) some of those infected with test-positive illness may not shed detectable virus or viral RNA; (2) some may seek care after shedding has ceased; or (3) sample quality may be compromised due to swab quality or inadequate storage. If sample collection is sufficient such that the source of the test-negative infection can be identified via RT-PCR, then sensitivity with respect to the test-positive illness can be improved by limiting to test-negative controls with an identified cause. In the case that misclassification is nondifferential with respect to vaccination, bias is likely to be minimal due to high specificity. However, care should be taken as test errors must be independent of potential confounders and effect modifiers for the bias to be strictly towards the null. 

\subsection{What about immortal time bias?}
Concerns have previously been raised about the potential for design-induced biases in the test-negative design resulting from failure to explicitly emulate a randomized trial. Here we show that, when the degree of design-induced bias is similar for test-positive and test-negative illnesses, they tend to cancel each other out and produce no bias in the TND estimate of vaccine effectiveness when they are equivalent. 

Consider the emulation of a trial where events occurring in the first 28 days are excluded in both the vaccine arm and the control arm, as was specified in the original trials of the SARS-CoV-2 vaccines. This is often done because, biologically, it is believed that during this window the vaccine has not had time to provoke a sufficient immunological response and therefore provides no protection, although discarding these events may produce selection bias due to depletion of susceptibles when there is an effect. Regardless, attempts to emulate this result using a TND are challenged by the fact that the TND does not have a well-defined start of follow up and only the timing of vaccination and testing are known. Therefore, investigators instead discard cases where vaccination occurred less than 28 days prior to receiving a test, i.e. only among the vaccinated. This results in a form of immortal time bias as the vaccinated are essentially \textit{immortal} during this 28 day period. Assume for a second, to simpilify exposition, that conditional on $X$ there is no other source of confounding. Note that under our framework, this would imply that 
\begin{equation*}
     \frac{\Pr(I^0 = 2, T^0 = 1 | V = 1, X)}{\Pr(I^0 = 2, T^0 = 1| V = 0, X)} = \eta_2(X)
\end{equation*}
for some $\eta_2(X) < 1$ as the person time in the first 28 days after receiving a vaccine is differentially discarded among those who received a vaccine compared to those who did not. However, if this exclusion of person time among the vaccinated is applied independently of the test result,  the bias also applies to the incidence of test-negative illness such that
\begin{equation*}
     \frac{\Pr(I^0 = 1, T^0 = 1 | V = 1, X)}{\Pr(I^0 = 1, T^0 = 1| V = 0, X)} = \eta_1(X)
\end{equation*}
for some $\eta_1(X) < 1$. In the special case that $\eta_2(X) = \eta_1(X)$, which could occur if the risks of test-positive and test-negative illness are equal over the 28 day period, we have equi-confounding and the design-induced bias is completely removed by $\Psi^*_{om}$ and $\Psi^*_{ipw}$.


\subsection{Relationship to vaccination mechanisms} \label{sec:vaccine_modelling}

\newpage
\subsection{A semi-parametric odds ratio model}
Define 
\begin{align*}
    h(i, t, x) &= f(I^0 = i, T^0 = t | V = 0, X = x) \\
    \beta(i, t,  x) &= \log \dfrac{f(I^0 = i, T^0 = t | V = 1, X = x)f(T^0 = 0 | V = 0, X = x)}{f(T^0 = 0 | V = 1, X = x)f(I^0 = i, T^0 = t | V = 0, X = x)}
\end{align*}
where $h(i, t, x)$ is the conditional density among the unvaccinated and the function $\beta(i, t,  x)$ is the log of the generalized odds ratio function with $\beta(i_{ref}, t_{ref},  x) = 0$ for user-specified reference values $i_{ref}$ and $t_{ref}$. Thus, $\beta(i, t,  x) = 0$ implies no unmeasured confounding for symptomatic infection or testing given $X = x$ and $\beta(i, t,  x) \neq 0$ encodes the degree of unmeasured confounding bias at the distributional level. This parameterization implies
\begin{align*}
    f(I^0 = i, T^0 = t | V = 1, X = x) \propto h(i, t, x) \exp\{\beta(i, t, x)\}
\end{align*}
with 
\begin{align*}
    f(I^0 = i, T^0 = t | V = 1, X = x) = \dfrac{h(i, t, x) \exp\{\beta(i, t, x)\}}{\int h(i, t, x) \exp\{\beta(i, t, x)\} \;di\,dt}
\end{align*}
where $\int h(i, t, x) \exp\{\beta(i, t, x)\} \;di\,dt$ is the proportionality or normalizing constant. 

For tri-level outcome this implies that
\begin{align*}
    \Pr(I^0 = 2, T^0 = 1 | V = 1, X = x) = \Pr(I^0 = 2, T^0 = 1 | V = 0, X = x)\dfrac{\Pr(I^0 = 1, T^0 = 1 | V = 1, X = x)}{\Pr(I^0 = 1, T^0 = 1 | V = 0, X = x)}
\end{align*}
and therefore 
\begin{align*}
    \dfrac{\Pr(I^0 = 2, T^0 = 1 | V = 1, X = x)}{\Pr(I^0 = 2, T^0 = 1 | V = 0, X = x)} = \dfrac{\Pr(I^0 = 1, T^0 = 1 | V = 1, X = x)}{\Pr(I^0 = 1, T^0 = 1 | V = 0, X = x)}
\end{align*}
which is condition A3.

By the invariance of odds ratios 
\begin{align*}
    \beta(i, t, x) = \dfrac{\Pr(V = 1 | I^0 = i, T^0 = t, X = x)\Pr(V = 0 | T^0 = 0, X = x)}{\Pr(V = 0 | I^0 = i, T^0 = t, X = x)\Pr(V = 1 | T^0 = 0, X = x)}
\end{align*}
and therefore we have that
\begin{align*}
    \log \dfrac{\pi(i, t, x)}{1 - \pi(i, t, x)} = \eta(x) + \beta(i, t, x)
\end{align*}
where
\begin{align*}
    &\pi(i, t, x) = \Pr(V = 1 | I^0 = i, T^0 = t, X = x) \\
    &\eta(x) = \log \dfrac{\Pr(V = 1 | T^0 = 1, X = x)}{\Pr(V = 1 | T^0 = 0, X = x)}
\end{align*}
Note that 

\begin{align*}
    \Pr&(I^0 = 2, T^0 = 1 | V = 1, X = x) \\
    &= \dfrac{f(I = i, T = t  | V = 0, X = x) \exp\{\beta(i, t, x)\}}{E\left[ \exp\{\beta(i, t, x)\} | V = 0, X = x\right]} \\
    &=\dfrac{\int \mathbbm 1 (I = 2, T = 1) f(I = i, T = t  | V = 0, X = x) \exp\{\beta(i, t, x)\}\;di\,dt}{\int \exp\{\beta(i, t, x)\} f(I = i, T = t | V = 0, X = x) \;di\,dt} \\
    &=\dfrac{\int \mathbbm 1 (I = 2, T = 1) f(I = i, T = t  | V = 0, X = x) \exp\{\beta(i, t, x)\}\;di\,dt}{\exp\{\beta(2, 1, x)\} \Pr(I = 2, T = 1 | V = 0, X = x) + \exp\{\beta(1, 1, x)\} \Pr(I = 1, T = 1 | V = 0, X = x) + \Pr(T= 0| V = 0, X = x) }
\end{align*}

\begin{align*}
    \dfrac{\Pr(T = 1 | V = 0, X = x)}{\Pr(T^0 = 1 | V = 1, X = x)} \left\{ \Pr(I^0 = 2, T^0 = 1 | V = 1, X = x) + \Pr(I^0 = 1, T^0 = 1 | V = 1, X = x) + \Pr(T^0 = 1 | V = 1, X = x)\right \}
\end{align*}


\begin{align*}
    \log \dfrac{\Pr(I^* = 1 | S = 1, V, X)}{\Pr(I^* = 0 | S = 1, V, X)} &= \eta(X) + \alpha (X) \\
    \eta(X) &= \log \dfrac{\Pr(I^* = 1 | S = 1, V=0, X)}{\Pr(I^* = 0 | S = 1, V=0, X)} \\
    \eta(X) + \alpha(X) &= \log \dfrac{\Pr(I^* = 1 | S = 1, V=1, X)}{\Pr(I^* = 0 | S = 1, V=1, X)} \\
    \alpha(X) &= \log \dfrac{\Pr(I^* = 1 | S = 1, V=1, X)\Pr(I^* = 0 | S = 1, V=0, X)}{\Pr(I^* = 0 | S = 1, V=1, X)\Pr(I^* = 1 | S = 1, V=0, X)} \\
\end{align*}
% Define the following:
% \begin{description}
%     \item[$\lambda$:] Force of infection 
%     \item[$\pi$:] Probability of symptoms given infection
%     \item[$P$:] Total population 
%     \item[$v$:] Proportion vaccinated
%     \item[$\mu$:] Probability of care-seeking given symptoms
%     \item[$\theta(t):$] Hazard ratio for infection given 
% \end{description}
% The rate of ascertaining test-positive, vaccinated persons is
% $$
% \Lambda_{V I}(t)=\lambda_I \pi_I \mu_V[(1-\varphi) e^{-\lambda_I t} + \varphi \theta e^{-\theta \lambda_I t}] v P .
% $$

% $$
% \Lambda_{U I}(t)=\lambda_I \pi_I \mu_U e^{-\lambda_I t}(1-v) P .
% $$

% Test-negative vaccinated and unvaccinated persons are ascertained at the rates
% $$
% \Lambda_{V N}(t)=\lambda_N \pi_N \mu_V v P
% $$
% and
% $$
% \Lambda_{U N}(t)=\lambda_N \pi_N \mu_U(1-v) P,
% $$
% respectively, assuming vaccination does not affect susceptibility to the test-negative conditions.

% Test-negative studies typically measure the odds ratio of vaccination among the test-positive and test-negative subjects, similar to the exposure odds ratio in case-control studies, using cumulative cases $(C)$. For the test-positive outcome,
% $$
% \begin{gathered}
% C_{V I}(t)=\pi_I \mu_V\left[(1-\varphi)\left(1-e^{-\lambda_I t}\right)+\varphi\left(1-e^{-\theta \lambda_I t}\right)\right] v P \\
% C_{U I}(t)=\pi_I \mu_U\left(1-e^{-\lambda_I t}\right)(1-v) P .
% \end{gathered}
% $$

% Under the assumption that test-negative infections are not immunizing, cumulative cases are proportional to the incidence rate and study duration:
% $$
% \begin{gathered}
% C_{V N}(t)=\lambda_N \pi_N \mu_V v P t \\
% C_{U N}(t)=\lambda_N \pi_N \mu_U(1-v) P t .
% \end{gathered}
% $$

% We consider the case of immunizing test-negative outcomes in Web Appendix 1. Using the vaccine-exposure odds ratio measured from cumulative cases,
% $$
% \begin{aligned}
% 1-O R^C(t) & =1-\frac{C_{V I}(t) C_{U N}(t)}{C_{U I}(t) C_{V N}(t)} \\
% & =1-\frac{(1-\varphi)\left(1-e^{-\lambda_I t}\right)+\varphi\left(1-e^{-\theta \lambda_I t}\right)}{1-e^{-\lambda_I t}} \\
% & =\varphi\left(1-\frac{1-e^{-\theta \lambda_I t}}{1-e^{-\lambda_I t}}\right) .
% \end{aligned}
% $$

% $\varphi$
\end{appendix}

