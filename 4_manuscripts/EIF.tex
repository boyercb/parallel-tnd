\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=1cm,right=1cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{natbib}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{bbm}
\def\ll{\lambda}
\def\LL{\Lambda}
\def\arctanh{\mathrm{arctanh}}
\def\tanh{\mathrm{tanh}}
\def\indep{\!\perp\!\!\!\perp}
\DeclareMathOperator{\E}{E}

\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\title{EIF for VE based on a TND study}


\begin{document}
\maketitle


Define the risk ratio of testing positive among the treated as:
\begin{align*}
    \Psi &= \dfrac{\Pr[I^1 = 2, T^1 = 1 | V = 1]}{\Pr[I^0 = 2, T^0 = 1 | V = 1]} \\
         &= \dfrac{\Pr[I^1 = 2, T^1 = 1 | V = 1]}{\E[\Pr[I^0 = 2, T^0 = 1 | V = 1, X] | V = 1]}.
\end{align*}
By consistency the numerator is equal to $\Pr[I^1 = 2, T^1 = 1 | V = 1]$ and by the previous results we have that 
\begin{equation*}
    \Pr[I^0 = 2, T^0 = 1 | V = 1, X] = \dfrac{\Pr[I = 1, T = 1 | V = 1, X]}{\Pr[I = 1, T = 1 | V = 0, X]} \Pr[I = 2, T = 1 | V = 0, X]
\end{equation*}
which is equivalent to 
\begin{equation*}
    \Pr[I^0 = 2, T^0 = 1 | V = 1, X] = \dfrac{1}{\psi(X)} \Pr[I = 2, T = 1 | V = 1, X].
\end{equation*}
where $\psi(X)$ is the conditional odds ratio defined previously, i.e.
\begin{equation*}
    \psi(X) = \dfrac{\dfrac{\Pr[I = 2, T = 1 | V = 1, X]}{\Pr[I = 1, T = 1 | V = 1, X]}}{\dfrac{\Pr[I = 2, T = 1 | V = 0, X]}{\Pr[I = 1, T = 1 | V = 0, X]}}
\end{equation*}
Hence
\begin{align*}
    \Psi &= \dfrac{\Pr[I = 2, T = 1 | V = 1]}{\E\left[\dfrac{1}{\psi(X)} \Pr[I = 2, T = 1 | V = 1, X] | V = 1\right]} \\
\end{align*}
To show this is identified under the biased sampling design consider
\begin{align*}
    \Psi &= \dfrac{\Pr[I = 2, T = 1 | V = 1]}{\E\left[\dfrac{1}{\psi(X)} \Pr[I = 2, T = 1 | V = 1, X] | V = 1\right]} \\
    &= \dfrac{\Pr[I = 2, T = 1 | V = 1]}{\int \dfrac{1}{\psi(X)} \Pr[I = 2, T = 1 | V = 1, X] f(x | V = 1) dx} \\
    &= \dfrac{\Pr[I = 2 | T = 1, V = 1] \Pr[T = 1 | V = 1]}{\int \dfrac{1}{\psi(X)} \Pr[I = 2 | T = 1, V = 1, X] \Pr[T = 1 | V = 1, X]  f(x | V = 1) dx} \\
    &= \dfrac{\Pr[I = 2 | T = 1, V = 1] \Pr[T = 1 | V = 1]}{\int \dfrac{1}{\psi(X)} \Pr[I = 2 | T = 1, V = 1, X] \Pr[T = 1 | V = 1]  f(x | T = 1, V = 1) dx} \\
\end{align*}
where the last line follows by Bayes theorem 
\begin{equation*}
    f(x | V = 1) = \dfrac{f(x | T = 1, V = 1)\Pr[T = 1 | V = 1]}{\Pr[T = 1 | V = 1, X]}
\end{equation*}
and further
\begin{align*}
    \Psi &= \dfrac{\Pr[I = 2 | T = 1, V = 1]}{\int \dfrac{1}{\psi(X)} \Pr[I = 2 | T = 1, V = 1, X] f(x | T = 1, V = 1) dx} \\
    &= \dfrac{\Pr[I = 2 | T = 1, V = 1]}{\E\left[\dfrac{1}{\psi(X)} \Pr[I = 2 | T = 1, V = 1, X] \bigg| T = 1, V = 1\right]} \\
    &= \dfrac{\Pr[I^* = 1 | S = 1, V = 1]}{\E\left[\dfrac{1}{\psi^*(X)} \Pr[I^* = 1 | S = 1, V = 1, X] \bigg| S = 1, V = 1\right]} \\
\end{align*}
where all elements are identified under biased sampling.


We continue to derive the EIF of
$$\Psi = \dfrac{E(I^*\mid S=1, V=1)}{E\left\{\dfrac{Pr(I^*=1\mid S=1, V=0, X)Pr(I^*=0\mid S=1, V=1, X)}{Pr(I^*=0\mid S=1, V=0, X)}\mid S=1, V=1\right\}}$$

To derive the EIF for $\Psi$, we use the point-mass approach in Hines et al. 2019.


We consider a one-dimensional parametric submodel

$$f_\theta(i^*, s, v, x)=\theta \mathbbm 1_{\tilde i^*, \tilde s, \tilde v, \tilde x}(i^*, s, v, x) + (1-\theta)f(i^*, s, v,x),$$

and let
$$\Psi_\theta = \dfrac{E_\theta(I^*\mid S=1, V=1)}{\int \dfrac{E_\theta(I^*\mid S=1, V=0, X=x)E_\theta(1-I^*\mid S=1, V=1, X=x)}{E_\theta(1-I^*\mid S=1, V=0, X=x)}f_\theta(x\mid S=1, V=1)dx}$$


We have
\begin{align*}
    \dfrac{\partial}{\partial\theta}E_\theta(I^*\mid S=1, V=1)\mid_{\theta=0} &= \int i^* \dfrac{\partial}{\partial\theta}f_\theta(i^*\mid S=1, V=1)\mathrm d i^*\mid_{\theta = 0}\\
    &= \dfrac{\mathbbm 1_{\tilde s,\tilde v}(1, 1)}{f_{S, V}(1, 1)}\{\tilde i^* - E(I^*\mid S=1, V=1)\}\\
    &= \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde i^* - E(I^*\mid S=1, V=1)\}
\end{align*}

and Similarly,
\begin{align*}
    \dfrac{\partial}{\partial\theta}E_\theta(I^*\mid S=1, V=0, X=x)\mid_{\theta=0} &= \dfrac{\tilde s(1-\tilde v)\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 0, x)}\{\tilde i^* - E(I^*\mid S=1, V=0, X=x)\}\\
     \dfrac{\partial}{\partial\theta}E_\theta(1-I^*\mid S=1, V=0, X=x)\mid_{\theta=0} &= -\dfrac{\tilde s(1-\tilde v)\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 0, x)}\{\tilde i^* - E(I^*\mid S=1, V=0, X=x)\}\\
     \dfrac{\partial}{\partial\theta}E_\theta(1-I^*\mid S=1, V=1, X=x)\mid_{\theta=0} &= -\dfrac{\tilde s\tilde v\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 1, x)}\{\tilde i^* - E(I^*\mid S=1, V=1, X=x)\}.
\end{align*}

We also have
$$\dfrac{\partial}{\partial\theta}f_\theta(x\mid S=1, V=1) = \dfrac{\tilde s \tilde v}{Pr(S=1, V=1)}\{\mathbbm 1_{\tilde x}(x)-f(x\mid S=1, V=1)\}$$

The pathwise derivative of $\Psi_\theta$ wrt $\theta$ is
\begin{align*}
    \dfrac{\partial}{\partial\theta}\Psi_\theta\mid_{\theta=0} &= \dfrac{\dfrac{\tilde s\tilde v}{Pr(S=1, V=1)}\{\tilde i^* - Pr(I^*=1\mid S=1, V=1)\}}{E\left\{\dfrac{Pr(I^*=1\mid S=1, V=0, X)Pr(I^*=0\mid S=1, V=1, X)}{Pr(I^*=0\mid S=1, V=0, X)}\mid S=1, V=1\right\}} -\\
    & \dfrac{E(I^*\mid S=1, V=1)}{\left[E\left\{\dfrac{Pr(I^*=1\mid S=1, V=0, X)Pr(I^*=0\mid S=1, V=1, X)}{Pr(I^*=0\mid S=1, V=0, X)}\mid S=1, V=1\right\}\right]^2}\times \\
    &\bigg[ \dfrac{\tilde s(1 - \tilde v)}{Pr(S=1, V=0)}\{\tilde i^* - Pr(I^* = 1\mid S=1, V=0, X=\tilde x)\}\dfrac{f(\tilde x\mid S=1, V=1)Pr(I^*=0\mid S=1, V=1, X=\tilde x)}{f(\tilde x\mid S=1, V=0)Pr(I^*=0\mid S=1, V=0, X=\tilde x)} -\\
    & \dfrac{\tilde s\tilde v}{Pr(S=1, V=1)}\{\tilde i^* - Pr(I^*=1\mid S=1, V=1, X=\tilde x)\}\dfrac{Pr(I^*=1\mid S=1, V=0, X=\tilde x)}{Pr(I^*=0\mid S=1, V=0, X=\tilde x)} + \\
    &\dfrac{\tilde s(1 - \tilde v)}{Pr(S=1, V=0)}\{\tilde i^* - Pr(I^* = 1\mid S=1, V=0, X=\tilde x)\}\times \\&\qquad \dfrac{f(\tilde x\mid S=1, V=1)Pr(I^*=0\mid S=1, V=1, X=\tilde x)Pr(I^*=1\mid S=1, V=0, X=\tilde x)}{f(\tilde x\mid S=1, V=0)Pr(I^*=0\mid S=1, V=0, X=\tilde x)^2}+\\
    &\dfrac{\tilde s\tilde v}{Pr(S=1, V=1)}\dfrac{Pr(I^*=1\mid S=1, V=0, X=\tilde x)Pr(I^*=0\mid S=1, V=1, X=\tilde x)}{Pr(I^*=0\mid S=1, V=0, X=\tilde x)} -\\
   & \dfrac{\tilde s\tilde v}{Pr(S=1, V=1)}E\left\{\dfrac{Pr(I^*=1\mid S=1, V=0, X)Pr(I^*=0\mid S=1, V=1, X)}{Pr(I^*=0\mid S=1, V=0, X)}\mid S=1, V=1\right\}\bigg]\\
   &= \dfrac{\tilde s\tilde v\tilde i^*}{\sigma\mu E\left\{\dfrac{\pi_0(X)\{1 - \pi_1(X)\}}{1 - \pi_0(X)}\mid S=1, V=1\right\}} - \dfrac{Pr(I^*=1\mid S=1, V=1)}{\left[E\left\{\dfrac{\pi_0(X)\{1 - \pi_1(X)\}}{1 - \pi_0(X)}\mid S=1, V=1\right\}\right]^2}\times \\
    &\bigg[ \dfrac{\tilde s(1 - \tilde v)}{\sigma\mu}\{\tilde i^* - \pi_0(\tilde x)\}\dfrac{\mu(\tilde x)\{1 - \pi_1(\tilde x)\}}{\{1 - \mu(\tilde x)\}\{1 - \pi_0(\tilde x)\}^2} + \dfrac{\tilde s\tilde v}{\sigma\mu}\{1-\tilde i^*\}\dfrac{\pi_0(\tilde x)}{1-\pi_0(\tilde x)}\bigg]
\end{align*}
where $\sigma=P(S=1)$, $\pi(x) = Pr(V=1\mid S=1, X=x)$, $\pi=Pr(V=1\mid S=1)$, $f_s(x)=f(x\mid S=1)$ and $\mu_v(x)=Pr(I^*=1\mid V=v, S=1, X=x)$.

By Hines et al. 2019, the efficient influence function for $\Psi$ is
\begin{align*}
    EIF(O_j) &=\dfrac{S_jV_j I^*_j}{\sigma \int \dfrac{\pi_0(x)\{1 - \pi_1(x)\}}{1 - \pi_0(x)}\mu(x)f_s(x)dx} - \dfrac{Pr(I^*=1, V=1\mid S=1)}{\left[\int \dfrac{\pi_0(x)\{1 - \pi_1(x)\}}{1 - \pi_0(x)}\mu(x)f_s(x)dx\right]^2}\times \\
    &\bigg[ \dfrac{S_j(1 - V_j)}{\sigma}\{I^*_j - \pi_0(X_j)\}\dfrac{\mu(X_j)\{1 - \pi_1(X_j)\}}{\{1 - \mu(X_j)\}\{1 - \pi_0(X_j)\}^2} + \dfrac{S_jV_j}{\sigma}\{1-I^*_j\}\dfrac{\pi_0(X_j)}{1-\pi_0(X_j)}\bigg] 
\end{align*}

\section{One-step correction estimator}
\begin{enumerate}   
    \item Specify the propensity score model $\pi(x;\alpha)$ and the outcome regression model $\mu_v(x;\beta)$. The nuisance parameters may be estimated by solving estimating equations
    \begin{align*}
        \sum_{j=1}^n X_j\{V_j - \pi(X_j;\alpha)\} &= 0;\\
        \sum_{j=1}^n \begin{pmatrix}
            X_j\\V_j
        \end{pmatrix}\{I^*_j - \mu_{V_j}(X_j;\beta)\}=0.
    \end{align*}
    Denote the resulting estimators as $\hat\alpha$, $\hat\beta$.
    \item Let $\hat \Pi$ be an estimator for $\Pi\equiv \int \dfrac{\mu_0(x)\{1 - \mu_1(x)\}}{1 - \mu_0(x)}\pi(x)f_s(x)dx$, for example,
    $$\hat\Pi = \dfrac{1}{n}\sum_{j=1}^n \dfrac{\mu_0(X_j;\hat\beta)\{1 - \mu_1(X_j;\hat\beta)\}}{1 - \mu_0(X_j;\hat\beta)}\pi(X_j;\hat \alpha).$$

    The initial estimator of $\Psi$ is
    $$\hat\Psi_0 = \dfrac{\dfrac{1}{n}\sum_{j=1}^n I^*_jV_j}{\hat\Pi}.$$
    \item The one-step corrected estimator is
    \begin{align*}
        \hat\Psi_1 &= \hat\Psi_0 +\dfrac{1}{n}\sum_{i=1}^n \bigg\{\dfrac{V_jI^*_j}{\hat\Pi}-\\&\qquad \dfrac{\dfrac{1}{n}\sum_{k=1}^n I^*_kV_k}{\hat\Pi^2}\bigg[S_j(1 - V_j)\{I^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + S_jV_j\{1-I^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]\bigg\}\\
        &= 2\hat\Psi_0-\dfrac{\hat\Psi_0}{\hat\Pi}\dfrac{1}{n}\sum_{j=1}^n\bigg[S_j(1 - V_j)\{I^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + S_jV_j\{1-I^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]
    \end{align*}
\end{enumerate}

\section{Doubly robust estimator}
Note that
\begin{align*}
    EIF(O_j) &=\dfrac{S_jV_j I^*_j}{\sigma \Pi} - \dfrac{\Psi}{\Pi}\bigg[ \dfrac{S_j(1 - V_j)}{\sigma}\{I^*_j - \mu_0(X_j)\}\dfrac{\pi(X_j)\{1 - \mu_1(X_j)\}}{\{1 - \pi(X_j)\}\{1 - \mu_0(X_j)\}^2} + \dfrac{S_jV_j}{\sigma}\{1-I^*_j\}\dfrac{\mu_0(X_j)}{1-\mu_0(X_j)}\bigg] 
\end{align*}

Solve
$$\dfrac{1}{n}\sum_{j=1}^n \dfrac{V_jI^*_j}{\Pi}-\dfrac{\Psi}{\Pi}\dfrac{1}{n}\sum_{j=1}^n\bigg[ (1-V_j)\{I^*_j - \mu_0(X_j)\}\dfrac{\pi(X_j)\{1 - \mu_1(X_j)\}}{\{1 - \pi(X_j)\}\{1 - \mu_0(X_j)\}^2} + V_j\{1-I^*_j\}\dfrac{\mu_0(X_j)}{1-\mu_0(X_j)}\bigg]=0,$$
we obtain the estimator 
\begin{align*}
\hat\Psi_2&= \dfrac{\sum_{i=1}^n V_jI_j^*}{\sum_{j=1}^n\bigg[ (1-V_j)\{I^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + V_j\{1-I^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]}
\end{align*}

We can show that $\hat\Psi_2$ is consistent if $\mu_0(\cdot;\beta)$ is correctly specified, and either $\pi(\cdot;\alpha)$ or $\mu_1(\cdot;\beta)$ is correctly specified. 

\begin{proof}
To prove the doubly-robustness of $\hat\Psi_2$, it suffices to show that 
\begin{align*}
    E\bigg[ (1-V)\{I^* - \dot \mu_0(X)\}\dfrac{\dot\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \dot\mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\dot\mu_0(X)}{1-\dot\mu_0(X)}\mid S=1\bigg] = \Pi
\end{align*}
if (1) $\dot\mu_v(\cdot)=\mu_v(\cdot)$ a.s. for $v=0,1$, or (2) $\dot \pi(\cdot)=\pi(\cdot)$ and $\dot\mu_0(\cdot)=\mu_0(\cdot)$  a.s.

    \begin{enumerate}
        \item If $\dot\mu_v(\cdot)=\mu_v(\cdot)$ a.s. for $v=0,1$, then
        \begin{align*}
            &E\bigg[ (1-V)\{I^* -  \mu_0(X)\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
            =& E\bigg[ \{I^* -  \mu_0(X)\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} \mid V=0, S=1\bigg]Pr(V=0\mid  S=1) + \\
            &\qquad E\bigg[E\{V(1-I^*)\mid X\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid  S=1\bigg]\\
            =& E\bigg[E \{I^* -  \mu_0(X)\mid V=0, S=1, X\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} \mid V=0, S=1\bigg]Pr(V=0\mid  S=1) + \\
            &\qquad E\bigg[\pi(X)\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid  S=1\bigg]\\
            &= 0 + \Pi = \Pi
        \end{align*}
    \item If $\dot\pi(\cdot)=\pi(\cdot)$ and $\dot\mu_0(\cdot)=\mu_0(\cdot)$ a.s., then 
\begin{align*}
    &E\bigg[ (1-V)\{I^* -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + V\{1-I^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    =& E\bigg[ \{1 - \pi(X)\}\{\mu_0(X) -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \pi(X)\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    &= \Pi
\end{align*}


    \end{enumerate}
\end{proof}



\bibliographystyle{plainnat}
\bibliography{sample}


\end{document}