  \section{Semiparametric Efficiency Theory}\label{sec:eif}
    In this section, we start by deriving the efficient influence function (EIF) for $\Psi$. We then obtain an estimator of $\Psi$ based on the EIF and establish the conditions under which the estimator is $\sqrt{n}$-consistent and asymptotically normal. We then evaluate the robustness of the estimator to misspecification of the nuisance functions. 
    
    \subsection{The efficient influence function}
    To derive the efficient influence function (EIF) for $\Psi$, we use the point-mass approach suggested in \textcite{hines_demystifying_2022}. The 
    \begin{lemma}
        The efficient influence function for $\Psi^*$ is given by 
        \begin{align*}
         \chi(P, O) &= \dfrac{1}{\sigma \Delta} \Bigg(S V Y^* - \Psi(P)  \times \bigg[S(1 - V)\{Y^* - \mu_0(X)\}\dfrac{\pi_0(X)\{1 - \mu_1(X)\}}{\{1 - \pi_0(X)\}\{1 - \mu_0(X)\}^2}\\
        &\qquad + S V\{1-Y^*\}\dfrac{\mu_0(X)}{1 - \mu_0(X)}\bigg] \Bigg)
    \end{align*}
    where 
    \begin{align*}
        \pi_y(X) &\equiv \Pr(V = 1 | S = 1, Y^* = y, X), \\
        \mu_v(X) &\equiv \Pr(Y^* = 1 | S = 1, V = v, X), \\
        \sigma &\equiv \Pr(S = 1, V = 1),
    \end{align*}
    and 
    \begin{equation*}
        \Delta \equiv E\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right].
    \end{equation*}
    \end{lemma}

    \begin{proof}
    Let $\Psi(P)$ be the target parameter under true law of the observed data $P$. To derive the EIF, we assume we have access to iid observations $O \equiv (X, V, S, Y^*)$ but show below that the resulting estimator is still identifiable under TND sampling. From the identifiability results in Proposition \ref{prop2}, we have that 
    % $$\Psi(P) = \dfrac{E(Y^*\mid S=1, V=1)}{E\left\{\dfrac{\Pr(Y^*=1\mid S=1, V=0, X)\Pr(V=1\mid S=1, Y^*=0, X)}{\Pr(V=0\mid S=1, Y^*=0, X)}\mid S=1, V=1\right\}}.$$ 
    $$\Psi(P) = \dfrac{E_P(Y^*  | S = 1, V = 1)}{E_P\left\{\dfrac{E_P(Y^*\mid S=1, V=0, X)E_P(1 - Y^*\mid S=1, V=1, X)}{E_P(1 - Y^*\mid S=1, V=0, X)} \bigg| S=1, V=1\right\}}.$$ 
    
    Define $P_t$ as a parametric submodel indexed by $t \in [0,1]$ such that
    $$P_t = t \widetilde{P} + (1 - t)P$$
    where $\widetilde{P}$ is smoothed parametric estimate of $P$ and note that $P_0 = P$. To find the influence function we will use the fact that if we perturb the target in direction of a point mass $\widetilde{o} = (\widetilde{y}^*, \widetilde{s}, \widetilde{v}, \widetilde{x})$ of $\widetilde{P}$
    $$ \chi(P, \widetilde{o}) = \frac{d}{dt} \Psi(P_t)\bigg\vert_{t=0}$$
    where the right-hand side is the so-called the G\^{a}teaux derivative. Note that
    \begin{align*}
        \dfrac{d}{dt}E_{P_t}(Y^*\mid S=1, V=1)\Big\rvert_{t=0} &= \int y^* \dfrac{d}{dt}f_{P_t}(y^*\mid S=1, V=1)\Big\rvert_{t=0} \,dy^*\\
        &= \dfrac{\mathbbm 1_{\tilde s,\tilde v}(1, 1)}{f_{S, V}(1, 1)}\{\tilde y^* - E_P(Y^*\mid S=1, V=1)\}\\
        &= \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde y^* - E_P(Y^*\mid S=1, V=1)\},
    \end{align*}
    and similarly
    \begin{align*}
        \dfrac{d}{dt}E_{P_t}(Y^*\mid S=1, V=0, X=x)\Big\rvert_{t=0} &= \dfrac{\tilde s(1-\tilde v)\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 0, x)}\{\tilde y^* - \mu_0(X)\}\\
         \dfrac{d}{dt}E_{P_t}(1-Y^*\mid S=1, V=0, X=x)\Big\rvert_{t=0} &= -\dfrac{\tilde s(1-\tilde v)\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 0, x)}\{\tilde y^* -\mu_0(X)\}\\
         \dfrac{d}{dt}E_{P_t}(1-Y^*\mid S=1, V=1, X=x)\Big\rvert_{t=0} &= -\dfrac{\tilde s\tilde v\mathbbm 1_{\tilde x}(x)}{f_{S, V, X}(1, 1, x)}\{\tilde y^* - \mu_1(X)\}.
    \end{align*}
We also have
$$\dfrac{d}{dt}f_{P_t}(x\mid S=1, V=1)\Big\vert_{t=0} = \dfrac{\tilde s \tilde v}{f_{S, V}(1, 1)}\{\mathbbm 1_{\tilde x}(x)-f_X(x\mid S=1, V=1)\}.$$
Then, applying the chain rule, the pathwise derivative of $\Psi_t$ wrt $t$ is
\begin{align*}
   &\frac{d}{dt} \Psi(P_t)\bigg\vert_{t=0} \\
   &= \frac{d}{dt} \left[\dfrac{E_P(Y^* = 1 | S = 1, V = 1)}{E_P\left\{\dfrac{E_P(Y^*\mid S=1, V=0, X)E_P(1 - Y^*\mid S=1, V=1, X)}{E_P(1 - Y^*\mid S=1, V=0, X)} \bigg| S=1, V=1\right\}}\right] \vast\vert_{t=0}  \\
   &= \dfrac{\dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde y^* - E_P(Y^*\mid S=1, V=1)\}}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{E_P(Y^*\mid S=1, V=1)}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]^2} \\
   &\qquad \times \bigg( \dfrac{\tilde s(1 - \tilde v)}{f_{S, V}(1, 0)}\{\tilde y^* - \mu_0(\tilde x)\}\dfrac{f_X(\tilde x\mid S=1, V=1)\{1-\mu_1(\tilde x)\}}{f_X(\tilde x\mid S=1, V=0)\{1-\mu_0(\tilde x)\}} \\
   & \qquad \qquad - \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\{\tilde y^* - \mu_1(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} + \dfrac{\tilde s(1 - \tilde v)}{f_{S, V}(1, 0)}\{\tilde y^* - \mu_0(\tilde x)\} \\
   &\qquad \qquad \times \dfrac{f_X(\tilde x\mid S=1, V=1)\{1-\mu_1(\tilde x)\}\mu_0(\tilde x)}{f_X(\tilde x\mid S=1, V=0)\{1-\mu_0(\tilde x)\}^2}+\dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}\dfrac{\mu_0(\tilde x)\{1-\mu_1(\tilde x)\}}{1-\mu_0(\tilde x)} \\
   & \qquad \qquad - \dfrac{\tilde s\tilde v}{f_{S, V}(1, 1)}E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]\bigg) 
\end{align*}
Collecting terms, and noting that $\sigma = f_{S,V}(1, 1)$ and
$$
\dfrac{f_X(\tilde x\mid S=1, V=1)}{f_X(\tilde x\mid S=1, V=0)f_{S,V}(1, 0)} = \dfrac{\pi(\tilde x)}{\{1 - \pi(\tilde x)\} \sigma}
$$
we have
\begin{align*}    
    &= \dfrac{\dfrac{\tilde s\tilde v}{\sigma}\{\tilde y^* - E_P(Y^*\mid S=1, V=1)\}}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{E_P(Y^*\mid S=1, V=1)}{E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]^2} \\
    &\qquad \times \bigg( \dfrac{\tilde s(1 - \tilde v)}{\sigma}\{\tilde y^* - \mu_0(\tilde x)\}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}} - \dfrac{\tilde s\tilde v}{\sigma}\{\tilde y^* - \mu_1(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \\ 
    &\qquad \qquad + \dfrac{\tilde s( 1 -\tilde v)}{\sigma}\{\tilde y^* - \mu_0(\tilde x)\}\dfrac{\pi(\tilde x)\{1 - \mu_1(\tilde x)\}\mu_0(\tilde x)}{\{1 - \pi(\tilde x)\}\{1 - \mu_0(\tilde x)\}^2} + \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)\{1 - \mu_1(\tilde x)\}}{1 - \mu_0(\tilde x)} \\
    &\qquad \qquad - \dfrac{\tilde s\tilde v}{\sigma}E_P\left[\dfrac{\mu_0(X)\{1-\mu_1(X)\}}{1-\mu_0(X)}\bigg| S=1, V=1\right]\bigg)
\end{align*}
The result can be further simplified as
\begin{align*}
    &= \dfrac{\tilde s\tilde v\tilde y^*}{\sigma E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{E_P(Y^*\mid S=1, V=1)}{E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)}\bigg| S=1, V=1\right]^2} \\
    &\qquad \times \bigg[ \dfrac{\tilde s(1 - \tilde v)}{\sigma}\{\tilde y^* - \mu_0(\tilde x)\}\dfrac{\pi(\tilde x)\{1 - \mu_1(\tilde x)\}}{\{1 - \pi(\tilde x)\}\{1 - \mu_0(\tilde x)\}^2} + \dfrac{\tilde s \tilde v}{\sigma}\{1-\tilde y^*\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)}\bigg]
\end{align*}
Therefore the efficient influence function for $\Psi$ is 
\begin{align*}
     \chi(P, O) &= \dfrac{S V Y^*}{\sigma E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right]} - \dfrac{\Psi(P)}{E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)}\bigg| S=1, V=1\right]} \\
    &\qquad \times \bigg[ \dfrac{S(1 - V)}{\sigma}\{Y^* - \mu_0(X)\}\dfrac{\pi(X)\{1 - \mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \dfrac{S V}{\sigma}\{1-Y^*\}\dfrac{\mu_0(X)}{1 - \mu_0(X)}\bigg] 
\end{align*}
    \end{proof}

    
% \begin{align*}
%     &\dfrac{\tilde s (1 - \tilde v)}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}}\left[\{\tilde y^* - \mu_0(\tilde x)\}\dfrac{1-\mu_0(\tilde x)}{1-\mu_0(\tilde x)} + \{\tilde y^* - \mu_0(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1-\mu_0(\tilde x)}\right]\\
%     &=  \dfrac{\tilde s (1 - \tilde v)}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}}\bigg[\{\tilde y^* - \mu_0(\tilde x) - \tilde y^* \mu_0(\tilde x) + \mu_0(\tilde x)^2\}\dfrac{1}{1-\mu_0(\tilde x)} \\
%     &\qquad + \{\tilde y^* \mu_0(\tilde x) - \mu_0(\tilde x)^2\}\dfrac{1}{1-\mu_0(\tilde x)}\bigg] \\
%     &= \dfrac{\tilde s (1 - \tilde v)}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}^2}\{\tilde y^* - \mu_0(\tilde x)\}
% \end{align*}

% \begin{align*}
%     &\dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)\{1 - \mu_1(\tilde x)\}}{1 - \mu_0(\tilde x)} - \dfrac{\tilde s\tilde v}{\sigma}\{\tilde y^* - \mu_1(\tilde x)\}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \\
%     &= \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \bigg\{1 - \mu_1(\tilde x) - \tilde i + \mu_1(\tilde x)\bigg\} \\
%     &= \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} \{1 - \tilde i\}
% \end{align*}

% \begin{align*}
%     \dfrac{\tilde s}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}^2}\{\tilde y^* - \mu_0(\tilde x)\} - \dfrac{\tilde s \tilde v}{\sigma}\dfrac{\pi(\tilde x)\{1-\mu_1(\tilde x)\}}{\{1-\pi(\tilde x)\}\{1-\mu_0(\tilde x)\}^2}\{\tilde y^* - \mu_0(\tilde x)\} + \dfrac{\tilde s \tilde v}{\sigma} \dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)} - \dfrac{\tilde s \tilde v}{\sigma} \tilde y^* \dfrac{\mu_0(\tilde x)}{1 - \mu_0(\tilde x)}
% \end{align*}


\subsection{An estimator based on the efficient influence function}
As discussed in \textcite{hines_demystifying_2022}, an estimator for $\Psi$ that removes plug-in bias may be found as the solution to the following estimating equation 
\begin{equation*}
    0 = \dfrac{1}{n} \sum_{i=1}^n \chi(P_n, O_i).
\end{equation*}
For convenience, first define 
\begin{equation*}
    \Delta = E_P\left[\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)} \bigg| S=1, V=1\right].
\end{equation*}
We solve 
\begin{align*}
    &\dfrac{1}{\Delta \sigma} \bigg(\dfrac{1}{n} \sum_{i=1}^n  S_i V_i Y_i^* - \dfrac{1}{n} \sum_{i=1}^n \Psi(P_n) \bigg[  S_i (1 - V_i)\{Y_i^* - \mu_0(X_i)\}\dfrac{\pi(X_i)\{1 - \mu_1(X_i)\}}{\{1 - \pi(X_i)\}\{1 - \mu_0(X_i)\}^2} \\
    &\qquad +  S_i V_i\{1-Y_i^*\}\dfrac{\mu_0(X_i)}{1 - \mu_0(X_i)}\bigg]\bigg) = 0
\end{align*}
for $\Psi(P_n)$ and obtain the estimator
\begin{equation*}
    \widehat{\Psi}_{dr} = \dfrac{\sum_{i=1}^n   S_i V_i Y_i^*}{\sum_{i=1}^n S_i (1 - V_i)\{Y_i^* - \mu_0(X_i)\}\dfrac{\pi(X_i)\{1 - \mu_1(X_i)\}}{\{1 - \pi(X_i)\}\{1 - \mu_0(X_i)\}^2} +  S_i V_i\{1-Y_i^*\}\dfrac{\mu_0(X_i)}{1 - \mu_0(X_i)}}.
\end{equation*}

Now consider the case that we have $m$ observations from $O_{tnd} \equiv (X, V, S=1, Y^*)$, i.e. the sampling design of the test-negative study. Note that summands in numerator and denominator are zero when $S_i=0$ such that $\widehat{\Psi}_{dr} = \widehat{\Psi}^*_{dr}$, where
\begin{equation*}
    \widehat{\Psi}^*_{dr} = \dfrac{\sum_{i=1}^{m}  V_i Y_i^*}{\sum_{i=1}^m(1 - V_i)\{Y_i^* - \mu_0(X_i)\}\dfrac{\pi(X_i)\{1 - \mu_1(X_i)\}}{\{1 - \pi(X_i)\}\{1 - \mu_0(X_i)\}^2} + V_i\{1-Y_i^*\}\dfrac{\mu_0(X_i)}{1 - \mu_0(X_i)}}.
\end{equation*}
and $\widehat{\Psi}^*_{dr}$ is estimable from using $O_{tnd}$.
% Using the plug-in results above an initial estimator of $\Psi$ is
% \begin{equation*}
%         \widehat{\Psi}_{0} = \dfrac{\sum_{i=1}^n V_i Y^*_i}{\sum_{i=1}^n V_i \mu_0(X_i)\dfrac{1 - \mu_1(X_i)}{1 - \mu_0(X_i)}},
%     \end{equation*}
% As defined in \textcite{hines_demystifying_2022}, the one-step corrected estimator is given by
% \begin{equation*}
%     \widehat{\Psi}_{1} = \widehat{\Psi}_{0} + \dfrac{1}{n} \sum_{i=1}^n \chi(P_n, O_i) 
% \end{equation*}
% which is equal to
% \begin{align*}
%     \widehat{\Psi}_{1} &= \widehat{\Psi}_{0} + \dfrac{ \sum_{i=1}^n V_i Y_i^*}{ \sum_{i=1}^n V_i \mu_0(X_i) \dfrac{\{1 - \mu_1(X_i)\}}{1 - \mu_0(X_i)}} - \dfrac{E_P(Y^*\mid S=1, V=1)}{\left[E_P\left\{\dfrac{\mu_0(X)\{1 - \mu_1(X)\}}{1 - \mu_0(X)}\bigg| S=1, V=1\right\}\right]^2} \\
%     &\qquad \times \bigg[ \dfrac{S(1 - V)}{\sigma}\{Y^* - \mu_0(X)\}\dfrac{\pi(X)\{1 - \mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \dfrac{S V}{\sigma}\{1-Y^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg]
% \end{align*}
% Specify the propensity score model $\pi(x;\alpha)$ and the outcome regression model $\mu_v(x;\beta)$. The nuisance parameters may be estimated by solving estimating equations
%     \begin{align*}
%         \sum_{j=1}^n X_j\{V_j - \pi(X_j;\alpha)\} &= 0;\\
%         \sum_{j=1}^n \begin{pmatrix}
%             X_j\\V_j
%         \end{pmatrix}\{Y^*_j - \mu_{V_j}(X_j;\beta)\}=0.
%     \end{align*}
%     Denote the resulting estimators as $\hat\alpha$, $\hat\beta$.
%     \item Let $\hat \Pi$ be an estimator for $\Pi\equiv \int \dfrac{\mu_0(x)\{1 - \mu_1(x)\}}{1 - \mu_0(x)}\pi(x)f_s(x)dx$, for example,
%     $$\hat\Pi = \dfrac{1}{n}\sum_{j=1}^n \dfrac{\mu_0(X_j;\hat\beta)\{1 - \mu_1(X_j;\hat\beta)\}}{1 - \mu_0(X_j;\hat\beta)}\pi(X_j;\hat \alpha).$$

%     The initial estimator of $\Psi$ is
%     $$\hat\Psi_0 = \dfrac{\dfrac{1}{n}\sum_{j=1}^n Y^*_jV_j}{\hat\Pi}.$$
%     \item The one-step corrected estimator is
%     \begin{align*}
%         \hat\Psi_1 &= \hat\Psi_0 +\dfrac{1}{n}\sum_{i=1}^n \bigg\{\dfrac{V_jY^*_j}{\hat\Pi}-\\&\qquad \dfrac{\dfrac{1}{n}\sum_{k=1}^n Y^*_kV_k}{\hat\Pi^2}\bigg[S_j(1 - V_j)\{Y^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + S_jV_j\{1-Y^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]\bigg\}\\
%         &= 2\hat\Psi_0-\dfrac{\hat\Psi_0}{\hat\Pi}\dfrac{1}{n}\sum_{j=1}^n\bigg[S_j(1 - V_j)\{Y^*_j - \mu_0(X_j;\hat\beta)\}\dfrac{\pi(X_j;\hat\alpha)\{1 - \mu_1(X_j;\hat\beta)\}}{\{1 - \pi(X_j;\hat\alpha)\}\{1 - \mu_0(X_j;\hat\beta)\}^2} + S_jV_j\{1-Y^*_j\}\dfrac{\mu_0(X_j;\hat\beta)}{1-\mu_0(X_j;\hat\beta)}\bigg]
%     \end{align*}

\subsection{Proof of double robustness}\label{sec:dr}
To prove the unique double robustness property of $\widehat{\Psi}^*_{dr}$, it suffices to show that 
\begin{align*}
    E\bigg[(1 - V_i)\{Y_i^* - \mu^\dagger_0(X_i)\}\dfrac{\pi^\dagger(X_i)\{1 - \mu^\dagger_1(X_i)\}}{\{1 - \pi^\dagger(X_i)\}\{1 - \mu^\dagger_0(X_i)\}^2} + V_i\{1-Y_i^*\}\dfrac{\mu^\dagger_0(X_i)}{\{1 - \mu^\dagger_0(X_i)\}}\bigg| S=1\bigg] = \dfrac{\Delta}{\sigma}
\end{align*}
where $\Delta$ is defined as previously, if either:
\begin{enumerate}
    \item $\mu_1^\dagger(X) = \mu_1(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$
    \item $\pi^\dagger(X) = \pi(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$
\end{enumerate}
hold.
\vspace{2em}

\begin{proof}
    Note that, by the invariance of the odds ratio, we have that
    \begin{equation*}
        \dfrac{1 - \mu_1(X)}{1 - \mu_0(X)} = \dfrac{\pi_0(X)}{1 - \pi_0(X)}.
    \end{equation*}
    First, suppose that $\mu_1^\dagger(X) = \mu_1(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$. Then we have that 
    \begin{align*}
            &E\bigg[(1 - V)\{Y^* - \mu^\dagger_0(X)\}\dfrac{\pi^\dagger(X)\{1 - \mu^\dagger_1(X)\}}{\{1 - \pi^\dagger(X)\}\{1 - \mu^\dagger_0(X)\}^2} + V\{1-Y^*\}\dfrac{\mu^\dagger_0(X)}{\{1 - \mu^\dagger_0(X)\}}\bigg| S=1\bigg]\\
            &= E\bigg[(1 - V)\{Y^* - \mu_0(X)\}\dfrac{\pi^\dagger(X)\{1 - \mu_1(X)\}}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}^2} + V\{1-Y^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg| S=1\bigg] \\
            &= E\bigg[\dfrac{(1 - V)\pi(X)}{1 - \pi(X)}\{Y^* - \mu_0(X)\}\dfrac{\pi^\dagger(X)}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}} + V\{1-Y^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg| S=1\bigg] \\
            &= E\bigg[(1 - V)\dfrac{\pi(X)}{1 - \pi(X)}\dfrac{\pi^\dagger(X)}{1 - \pi^\dagger(X)} \dfrac{Y^*}{1 - \mu_0(X)} - (1 - V)\dfrac{\pi(X)}{1 - \pi(X)}\dfrac{\pi^\dagger(X)}{1 - \pi^\dagger(X)} \dfrac{\mu_0(X)}{1 - \mu_0(X)} \\
            &\qquad +  V\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}} - Y^*V \dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}} \bigg| S=1\bigg] \\
            %&= E\bigg[E(1 - V | S=1, Y^*, X)\{Y^* - \mu_0(X)\}\dfrac{\pi^\dagger(X)\{1 - \mu_1(X)\}}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}^2} + V\{1-Y^*\}\dfrac{\mu_0(X)}{\{1 - \mu_0(X)\}}\bigg| S=1\bigg] \\
            &= E\bigg[(1 - V) \dfrac{\pi(X)}{1 - \pi(X)}\dfrac{\dot\pi(X)}{1 - \dot\pi(X)}\bigg| S=1, Y^*=1\bigg]\Pr(Y^*=1\mid  S=1) \\
            &\qquad - E\bigg[\dfrac{(1 - V)\pi(X)}{1 - \pi(X)}\dfrac{\pi^\dagger(X)\mu_0(X)}{\{1 - \pi^\dagger(X)\}\{1 - \mu_0(X)\}} + \dfrac{\mu_0(X)}{1-\mu_0(X)}\bigg| S = 1, Y^* = 0 \bigg]\Pr(Y^*=0\mid  S=1) \\
            &\qquad + E\bigg[\{1-Y^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\bigg| S = 1, V = 1\bigg] \\
            &= E\bigg[E \{Y^*-  \mu_0(X)\mid S=1, V=0, X\}\dfrac{\dot\pi(X)\{1 - \mu_1(X)\}}{\{1 - \dot\pi(X)\}\{1 - \mu_0(X)\}^2} \mid S=1, V = 0\bigg] \\
            &\qquad \times \Pr(V=0\mid  S=1) + E\bigg[\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid  S=1, V = 1\bigg] \Pr(V =1 \mid S = 1)\\
            &= 0 + \Pi = \Pi
        \end{align*}
    Second, suppose that $\pi^\dagger(X) = \pi(X)$ and $\mu_0^\dagger(X) = \mu_0(X)$. 
\end{proof}
    \begin{enumerate}
        \item If $\dot\mu_v(\cdot)=\mu_v(\cdot)$ a.s. for $v=0,1$, then
        
    \item If $\dot\pi(\cdot)=\pi(\cdot)$ and $\dot\mu_0(\cdot)=\mu_0(\cdot)$ a.s., then 
\begin{align*}
    &E\bigg[ (1-V)\{Y^* -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + V\{1-Y^*\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    =& E\bigg[ \{1 - \pi(X)\}\{\mu_0(X) -  \mu_0(X)\}\dfrac{\pi(X)\{1 - \dot\mu_1(X)\}}{\{1 - \pi(X)\}\{1 - \mu_0(X)\}^2} + \pi(X)\{1-\mu_1(X)\}\dfrac{\mu_0(X)}{1-\mu_0(X)}\mid S=1\bigg]\\
    &= \Delta
\end{align*}


    \end{enumerate}
    
    \newpage
    