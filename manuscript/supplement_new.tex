
\title{\textit{eAppendix:} Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding}
\date{ }
\maketitle
\pagenumbering{arabic}% resets `page` counter to 1

\textbf{This file contains:}

\textbf{eAppendices 1-9, eTables 1-4, eFigures 1-2}
\newpage 
\begin{appendix}
    \renewcommand{\thesection}{eAppendix \arabic{section}}
    \renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
    
    \renewcommand{\figurename}{eFigure}
    %\renewcommand{\thefigure}{A\arabic{figure}}
    \setcounter{figure}{0}

    \renewcommand{\tablename}{eTable}
    %\renewcommand{\thetable}{A\arabic{table}}
    \setcounter{table}{0}
    
    \renewcommand{\theequation}{A\arabic{equation}}
    \setcounter{equation}{0}

    \singlespacing
     \part{ } % Start the appendix part
    \parttoc % Insert the appendix TOC

%    \appendixwithtoc
    \newpage
    \begin{refsection}  

    \section{A nonparametric odds ratio model}\label{sec:or_model}
Following \textcite{tchetgen_universal_2023}, we parameterize a unit's contribution to the likelihood for the potential outcome under no treatment $Y^0$ conditional on $V$ and covariates $X$ and assuming independent sampling. Let
\begin{align*}
    h(y, x) &= f(Y^0 = y | V = 0, X = x), \\
    \beta(y,  x) &= \log \dfrac{f(Y^0 = y | V = 1, X = x)f(Y^0 = y_{ref} | V = 0, X = x)}{f(Y^0 = y_{ref} | V = 1, X = x)f(Y^0 = y | V = 0, X = x)},
\end{align*}
where $h(y, x) $ is the conditional density among the unvaccinated and the function $ \beta(y,  x)$ is the log of the generalized odds ratio function with $\beta(y_{ref},  x) = 0$ for user-specified reference values $y_{ref}$. Here we use $y_{ref} = 0$. Thus, $\beta(y,  x) = 0$ implies no unmeasured confounding for symptomatic infection or testing given $X = x$ and $\beta(y,  x) \neq 0$ encodes the degree of unmeasured confounding bias at the distributional level. 

This parameterization implies:
\begin{align*}
    f(Y^0 = y | V = 1, X = x) \propto h(y, x) \exp\{\beta(y, x)\}
\end{align*}
with 
\begin{align*}
    f(Y^0 = y | V = 1, X = x)  = \dfrac{h(y, x) \exp\{\beta(y, x)\}}{\int h(y, x) \exp\{\beta(y, x)\} \;dy}
\end{align*}
where $\int h(y, x) \exp\{\beta(y, x)\} \;dy$ is a proportionality or normalizing constant, ensuring the left-hand side is a proper density. This parameterization can, in principle, represent any likelihood function one might encounter in practice and therefore imposes no restrictions on the data generation process (i.e. it is completely nonparameteric). 

For our focal outcome of testing positive, $Y = 2$, we have:
\begin{align*}
    \Pr(Y^0 = 2 | V = 1, X = x) = \dfrac{\Pr(Y^0 = 2 | V = 0, X = x)\exp\{\beta(2, x)\}}{\int f(Y^0=y | V = 0, X = x)\exp\{\beta(y, x)\} \;dy}.
\end{align*}
Because $Y$ has only three levels, the denominator may be written as
\begin{align*}
    \int & f(Y^0=y | V = 0, X = x) \exp\{\beta(y, x)\} \;dy =  \\
    &= \sum_{y=0}^2\Pr(Y^0=y| V = 0, X = x)\exp\{\beta(y, x)\} \\
    &= \dfrac{\Pr(Y^0=0| V = 0, X = x)}{\Pr(Y^0=0| V = 1, X = x)} \bigg\{\Pr(Y^0=2| V = 1, X = x) + \Pr(Y^0=1| V = 1, X = x) \\
    &\qquad   + \Pr(Y^0=0| V = 1, X = x) \bigg\} \\  
    &= \dfrac{\Pr(Y^0=0| V = 0, X = x)}{\Pr(Y^0=0| V = 1, X = x)}
\end{align*}
and therefore 
\begin{align*}
    \Pr(Y^0 = 2 | V = 1, X = x) = \Pr(Y^0 = 2 | V = 0, X = x)\exp\{\beta(2, x)\}\dfrac{\Pr(Y^0=0| V = 1, X = x)}{\Pr(Y^0=0| V = 0, X = x)}.
\end{align*}
Under Assumption \ref{app_ass3}, $\beta(2, x) = \beta(1, x)$ and thus we have: 
\begin{align*}
    \Pr(Y^0 = 2 | V = 1, X = x) = \Pr(Y^0 = 2 | V = 0, X = x)\dfrac{\Pr(Y^0 = 1 | V = 1, X = x)}{\Pr(Y^0 = 1| V = 0, X = x)},
\end{align*}
and, re-arranging, we find
\begin{align*}
    \dfrac{\Pr(Y^0 = 2| V = 1, X = x)}{\Pr(Y^0 = 2| V = 0, X = x)} = \dfrac{\Pr(Y^0 = 1 | V = 1, X = x)}{\Pr(Y^0 = 1 | V = 0, X = x)},
\end{align*}
which is expression \ref{eqn:proxy} in the main text. 

By the invariance of odds ratios 
\begin{align*}
    \beta(y, x) = \log \dfrac{\Pr(V = 1 | Y^0 = y, X = x)\Pr(V = 0 | Y^0 = 0, X = x)}{\Pr(V = 0 | Y^0 = y, X = x)\Pr(V = 1 | Y^0 = 0, X = x)}.
\end{align*}
% and therefore we have that
% \begin{align*}
%     \log \dfrac{\pi(y, x)}{1 - \pi(y, x)} = \eta(x) + \beta(y, x)
% \end{align*}
% where
% \begin{align*}
%     &\pi(y, x) = \Pr(V = 1 | Y^0 = y, X = x), \\
%     &\eta(x) = \log \dfrac{\Pr(V = 1 | Y^0 = 0, X = x)}{\Pr(V = 0 | Y^0 = 0, X = x)}.
% \end{align*}
Under Assumption \ref{app_ass3}, again $\beta(2, x) = \beta(1, x)$ implying: 
\begin{align*}
    \dfrac{\Pr(V = 1 | Y^0 = 2, X = x)}{\Pr(V = 0 | Y^0 = 2, X = x)} = \dfrac{\Pr(V = 1 | Y^0 = 1, X = x)}{\Pr(V = 0 | Y^0 = 1, X = x)},
\end{align*}
which is also discussed in the main text. 


\newpage 
\section{Extended discussion of equi-confounding assumption}\label{sec:mechanisms}

    \subsection{Unmeasured health-seeking behavior}
    Among the stated motivations for the test-negative design when it was first introduced was the potential for the TND to adjust for unmeasured health-seeking behavior. Early observational studies of flu vaccines tended to overstate their effectiveness relative to randomized trials. At the time, it was hypothesized that the bias may have been caused by differential health-seeking behavior, as healthier adults were more likely to get vaccinated and also engage in other health behaviors, such as hand-washing and personal distancing, that reduce risk of infection. While hard to measure as a traditional covariate, similar health-seeking might also be revealed by an individual's propensity to pursue a test when sick and therefore conditioning on receipt of a test may effectively resolve the issue.
    
    Under our framework, if unmeasured health-seeking exerts equal influence on the risk of infection for test-positive and test-negative illness then it satisfies Assumption \ref{ass3a}, i.e.
    \[\dfrac{\Pr(I^0=2|V=1,X)}{\Pr(I^0=2|V=0,X)}=\dfrac{\Pr(I^0=1|V=1,X)}{\Pr(I^0=1|V=0,X)}.\]
    This may be more plausible when the other risk-reducing interventions that these individuals engage in are not specific to the focal illness, but instead affect risk of other infections similarly. 
    
    At the same time, if unmeasured health-seeking behavior results in equal propensity for pursuing a test when sick then it also satisfies Assumption \ref{ass3b}
    \[\dfrac{\Pr(T^0 = 1 | I^0 =2, V =1, X)}{\Pr(T^0 = 1 | I^0 =2, V =0, X)}=\dfrac{\Pr(T^0 = 1 | I^0 =1, V =1, X)}{\Pr(T^0 = 1 | I^0 =1, V =0, X)}.\]
    The plausibility of this assumption can be increased by certain design elements of the TND. Namely, the symptom screen, when well-defined and effectively implemented, equalizes the conditions under which a test is pursued while the (assumed) unavailability of testing outside health facilities ensures individuals are blind to the causative agent. 

    We note that the equi-confounding framework also allows for alternative equi-confounding structures involving unmeasured health-seeking that could lead to over- or under-optimistic estimates of vaccine effectiveness. 
    
    \subsection{Immortal time among vaccinated}
    Concerns have also been raised about the potential for design-induced biases in the test-negative design resulting from failure to explicitly emulate a randomized trial \cite{li2024comparison}. Here we show that, when the degree of design-induced bias is similar for test-positive and test-negative illnesses, they tend to cancel each other out and produce no bias in the TND estimate of vaccine effectiveness when they are equivalent. 
    
    Consider the emulation of a trial where events occurring in the first 28 days are excluded in both the vaccine arm and the control arm, as was specified in the original trials of the SARS-CoV-2 vaccines. This is often done because, biologically, it is believed that during this window the vaccine has not had time to provoke a sufficient immunological response and therefore provides no protection, although discarding these events may produce selection bias due to depletion of susceptibles when there is an effect. Regardless, attempts to emulate this result using a TND are challenged by the fact that the TND does not have a well-defined start of follow up and only the timing of vaccination and testing are known. Therefore, investigators instead discard cases where vaccination occurred less than 28 days prior to receiving a test, i.e. only among the vaccinated. This results in a form of immortal time bias as the vaccinated cannot have the outcome during this 28 day period. Assume for a second, to simplify exposition, that conditional on $X$ there is no other source of confounding. Note that under our framework, this would imply that 
    \begin{equation*}
         \frac{\Pr(Y^0 = 2 | V = 1, X)}{\Pr(Y^0 = 2 | V = 0, X)} = \delta_2(X)
    \end{equation*}
    for some $\delta_2(X) < 1$ as the person time in the first 28 days after receiving a vaccine is differentially discarded among those who received a vaccine compared to those who did not. However, if this exclusion of person time among the vaccinated is applied independently of the test result,  the bias also applies to the incidence of test-negative illness such that
    \begin{equation*}
         \frac{\Pr(Y^0 = 1 | V = 1, X)}{\Pr(Y^0 = 1 | V = 0, X)} = \delta_1(X)
    \end{equation*}
    for some $\delta_1(X) < 1$. In the special case that $\delta_2(X) = \delta_1(X)$, which could occur if the trends in the incidence of test-positive and test-negative illness among the unvaccinated are parallel over follow up (before and after the 28 day period where immortal time is accruing), we have equi-confounding and the design-induced bias is completely removed by $\Psi^*_{om}$ and $\Psi^*_{ipw}$.
    
    % \subsection{Assortativity of vaccination}

    \subsection{Correlated vaccination history}
    Prior work on the TND suggests that the design could be biased when there are other vaccines covering test-negative illness in addition to the focal vaccine for the test positive pathogen when vaccination behavior is correlated. For instance, denote by $V^*$ the vaccination against the test-negative pathogen and assume first $V^*$ is unavailable to the investigator. As shown in the DAG in eFigure \ref{fig:additional_dags} this would also violate our identifiability assumptions because it would imply the existence of a confounder that does not affect the test positive and test negative illnesses equally, i.e., 
    \[\dfrac{\Pr(I^0 = 2 | V =1, X)}{\Pr(I^0 = 2 | V =0, X)} \neq \dfrac{\Pr(I^0 = 1 | V =1, X)}{\Pr(I^0 = 1 | V =0, X)}. \]
    However, this could be resolved by measuring and adjusting for vaccination for the test negative illness. This further highlights the need to carefully consider and, where possible, document the source of the test negative illnesses. Investigators should also regularly collect full vaccination history.
    \[\dfrac{\Pr(I^0 = 2 | V =1, V^*, X)}{\Pr(I^0 = 2 | V =0, V^*, X)} = \dfrac{\Pr(I^0 = 1 | V =1,V^*,  X)}{\Pr(I^0 = 1 | V =0,V^*,  X)} \]

    \subsection{Home testing}
    Rapid diagnostic tests that provide quick confirmation of the source of a suspected infection are increasingly available for home use. The existence of these tests may invalidate Assumption \ref{ass3b} if (a) they affect the probability of seeking care when positive versus negative and (b) they are not uniformly available for all test positive and test negative illnesses. That is, under these conditions we would expect
    \[\dfrac{\Pr(T^0 = 1 | I^0 =2, V =1, X)}{\Pr(T^0 = 1 | I^0 =2, V =0, X)} \neq\dfrac{\Pr(T^0 = 1 | I^0 =1, V =1, X)}{\Pr(T^0 = 1 | I^0 =1, V =0, X)},\]
    and therefore our identifiability results in the main text would not hold. 
    
    However, let $R$ be an indicator of taking an at home rapid test (1: used rapid test, 0: did not use rapid test). Now imagine we were able to screen for use of a rapid test when recruiting for our test negative study. We may be willing to still assume 
    \[\dfrac{\Pr(T^0 = 1 | R =0, I^0 =2, V =1, X)}{\Pr(T^0 = 1 |  R =0,  I^0 =2, V =0, X)} = \dfrac{\Pr(T^0 = 1 |  R =0, I^0 =1, V =1, X)}{\Pr(T^0 = 1 |  R =0, I^0 =1, V =0, X)} \]
    In which case our identifiability results from the main text hold conditional on $R=0$. 

    Alternatively, multiplex rapid diagnostics tests that test for multiple pathogens simultaneously are increasingly available for home use. If one could limit test-negative controls to illnesses covered by multiplex testing then rapid at-home testing may affect test-seeking equally for test-positive and test-negative illnesses (unless one is perceived as more dangerous/life-threatening), in which case it may no longer bias our estimates.



    \begin{figure}[p]
    \centering
        \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.75cm, inner sep = 0pt,minimum size = 0.5pt,  very thick]
            \tikzstyle{every state}=[
              draw = none,
              fill = none
            ]
            \node[state] (x) {$X$};
            \node[state] (v) [right of=x] {$V$};
            \node[state] (i) [right of=v] {$I = 2$};
            \node[state] (t) [right of=i] {$T$};
            \node[state] (i1) [below of=i] {$I = 1$};
            \node[state] (u) [below of=x] {$U$};
            \node[state] (v1) [below of=v] {$V^*$};
   
            \path[->] (x) edge node {} (v);
            \path[->] (x) edge [out=45, in=135] node {} (i);
    
            \path[->] (v) edge node {} (i);
            
            \path[->] (i) edge [color=forestgreen] node {} (t);
            \path[->] (i1) edge [color=forestgreen] node {} (t);

            \path[->] (x) edge [out=45, in=135] node {} (t);
    
            \path[->] (v1) edge [dashed] node {} (i1);
            \path[->] (u) edge [dashed] node {} (v1);
            \path[->] (u) edge node {} (v);
            \path[->] (u) edge node {} (x);
            \path[->] (u) edge node {} (t);
            \path[->] (u) edge [color=bblue] node {} (i);
            \path[->] (u) edge [color=bblue, out=315, in=225] node  {} (i1);

        \end{tikzpicture}
        \caption{Correlated vaccination behavior}
        \end{subfigure}
        \begin{subfigure}[t]{0.9\textwidth}
        \centering
        \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.75cm, inner sep = 0pt,minimum size = 0.5pt,  very thick]
            \tikzstyle{every state}=[
              draw = none,
              fill = none
            ]
            \node[state] (x) {$X$};
            \node[state] (v) [right of=x] {$V$};
            \node[state] (i) [right of=v] {$I = 2$};
            \node[state] (r) [right of=i] {$R$};
            \node[state] (t) [right of=r] {$T$};
            \node[state] (i1) [below of=i] {$I = 1$};
            \node[state] (u) [below of=v] {$U$};
   
            \path[->] (x) edge node {} (v);
            \path[->] (x) edge [out=45, in=135] node {} (i);
    
            \path[->] (v) edge node {} (i);
            
            \path[->] (i) edge [forestgreen, out=45, in=135] node {} (t);
            \path[->] (i1) edge [forestgreen] node {} (t);

            \path[->] (i) edge [forestgreen] node {} (r);
            \path[->] (i1) edge [forestgreen] node {} (r);
            \path[->] (r) edge [dashed] node {} (t);

            \path[->] (x) edge [out=45, in=135] node {} (t);
    
            \path[->] (u) edge node {} (v);
            \path[->] (u) edge node {} (x);
            \path[->] (u) edge [out=315, in=240] node {} (t);
            \path[->] (u) edge [bblue] node {} (i);
            \path[->] (u) edge [bblue] node  {} (i1);

        \end{tikzpicture}
        \caption{At home testing}
        \end{subfigure}
        
        
    \caption{Causal DAGs showing example mechanisms where the equi-confounding assumption may be invalidated or require additional refinement.}\label{fig:additional_dags}
\end{figure}
\clearpage
\newpage
    \section{Proofs of main identifiability results} \label{sec:proofs}
    For convenience, we restate below the core identifiability conditions from the main text. 
    \begin{enumerate}[label=\upshape(A\arabic*), ref=A\arabic*]
    \item\label{app_ass1} \textit{Consistency of potential outcomes}. For all individuals $i$ and for $v \in \{0, 1\}$, we have $Y_i^v = Y_i$ when $V_i = v$. 
    \item\label{app_ass2} \textit{No effect of vaccination on testing negative and symptomatic among the vaccinated}. That is, $\Pr(Y^0=1 | V = 1, X) = \Pr(Y^1=1 | V = 1, X).$
    \item\label{app_ass3} \textit{Odds ratio equi-confounding}. Degree of unmeasured confounding bias on the odds ratio scale is the same for test-positive and test-negative illnesses, i.e. 
    $$OR_2(X) = OR_1(X), $$
    $$ \text{where } OR_y(X) = \frac{\Pr(Y^0 = y | V = 1, X)\Pr(Y^0 = 0 | V = 0, X)}{\Pr(Y^0 = 0 | V = 1, X)\Pr(Y^0 = y | V = 0, X)}.$$
    \item\label{app_ass4} \textit{Overlap of vaccination among test-positives and test-negatives}. Define $\mathcal{S}_y(v)$ as the support of the law of $(Y^0 = y, V = v, X)$, then for $v$ in $\{0,1\}$, then it must be that $\mathcal{S}_2(1) \subseteq \mathcal{S}_2(0)$ and $\mathcal{S}_2(v) \subseteq \mathcal{S}_1(v).$
    %\item[(A5)] No direct effect of vaccination on test-seeking behavior among the vaccinated. That is, for $i$ in $\{1,2\}$, $\Pr[T^1 = 1 | I^1 = i, V = 1, X] = \Pr[T^0 = 1 | I^0 = i, V = 1, X].$
    \end{enumerate}

    
    \newpage
    \subsection{Proof of Proposition \ref{prop1}} \label{sec:proof1}
    
    \begin{proof}
    % We begin by showing that, under Assumption \ref{app_ass1} alone, the causal risk ratio for medically-attended illness among the vaccinated, $\Psi_{RRV}$, 
    % \begin{equation*}
    %     \Psi \equiv \dfrac{\Pr(Y^1=2|V=1)}{\Pr(Y^0=2|V=1)},
    % \end{equation*}
    % is equivalent to two expressions involving only the (unobserved) treatment-free potential outcome, $Y^0$, namely 
    % \begin{equation}
    %     \Psi^0_{om} \equiv \dfrac{\Pr(Y = 2 | V = 1)}{E\left[\Pr(Y = 2 | V = 0, X) \dfrac{\Pr(Y^0 = 2 | V = 1, X)}{\Pr(Y = 2 | V = 0, X)}\Big| V = 1 \right]}
    % \end{equation}
    % and 
    % \begin{equation}
    %     \Psi^0_{ipw} \equiv \dfrac{E\{V \mathbbm 1(Y = 2)\}}{E\left\{ (1 - V) \mathbbm 1(Y = 2) \dfrac{ \Pr(V = 1 | Y^0 = 2,  X)}{ \Pr(V = 0 | Y^0 = 2,  X)}\right\}}.
    % \end{equation}
    
    For the first expression, we have 
    \begin{align*}
        \Psi &= \dfrac{\Pr(Y^1=2|V=1)}{\Pr(Y^0=2|V=1)} \\
        &= \dfrac{\Pr(Y^1=2|V=1)}{E[E\{\mathbbm 1 (Y^0 = 2) | V = 1, X\} | V = 1]} \\
        &= \dfrac{\Pr(Y^1=2|V=1)}{E\left\{\int \mathbbm 1 (Y^0 = 2) f(Y^0 = y | V = 1, X) dy \mid  V = 1\right\}} \\
        &= \dfrac{\Pr(Y^1=2|V=1)}{E\left\{\int \mathbbm 1 (Y^0 = 2) \dfrac{f(Y^0 = y | V = 0, X) \exp\{\beta(y, X)\}}{\int f(Y^0 = y | V = 0, X) \exp\{\beta(y, X)\}dy}dy \Big|  V = 1\right\}} \\
        &= \dfrac{\Pr(Y^1=2|V=1)}{E\left\{ \Pr(Y^0 = 2 | V = 0, X) \dfrac{\Pr(Y^0 = 1 | V = 1, X)}{\Pr(Y^0 = 1 | V = 0, X)} \Big|  V = 1\right\}} \\
         &= \dfrac{\Pr(Y^1=2|V=1)}{E\left\{ \Pr(Y^0 = 2 | V = 0, X) \dfrac{\Pr(Y^1 = 1 | V = 1, X)}{\Pr(Y^0 = 1 | V = 0, X)} \Big| V = 1\right\}} \\
        &= \dfrac{\Pr(Y=2|V=1)}{E\left\{ \Pr(Y = 2 | V = 0, X) \dfrac{\Pr(Y = 1 | V = 1, X)}{\Pr(Y = 1 | V = 0, X)} \Big|  V = 1\right\}} \\
        &= \dfrac{\Pr(Y=2|V=1)}{E\left\{ \mathbbm 1(Y = 1) \dfrac{\Pr(Y = 2 | V = 0, X)}{\Pr(Y = 1 | V = 0, X)} \Big| V = 1\right\}} \\
        &= \dfrac{E\left\{V \mathbbm 1(Y = 2)\right\}}{E\left\{ V \mathbbm 1(Y=1) \dfrac{\Pr(Y = 2 | V = 0, X) }{\Pr(Y = 1 | V = 0, X)}  \right\}}.
    \end{align*}
    The first line restates the definition. The second uses the law of iterated expectation. The third applies the definition of conditional expectation. The fourth applies the nonparametric odds ratio model in \ref{sec:or_model}. The fifth applies the equi-confounding assumption (Assumption \ref{app_ass3}) in which $\beta(2,X) = \beta(1,X)$ and simplifies. The sixth applies the no effect on test-negative illness (Assumption \ref{app_ass2}). The seventh applies consistency (Assumption \ref{app_ass1}). The eigth reverses the iterated expectation and the ninth uses the definition of conditional expectation with the $\Pr(V=1)$ terms in numerator and denominator canceling.
    
    For the second, we have 
    \begin{align*}
        \Psi &= \dfrac{\Pr(Y^1=2|V=1)}{\Pr(Y^0=2|V=1)} \\
        &= \dfrac{E\left\{\dfrac{V}{\Pr(V = 1)} \mathbbm 1 (Y^1 = 2)\right\}}{E\left\{\dfrac{V}{\Pr(V=1)}\mathbbm 1 (Y^0 = 2)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (Y^1 = 2)\}}{E\left[ E\{V \mathbbm 1 (Y^0 = 2)| Y^0, X\}\right]} \\
        &= \dfrac{E\{V \mathbbm 1 (Y^1 = 2)\}}{E\left\{\mathbbm 1 (Y^0 = 2) E(V | Y^0 = 2, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (Y^1 = 2)\}}{E\left\{\mathbbm 1 (Y^0 = 2) \dfrac{ E(V | Y^0 = 2, X)}{ E(1-V | Y^0 = 2, X)}E(1-V|Y^0 = 2, X)\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (Y^1 =2)\}}{E\left\{(1 - V)\mathbbm 1 (Y^0 = 2) \dfrac{ \Pr(V=1 | Y^0 = 2, X)}{ \Pr(V=0 | Y^0 = 2, X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (Y^1 =2)\}}{E\left\{(1 - V)\mathbbm 1 (Y^0 = 2) \dfrac{ \Pr(V=1 | Y^1 = 1, X)}{ \Pr(V=0 | Y^0 = 1, X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (Y =2)\}}{E\left\{(1 - V)\mathbbm 1 (Y = 2) \dfrac{ \Pr(V=1 | Y = 1, X)}{ \Pr(V=0 | Y = 1, X)}\right\}}.
    \end{align*}
    The first line restates the definition. The second uses the definition of conditional expectation. The third applies the law of iterated expectation and cancels the $\Pr(V=1)$ terms in the numerator and denominator. The fourth multiplies by one. The fifth converts expectation to probabilities. The sixth applies the equi-confounding assumption (Assumption \ref{app_ass2}) and the no effect on test-negative illness (Assumption \ref{app_ass2}). The seventh applies consistency. 
     
    \end{proof}
    \newpage

    \subsection{Proof of Proposition \ref{prop2}}\label{sec:proof2}
    
    \begin{proof}
    Under the sampling design for a TND we define expectations as 
    \begin{align*}
        E_{TND}(Y) &= \int y f(y|S=1)dy \\
        &= E(Y|S=1)
    \end{align*}
    where $S = \mathbbm 1(Y \neq 0)$. Therefore evaluating the first expression under the sampling design we have,
        \begin{align*}
        \Psi^*_{om} &= \dfrac{E_{TND}\left\{V \mathbbm 1(Y = 2)\right\}}{E_{TND}\left\{ V \mathbbm 1(Y=1) \dfrac{E_{TND}\{\mathbbm 1 (Y = 2 )| V = 0, X\} }{E_{TND}\{\mathbbm 1 (Y = 1) | V = 0, X\}}  \right\}} \\
        &=\dfrac{E\left\{V \mathbbm 1(Y = 2) | S = 1\right\}}{E\left\{ V \mathbbm 1(Y=1)  \dfrac{E\{\mathbbm 1 (Y = 2 )| S=1, V = 0, X\} }{E\{\mathbbm 1 (Y = 1) | S= 1, V = 0, X\}}  \Big| S = 1\right\}} \\
        &=\dfrac{E\left\{\dfrac{1}{\Pr(S=1)}V \mathbbm 1(Y = 2, S = 1) \right\}}{E\left\{\dfrac{1}{\Pr(S=1)} V \mathbbm 1(Y=1, S=1)  \dfrac{E\left\{\frac{\mathbbm 1 (Y = 2, S=1 )}{\Pr(S=1|V=0,X)} \big| V = 0, X\right\} }{E\left\{\frac{\mathbbm 1 (Y = 1, S=1 )}{\Pr(S=1|V=0,X)} \big| V = 0, X\right\} }  \right\}} \\
        &=\dfrac{E\left\{V \mathbbm 1(Y = 2, S = 1) \right\}}{E\left\{ V \mathbbm 1(Y=1, S=1)  \dfrac{E\left\{\mathbbm 1 (Y = 2, S=1 ) | V = 0, X\right\} }{E\left\{\mathbbm 1 (Y = 1, S=1 ) | V = 0, X\right\}  }  \right\}} \\
        &=\dfrac{E\left\{V \mathbbm 1(Y = 2, Y\neq0) \right\}}{E\left\{ V \mathbbm 1(Y=1, Y\neq0)  \dfrac{E\left\{\mathbbm 1 (Y = 2, Y\neq0 ) | V = 0, X\right\} }{E\left\{\mathbbm 1 (Y = 1, Y\neq0 ) | V = 0, X\right\}  }  \right\}} \\
        &=\dfrac{E\left\{V \mathbbm 1(Y = 2) \right\}}{E\left\{ V \mathbbm 1(Y=1)  \dfrac{E\left\{\mathbbm 1 (Y = 2 ) | V = 0, X\right\} }{E\left\{\mathbbm 1 (Y = 1) | V = 0, X\right\}  }  \right\}} \\
        &=\dfrac{E\left\{V \mathbbm 1(Y = 2) \right\}}{E\left\{ V \mathbbm 1(Y=1)  \dfrac{\Pr(Y = 2  | V = 0, X) }{\Pr(Y = 1 | V = 0, X)  }  \right\}}.
    \end{align*}
    The first line and second lines apply the sampling design definition. The third applies the definition of conditional expectation. The fourth cancels selection probabilities $\Pr(S=1)$ and $\Pr(S=1|V=0,X)$ in numerator and denominator. The fifth uses the fact that $S = \mathbbm 1(Y \neq 0)$ for every individual under the sampling design. The sixth applies the definition of $Y$, i.e. $Y = 2 \implies Y \neq 0$. The last line convert expectations to corresponding probabilities. This shows that $\Psi^*_{om}$ is equivalent to $\Psi_{om}$ under the sampling design.
    
    For the second expression, similarly, we have,
    \begin{align*}
        \Psi^*_{ipw} &= \dfrac{E_{TND}\{V \mathbbm 1 (Y =2)\}}{E_{TND}\left\{(1 - V)\mathbbm 1 (Y = 2) \dfrac{ E_{TND}(V | Y = 1, X)}{ E_{TND}(1-V | Y = 1, X)}\right\}} \\
        &= \dfrac{E\{V \mathbbm 1 (Y =2) | S =1\}}{E\left\{(1 - V)\mathbbm 1 (Y = 2) \dfrac{ E(V | S= 1, Y = 1, X)}{ E(1-V | S = 1, Y = 1, X)}\Big| S = 1\right\}} \\
        &= \dfrac{E\left\{\dfrac{1}{\Pr(S=1)} V \mathbbm 1 (Y =2, S=1) \right\}}{E\left\{\dfrac{1}{\Pr(S=1)}(1 - V)\mathbbm 1 (Y = 2, S = 1) \dfrac{ E(V | S= 1, Y = 1, X)}{ E(1-V | S = 1, Y = 1, X)}\right\}} \\
        &= \dfrac{E\left\{ V \mathbbm 1 (Y =2, S=1) \right\}}{E\left\{(1 - V)\mathbbm 1 (Y = 2, S = 1) \dfrac{ E(V | S= 1, Y = 1, X)}{ E(1-V | S = 1, Y = 1, X)}\right\}} \\
        &= \dfrac{E\left\{ V \mathbbm 1 (Y =2, Y\neq0) \right\}}{E\left\{(1 - V)\mathbbm 1 (Y = 2, Y\neq0) \dfrac{ E(V | Y\neq0, Y = 1, X)}{ E(1-V | Y\neq0, Y = 1, X)}\right\}} \\
        &= \dfrac{E\left\{ V \mathbbm 1 (Y =2) \right\}}{E\left\{(1 - V)\mathbbm 1 (Y = 2) \dfrac{ E(V | Y = 1, X)}{ E(1-V | Y = 1, X)}\right\}} \\
        &= \dfrac{E\left\{ V \mathbbm 1 (Y =2) \right\}}{E\left\{(1 - V)\mathbbm 1 (Y = 2) \dfrac{ \Pr(V=1 | Y = 1, X)}{ \Pr(V=0 | Y = 1, X)}\right\}} .
    \end{align*}
    The first line and second lines apply the sampling design definition. The third applies the definition of conditional expectation. The fourth cancels selection probabilities $\Pr(S=1)$ in numerator and denominator. The fifth uses the fact that $S = \mathbbm 1(Y \neq 0)$ for every individual under the sampling design. The sixth applies the definition of $Y$, i.e. $Y = 2 \implies Y \neq 0$. The last line convert expectations to corresponding probabilities. This shows that $\Psi^*_{ipw}$ is equivalent to $\Psi_{ipw}$ under the sampling design.
    
    \end{proof}
    
    \newpage
    \section{Additional Estimation Details}\label{sec:app_estimation}
    \subsection{Cohort estimators}
    The identified expressions $\Psi_{om}$ and $\Psi_{ipw}$ in Proposition \ref{prop1} suggest two plug-in estimators for $\Psi$ when one has access to the full sample (including those untested) from the underlying cohort, i.e. $O_{cohort} = \{(X_i, V_i, Y_i) : i = 1, \ldots, N\}$. First, an estimator based on modeling the outcome
    \begin{equation}\label{eqn:om_estimator_cohort}
        \widehat{\Psi}_{om} = \dfrac{\sum_{i=1}^N V_i \mathbbm 1(Y_i=2)}{\sum_{i=1}^N V_i\mathbbm 1(Y_i=1) \dfrac{\widehat{\mu}_{2,0}(X_i)}{\widehat{\mu}_{1,0}(X_i)}},
    \end{equation}
    where $\mu_{y,v}(X) = \Pr(Y=y \mid V=v, X)$ is the probability of testing positive among those with vaccine status $V = v$ in the full sample which could be obtained from, for example, a multinomial logistic regression. 
    
    Second, an inverse probability weighting estimator
    \begin{equation}\label{eqn:ipw_estimator_cohort}
        \widehat{\Psi}_{ipw} = \dfrac{\sum_{i=1}^N V_i \mathbbm 1(Y_i = 2)}{\sum_{i=1}^N (1 - V_i) \mathbbm 1 (Y_i = 2) \dfrac{\widehat{\pi}_1(X_i)}{1 - \widehat{\pi}_1(X_i)}},
    \end{equation}
    where $\pi_y(X) = \Pr(V=1\mid Y=y, X)$ is the  probability of vaccination among those who test-negative. The terms $\widehat{\mu}_{1,0}(X)$, $\widehat{\mu}_{2,0}(X)$ and $\widehat{\pi}_1(X)$ are all nuisance functions. 

    \subsection{TND estimators}
    Alternatively, the identified expressions $\Psi^*_{om}$ and $\Psi^*_{ipw}$ in Proposition \ref{prop2} suggest two plug-in estimators for $\Psi$ under TND sampling, i.e. $O_{TND} = \{(X_i, V_i, S_i=1, Y^*_i) : i = 1, \ldots, n\}$. First, an estimator based on modeling the outcome
    \begin{equation}\label{eqn:om_estimator_tnd}
        \widehat{\Psi}_{om}^* = \dfrac{\sum_{i=1}^n V_i Y^*_i}{\sum_{i=1}^n V_i \widehat{\mu}^*_0(X_i)\dfrac{1 - \widehat{\mu}^*_1(X_i)}{1 - \widehat{\mu}^*_0(X_i)}},
    \end{equation}
    where $\mu^*_v(X) = \Pr(Y^*=1 \mid S=1, V=v, X)$ is the probability of testing positive among those with vaccine status $V = v$ in the full sample.

    Second, an inverse probability weighting estimator
    \begin{equation}\label{eqn:ipw_estimator_tnd}
        \widehat{\Psi}_{ipw}^* = \dfrac{\sum_{i=1}^n V_i Y^*_i}{\sum_{i=1}^n (1 - V_i) Y^*_i \dfrac{\widehat{\pi}_0^*(X_i)}{1 - \widehat{\pi}_0^*(X_i)}},
    \end{equation}
    where $\pi^*_0(X) = \Pr(V=1\mid S=1, Y^*=0, X)$ is the  probability of vaccination among those who test-negative. Both $\widehat{\mu}^*_v(X)$ and $\widehat{\pi}^*_0(X)$ are nuisance functions. 
    \newpage

    \subsection{Estimating Equations}
    Here we provide a detailed description of methods for obtaining standard errors and corresponding confidence intervals for $\widehat{\Psi}_{om}^*$ and $\widehat{\Psi}_{ipw}^*$ based on stacked estimating equations. Throughout this section we presume models for nuisance functions are correctly specified. Although not the focus of this paper, a similar procedure could be used to obtain standard errors and confidence intervals for $\widehat{\Psi}_{om}$ and $\widehat{\Psi}_{ipw}$.

    We first introduce additional notation. As before, we observe data $O = (X, V, S=1, Y^*)$ for $i = 1, \ldots, n$, where $n$ is the number of observed units. Let $\mathbb{P}(g)$ denote the average of function $g(O)$  across $n$ units, i.e., $\mathbb{P}(g) = n^{-1}\sum_{i=1}^n g(O_{i})$. Let $\operatorname{expit}(x) = \exp(x)/{1 + \exp(x)}$. For  random variables $V$ and $W$ , let $V \overset{P}{\rightarrow} W$ and $V \overset{D}{\rightarrow} W$ denote $V$ converges to $W$ in probability and in distribution, respectively.

    The estimators $\widehat{\Psi}^*_{om}$ and $\widehat{\Psi}^*_{ipw}$ can be expressed via stacked estimating equations of the form:
    \begin{equation*}
        \psi(O; \nu, \Psi^*) = \begin{pmatrix}
            \psi_{\text{nuis}}(Y^*, V, X; \nu) \\
            \psi_{\text{effect}}(Y^*, V, X; \Psi^*)
        \end{pmatrix}
    \end{equation*}
    where the first estimates the parameters $\nu$ of a model for the nuisance function $\mu_v(X)$ and the second estimates the causal estimand $\Psi^*$ for the target parameter $\Psi$, with estimates obtained as the solution to
    \begin{equation*}
        0 = \mathbb{P}\big\{\psi(O; \widehat{\nu}, \widehat{\Psi}^*)\big\}.
    \end{equation*}
    We now consider examples for each estimator in turn. 
    
    For the outcome modeling estimator, following the suggestion in the main paper, one might specify a pooled over $V$ logistic regression model for the nuisance function, i.e. $\mu^*_V(X; \nu_{om}) = \operatorname{expit}\{(1, V, X, VX)'\nu_{om}\}$. In this case, the estimating equation is given by 
    \begin{equation*}
        \psi_{om}(O; \nu_{om}, \Psi^*_{om}) = \begin{pmatrix}
            (1, V, X, VX)'\left[ Y^* - \mu^*_V(X; \nu_{om})\right] \\
            V \left\{Y^* - (1 - Y^*) \dfrac{\mu^*_0(X; \widehat{\nu}_{om})}{1 - \mu^*_0(X; \widehat{\nu}_{om})}\Psi^*_{om}\right\}
        \end{pmatrix}.
    \end{equation*}

    For the inverse probability weighting estimator, again following the suggestion in the main paper, one might specify a logistic regression model for the nuisance function, i.e. $\pi^*_0(X; \nu_{ipw}) = \operatorname{expit}\{(1, X)'\nu_{ipw}\}$. In this case, the estimating equation is given by 
    \begin{equation*}
        \psi_{ipw}(O; \nu_{ipw}, \Psi^*_{ipw}) = \begin{pmatrix}
            (1, X)'\left[V(1-Y^*) - \pi^*_0(X; \nu_{ipw})\right] \\
            Y^* \left\{V - (1-V) \dfrac{\pi^*_0(X; \widehat{\nu}_{ipw})}{1 - \pi^*_0(X; \widehat{\nu}_{ipw})}\Psi^*_{ipw}\right\}
        \end{pmatrix}.
    \end{equation*}

    We can characterize the statistical properties of the estimator using M-estimation theory \cite{stefanski_calculus_2002}. Under regularity conditions, we have the following asymptotic normality of the estimators:

    \[\sqrt{n}\left\{\big(\widehat{\Psi}^*, \widehat{\nu}\big)'-\big(\underline{\Psi}^*, \underline{\nu}\big)^\prime \right\} \xrightarrow{D} N\left(0, V_1^{-1} V_2\left(V_1^{-1}\right)^{\prime}\right)\]
    where 
    \begin{align*}
        V_1&=\left.E\left\{\frac{\partial \psi\left(O ; \Psi^*, \nu\right)}{\partial\left(\Psi^*, \nu\right)^{\prime}}\right\}\right|_{\left(\Psi^*, \nu\right)=\left(\underline{\Psi}^*, \underline{\nu}\right)}, \\
        V_2&=E\left[\left\{\Psi\left(O ;\underline{\Psi}^*, \underline{\nu}\right)\right\}\left\{\Psi\left(O ; \underline{\Psi}^*, \underline{\nu}\right)\right\}^{\prime}\right]
    \end{align*}
and where $\left(\underline{\Psi}^*, \underline{\nu}\right)$ is the probability limit of the estimators. By the law of large numbers, we have $\underline{\Psi}^*_{om}=\Psi^*_{om}$ if  $\underline{\nu}_{om}=\nu_{om}$ and $\underline{\Psi}^*_{ipw}=\Psi^*_{ipw}$ if $\underline{\nu}_{ipw}=\nu_{ipw}$, which occurs when the nuisance functions are correctly specified.

A consistent variance estimator for $\widehat{\Psi}^*$ can be obtained by substituting the empirical analogs of $V_1$ and $V_2$, i.e., $\widehat{\sigma}^2=\widehat{V}_1^{-1} \widehat{V}_2(\widehat{V}_1^{-1})^{\prime}$. Confidence intervals can be constructed leveraging the asymptotic normality of $\widehat{\Psi}^*$ and the consistent variance estimator $\widehat{\sigma}^2$. A $100(1-\alpha) \%$ confidence interval for $\widehat{\Psi}^*$ is given as
$$
\left(\widehat{\Psi}^*-z_{1-\frac{\alpha}{2}} \frac{\widehat{\sigma}}{\sqrt{n}}, \widehat{\Psi}^*+z_{1-\frac{\alpha}{2}} \frac{\widehat{\sigma}}{\sqrt{n}}\right)
$$
where $z_\alpha$ is the $100 \alpha$th percentile of the standard normal distribution.

\subsection{Bootstrapping}
An alternative method for obtaining standard errors and confidence intervals for $\widehat{\Psi}^*_{om}$ or $\widehat{\Psi}^*_{ipw}$ is the  bootstrap \cite{diciccio_bootstrap_1987}. As in the main text, let $O = \{(X_i, V_i, S_i = 1, Y_i^*)\}_{i=1}^n$ represent the available data which are  independent realizations from unknown probability distribution $F_{\theta}$. 

\begin{enumerate}
    \item Draw bootstrap replicates $O_1, \ldots, O_B$ from $\widehat{F}_{\theta}$ by sampling with replacement from the dataset. 
    \item For each replicate, $O_b$, estimate $\Psi^*$ by first estimating the nuisance function, $\mu^*_v(X)$ or $\pi_0^*(X)$, and then plug into expression \ref{eqn:om_estimator_tnd} or \ref{eqn:ipw_estimator_tnd}, thereby obtaining estimates $\widehat{\Psi}^*_1, \ldots, \widehat{\Psi}^*_B$.
    \item Calculate the bootstrapped standard error from $\widehat{\sigma}_B = \sqrt{ \sum_{i=1}^{B} (\widehat{\Psi}^*_i - \overline{\Psi}^*)^2/(B-1)}$ where $\overline{\Psi}^* = B^{-1}\sum_{i=1}^B \widehat{\Psi}^*_i$.
    \item Form a $100(1-\alpha)\%$ confidence interval for $\Psi^{\star}$ via
    $$
    \left(\widehat{\Psi}^*-z_{1-\frac{\alpha}{2}} \frac{\widehat{\sigma}}{\sqrt{n}}, \widehat{\Psi}^*+z_{1-\frac{\alpha}{2}} \frac{\widehat{\sigma}}{\sqrt{n}}\right)
    $$
    where $z_\alpha$ is the $100 \alpha$th percentile of the standard normal distribution.
\end{enumerate}
 Steps 3 and 4 may be replaced by alternative procedures which, in some cases, may yield better performing intervals.
    \newpage

     \section{Doubly robust estimator under cohort sampling}\label{sec:eif_cohort}

Recall, the parameter of interest is the causal risk ratio among the vaccinated $\Psi = \Pr(Y^1=2|V=1)/\Pr(Y^0 = 2 | V = 1)$. We focus on the denominator $\psi = \Pr(Y^0 = 2 | V = 1)$ as identification of the numerator is trivial under causal consistency. Identification of $\psi$ under equi-confounding is structurally similar to Universal Difference-in-Differences (UDiD), with a key difference that the NCO and Outcome are mutually exclusive by design. Under the UDiD framework, \textcite{tchetgen_universal_2023} provide the following estimators of $\psi$:
\begin{equation} \label{eqn:udid1}
    E\left[\dfrac{E[\mathbbm 1(Y=2)\exp\{\beta(Y,X)\} | V = 0, X]}{E[\exp\{\beta(Y,X)\} | V = 0, X]}\bigg|V = 1\right]
\end{equation}
and 
\begin{equation}\label{eqn:udid2}
    \dfrac{E[(1-V)\mathbbm 1(Y=2)\exp\{\eta(X) + \beta(Y,X)\}]}{E[(1-V)\exp\{\eta(X) +\beta(Y,X)\}]}
\end{equation}
where
\begin{align*}
    \beta(y,X) &= \log\dfrac{\Pr(Y^0=y|V=1,X)\Pr(Y^0=0|V=0,X)}{\Pr(Y^0=0|V=1,X)\Pr(Y^0=y|V=0, X)} \\
    &= \log\dfrac{\Pr(V=1 | Y^0=y,X)\Pr(V=0|Y^0=0,X)}{\Pr(V=0|Y^0=y, X)\Pr(V=1|Y^0=0,X)} \\
    \eta(X) &= \log \dfrac{\Pr(V = 1 | Y^0 = 0, X)}{\Pr(V = 0 | Y^0 = 0, X)}
\end{align*}
and $\beta(1,X) = \beta(2,X)$ under equi-confounding. \textcite{tchetgen_universal_2023} also provide a doubly-robust estimator of $\psi$, given by 
\begin{equation}\label{eqn:udid3}
     E\left[\dfrac{(1 - V)}{\Pr(V=1)} \exp\{\eta(X) + \beta(Y,X)\}\{\mathbbm 1(Y = 2) - \xi(X)\} + \dfrac{V}{\Pr(V=1)}\xi(X)  \right]
\end{equation}
where 
\[\xi(X) = \dfrac{E[\mathbbm 1(Y=2)\exp\{\beta(Y,X)\} | V = 0, X]}{E[\exp\{\beta(Y,X)\} | V = 0, X]}.\]
Equation \ref{eqn:udid3} requires correct specification of $\beta(Y, X)$ as it is common to both equation \ref{eqn:udid1} and  \ref{eqn:udid2}.  However, assuming $\beta(Y,X)$ is correctly specified, equation \ref{eqn:udid3} is consistent if either of (1) a model for the outcome under $V=0$ or (2) a model for the odds of $V$ given the reference is correct. 


We begin by showing equations \ref{eqn:udid1} and \ref{eqn:udid2} are equivalent to the denominators of identifying expressions $\Psi_{om}$ and $\Psi_{ipw}$ from Proposition \ref{prop1} in the main text, i.e.
 \begin{equation*}
        E\left[\Pr[Y = 2 | V = 0, X] \dfrac{\Pr[Y = 1 | V = 1, X]}{\Pr[Y = 1 | V = 0, X]} \Big| V = 1 \right]
    \end{equation*}
    and 
    \begin{equation*}
        E\left[ \dfrac{(1 - V)}{\Pr(V=1)} \mathbbm 1(Y = 2) \dfrac{\pi(1,X)}{1 - \pi(1,X)}\right]
    \end{equation*}
where $\pi(y, X) = \Pr(V = 1 | Y^0 = y, X)$ and $\log \dfrac{\pi(Y, X)}{1 - \pi(Y, X)} = \eta(X) + \beta(Y, X)$.

We then show that equation \ref{eqn:udid3} retains its double-robustness property under our set up with
    \begin{align*}
        \xi(X) &= \dfrac{\exp\{\beta(1,X)\}}{E[\exp\{\beta(Y,X)\} | V = 0, X]}\mu_2(0,X) \\
            &= \exp\{\alpha(1,X)\}\mu_2(0,X)
    \end{align*}
    where $\alpha(y, X) = \log \dfrac{\Pr(Y^0=y|V=1,X)}{\Pr(Y^0=y|V=0,X)}$ and $\mu_y(v,X) = E[\mathbbm 1 (Y= y) | V = v, X]$. That is, Equation \ref{eqn:udid3} is consistent for $\phi$ if $\beta(Y,X)$ is correctly specified and either or both of the following hold:
    \begin{enumerate}
        \item $\mu_y^\dagger(0,X) = \mu_y(0,X)$ for $y \in \{1, 2\}$.
        \item $\eta^\dagger(X) = \eta(X)$.
    \end{enumerate}
    where  $\mu_y^\dagger(0,X)$ is a, possibly misspecified, estimator of $\mu_y(0,X)$ and likewise $\eta^\dagger(X)$ is a, possibly misspecified, estimator of $\eta(X)$.

\subsection{Proof of equivalence with UDiD}
 For equation \ref{eqn:udid1}, we have
\begin{align*}
     E&\left[\dfrac{E[\mathbbm 1(Y=2)\exp\{\beta(Y,X)\} | V = 0, X]}{E[\exp\{\beta(Y,X)\} | V = 0, X]}\bigg|V = 1\right] = \\
     &\quad = E \left[\dfrac{\int \mathbbm 1(Y=2)\exp\{\beta(y,X)\} f(y| V = 0, X)dy}{\int \exp\{\beta(y,X)\} f(y| V = 0, X)}\bigg|V = 1\right] \\
     &\quad = E \left[\dfrac{\Pr(Y = 2 | V = 0, X)\exp\{\beta(2,X)\} }{\int \exp\{\beta(y,X)\} f(y| V = 0, X)}\bigg|V = 1\right] \\
     &\quad = E \left[\dfrac{\Pr(Y = 2 | V = 0, X)\exp\{\beta(1,X)\} }{\int \exp\{\beta(y,X)\} f(y| V = 0, X)}\bigg|V = 1\right] \\
     &\quad = E \left[\dfrac{\Pr(Y = 2 | V = 0, X)\exp\{\beta(1,X)\} }{\Pr(Y^0 = 0| V = 0, X)/\Pr(Y^0 =0|V = 1, X)}\bigg|V = 1\right] \\
     &\quad = E \left[\Pr(Y = 2|V = 0, X)\dfrac{\Pr(Y = 1 | V = 1, X)}{\Pr(Y = 1 | V = 0, X)}\bigg|V = 1\right]
\end{align*}
For equation \ref{eqn:udid2}, we have
\begin{align*}
     &\dfrac{E[(1-V)\mathbbm 1(Y=2)\exp\{\eta(X) + \beta(Y,X)\}]}{E[(1-V)\exp\{\eta(X) +\beta(Y,X)\}]} = \\
     &\qquad = \dfrac{E\left[(1-V)\dfrac{\pi(2,X)}{1-\pi(2,X)}\mathbbm 1(Y=2)\right]}{E\left[(1-V)\dfrac{\pi(Y, X)}{1-\pi(Y,X)}\right]} \\
     &\qquad = \dfrac{E\left[(1-V)\dfrac{\pi(1,X)}{1-\pi(1,X)}\mathbbm 1(Y=2)\right]}{E\left[(1-V)\dfrac{\pi(Y, X)}{1-\pi(Y,X)}\right]} \\
     &\qquad = \dfrac{E\left[(1-V)\dfrac{\pi(1,X)}{1-\pi(1,X)}\mathbbm 1(Y=2)\right]}{E\left[E[(1-V) | Y, X]\dfrac{\pi(Y, X)}{1-\pi(Y,X)}\right]} \\
     &\qquad = \dfrac{E\left[(1-V)\dfrac{\pi(1,X)}{1-\pi(1,X)}\mathbbm 1(Y=2)\right]}{E\left[\pi(Y,X)\right]} \\
     &\qquad = E\left[\dfrac{(1-V)}{\Pr[V=1]}\dfrac{\pi(1,X)}{1-\pi(1,X)}\mathbbm 1(Y=2)\right]
\end{align*}
\subsection{Proof of double robustness}
When $\eta^\dagger(X) = \eta(X)$:
\begin{align*}
    E&\left[\dfrac{(1 - V)}{\Pr(V=1)} \exp\{\eta^\dagger(X) + \beta(Y,X)\}\{\mathbbm 1(Y = 2) - \xi^\dagger(X)\} + \dfrac{V}{\Pr(V=1)}\xi^\dagger(X) \right]\\
     &= E\left[\dfrac{(1 - V)}{\Pr(V=1)}  \dfrac{\pi(Y^0,X)}{1-\pi(Y^0,X)} \{\mathbbm 1(Y^0 = 2) - \xi^\dagger(X)\}  + \dfrac{V}{\Pr(V=1)}\xi^\dagger(X)\right] \\
      &= E\left[\dfrac{E[1-V|Y^0, X]}{\Pr(V=1)}  \dfrac{\pi(Y^0,X)}{1-\pi(Y^0,X)}\{\mathbbm 1(Y^0 = 2) - \xi^\dagger(X)\} + \dfrac{V}{\Pr(V=1)}\xi^\dagger(X)\right]\\
       &= E\left[\dfrac{E[V|Y^0, X]}{\Pr(V=1)} \{\mathbbm 1(Y^0 = 2) - \xi^\dagger(X)\} + \dfrac{V}{\Pr(V=1)}\xi^\dagger(X)\right] \\
       &= E\left[\dfrac{V}{\Pr(V=1)} \{\mathbbm 1(Y^0 = 2) - \xi^\dagger(X)\} + \dfrac{V}{\Pr(V=1)}\xi^\dagger(X)\right]\\
        &= E\left[\mathbbm 1(Y^0 = 2) |V=1\right]
\end{align*}
When $\mu_y^\dagger(0,X) = \mu_y(0,X) \implies \xi^\dagger(X) = \xi(X)$:
\begin{align*}
     E&\left[\dfrac{(1 - V)}{\Pr(V=1)} \exp\{\eta^\dagger(X) + \beta(Y,X)\}\{\mathbbm 1(Y = 2) - \xi^\dagger(X)\} + \dfrac{V}{\Pr(V=1)}\xi^\dagger(X) \right]\\
     &= E\left[\dfrac{(1 - V)}{\Pr(V=1)} \dfrac{\pi^\dagger(Y^0,X)}{1-\pi^\dagger(Y^0, X)}\{\mathbbm 1(Y^0 = 2) - \xi(X)\} + \dfrac{V}{\Pr(V=1)}\xi(X) \right] \\
      &= E\left[\dfrac{E[1-V|Y^0, X] }{\Pr(V=1)} \dfrac{\pi^\dagger(Y^0,X)}{1-\pi^\dagger(Y^0, X)}\{\mathbbm 1(Y^0 = 2) - \xi(X)\} + \dfrac{V}{\Pr(V=1)}\xi(X) \right] \\
      &= E\left[\dfrac{ E[V|Y^0, X]}{\Pr(V=1)}\underbrace{\dfrac{E[1-V|Y^0, X]}{E[V|Y^0, X]} \dfrac{\pi^\dagger(Y^0,X)}{1-\pi^\dagger(Y^0, X)}}_{=\exp\{(\eta^\dagger(X) - \eta(X)\}}\{\mathbbm 1(Y^0 = 2) - \xi(X)\} + \dfrac{V}{\Pr(V=1)}\xi(X) \right] \\
      &= E\left[\dfrac{ E[V|Y^0, X]}{\Pr(V=1)}\exp\{(\eta^\dagger(X) - \eta(X)\}\{\mathbbm 1(Y^0 = 2) - \xi(X)\} + \dfrac{V}{\Pr(V=1)}\xi(X) \right] \\
      &= E\left[\dfrac{V}{\Pr(V=1)}\exp\{(\eta^\dagger(X) - \eta(X)\}\{\mathbbm 1(Y^0 = 2) - \xi(X)\} + \dfrac{V}{\Pr(V=1)}\xi(X) \right] \\
      &= E\left[E\left[\exp\{(\eta^\dagger(X) - \eta(X)\}\{\mathbbm 1(Y^0 = 2) - \xi(X)\} | V = 1\right] + E\left[\xi(X)\} |V=1\right]\right] \\
      &= E\left[E\left[\exp\{(\eta^\dagger(X) - \eta(X)\}\{\underbrace{ E[\mathbbm1(Y^0 = 2)|V=1,X] - \xi(X)}_{=0}\} | V = 1\right] + E\left[\xi(X) |V=1\right]\right] \\
        &= E\left[\mathbbm1(Y^0 = 2)|V=1\right]
\end{align*}
\newpage
\subsection{Estimation}
\textbf{Steps:}
\begin{enumerate}
    \item Fit $\mu_y^\dagger(0,X)$ e.g., via multinomial regression of $Y$ on $(1,X)'$ among those with $V= 0$.
    \item Fit $\pi^\dagger(1,X)$ e.g., via logistic regression of $V$ on $(1,X)'$ among those with $Y=1$. Note that 
    \[\log \dfrac{\pi(1,X)}{1-\pi(1,X)} = \eta(X) + \beta(1, X),\]
    but as of yet we cannot differentiate between them.
    \item Estimate $\eta^\dagger(X)$ by solving 
    \[E\left[\dfrac{(1-V)}{1 -\pi(Y,X)}\right] = 1 \iff E[(1-V)\{1 - \pi^\dagger(Y,X)\} - 1] = 0\]
    where 
    \[\log \dfrac{\pi^\dagger(Y,X)}{1 - \pi^\dagger(Y,X)} = \begin{cases} \eta(X) & \text{if } Y = 0 \\
    \log \dfrac{\pi^\dagger(1,X)}{1-\pi^\dagger(1,X)}  & \text{if } Y \in \{1,2\}
    \end{cases}\]
    \item Estimate doubly-robust $\beta_{DR}^\dagger(Y,X)$ via 
    \[E\left[\{V - \operatorname{expit}(\eta^\dagger(X))\}\exp\{-\beta_{DR}(Y,X)V\}\{S(Y,X) - \mu_Y^\dagger(0, X)S(Y,X)\}\right] = 0\]
    where $S(Y,X) = \mathbbm 1 (Y = 1)$.
    \item Re-estimate $\eta_{DR}^\dagger(X)$ by solving 
    \[E\left[\dfrac{(1-V)}{1 -\pi(Y,X)}\right] = 1 \iff E[(1-V)\{1 + \exp\{\eta_{DR}^\dagger(X) + \beta^\dagger_{DR}(Y,X)\}\} - 1] = 0\]
    \item Estimate effect via
    \[\dfrac{E[V \mathbbm 1(Y =2)]}{ E\left[(1 - V) \exp\{\eta_{DR}^\dagger(X) + \beta^\dagger_{DR}(Y,X)\}\{\mathbbm 1(Y = 2) - \xi^\dagger(X)\} + V\xi^\dagger(X)\}  \right]}\]
    where
    \[\xi^\dagger(X)= \dfrac{\exp\{\beta^\dagger_{DR}(Y,X)\}}{E[\exp\{\beta^\dagger_{DR}(Y,X)\}|V=0,X ]}\mu_2^\dagger(0,X).\]
\end{enumerate}
\newpage
\section{Doubly robust estimator under TND sampling}\label{sec:eif}
Under TND sampling, i.e. $O_{TND} = \{(X_i, V_i, S_i=1, Y_i) : i = 1, \ldots, n\},$ with selection $S = \mathbbm 1(Y\neq 0)$, 
we show in the main text that equations \ref{eqn:om_estimand} and \ref{eqn:ipw_estimand}, and by extension equations \ref{eqn:udid1} and \ref{eqn:udid2}, are still identifiable by
    \begin{equation*}
        \Psi_{om}^* = \dfrac{E[V \mathbbm 1 (Y = 2)|S =1]}{E\left[V\mathbbm 1 (Y = 1) \dfrac{\Pr[Y = 2 | S = 1, V = 0, X]}{\Pr[Y = 1| S = 1, V = 0, X]}\Big| S = 1 \right]}
    \end{equation*}
    and 
     \begin{equation*}
        \Psi_{ipw}^* = \dfrac{E[V 1 (Y = 2)|S =1]}{E\left[ (1 - V) 1 (Y = 2) \dfrac{\Pr[V=1 | S=1, Y=1, X]}{\Pr[V=0 | S=1, Y=1, X]} \bigg| S = 1\right]}.
    \end{equation*}
    These equations could alternatively be parameterized as 
    \begin{equation}\label{eqn:om_estimand_tnd_alt}
        \Psi_{om}^* = \dfrac{E[V Y^*|S =1]}{E\left[V(1-Y^*) \dfrac{\mu^*(0,X)}{1 - \mu^*(0,X)}\Big| S = 1 \right]}
    \end{equation}
    and 
    \begin{equation}\label{eqn:ipw_estimand_tnd_alt}
        \Psi_{ipw}^* = \dfrac{E[V Y^*)|S =1]}{E\left[ (1 - V) Y^* \dfrac{\pi^*(0,X)}{1 - \pi^*(0,X)} \bigg| S = 1\right]}
    \end{equation}
    where $Y^* = \mathbbm 1(Y= 2)$ and $\mu^*(0,X) = \Pr(Y^*=1|S=1,V=0,X)$ and $\pi^*(0,X) = \Pr(V=1 | S=1, Y^*=0, X)$. 
    
    As shown below, both $\Psi_{om}^*$ and $\Psi_{ipw}^*$ are equivalent to
    \[\Psi^*_{dr} = \dfrac{E[V Y^*|S =1]}{E\left[VY^*\exp\{-\phi^*(X)\}\Big| S = 1 \right]}\]
    where 
    \[\phi^*(X) = \log \dfrac{\Pr[Y^*=1|S=1, V=1,X]\Pr[Y^*=0|S=1, V=1,X]}{\Pr[Y^*=0|S=1, V=0,X]\Pr[Y^*=1|S=1, V=0,X]}\]
    is the log of the conditional odds ratio function. Furthermore, as described in \textcite{tchetgen_tchetgen_doubly_2010}, a doubly-robust estimator of $\phi^*(X)$ may be obtained as the solution to the empirical analogue of the population moment equation
    \[E[(1,X)'(V-\pi^*(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^*(0,X)) | S = 1] = 0.\]
    We show this double-robustness property for $\phi^*(X)$, and by extension $\Psi^*_{dr}$, when either or both of the following conditions hold:
    \begin{enumerate}
        \item $\mu^\dagger(0,X) = \mu^*(0,X)$.
        \item $ \pi^\dagger(0,X) = \pi^*(0,X)$.
    \end{enumerate}
    where  $\mu^\dagger(0,X)$ is a, possibly misspecified, estimator of $\mu^*(0,X)$ and likewise $\pi^\dagger(0,X)$ is a, possibly misspecified, estimator of $\pi^*(0,X)$.
    \subsection{Proof of double robustness}
     Note that 
    \begin{align*}
        f(V = v | Y^*, X) &= \dfrac{\Pr(V = v | S = 1, Y^* = 0, X)\exp\{\phi^*(X)vY^*\}}{\sum_{v'}\Pr(V = v' | S = 1, Y^* = 0, X)\exp\{\phi^*(X)v'Y^*\}} \\
        &= c_v\Pr(V = v | S = 1, Y^* = 0, X)\exp\{\phi^*(X)vY^*\}
    \end{align*}
    where $c_v = 1 / \sum_{v'}\Pr(V = v' | S = 1, Y^* = 0, X)\exp\{\phi^*(X)v'Y^*\}$ is a constant. 
    \vspace{1em} \\
    \noindent Now consider $ \pi^\dagger(0,X) = \pi^*(0,X)$:
    \begin{align*}
        &E[W(X)(V-\pi^\dagger(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^\dagger(0,X)) | S = 1]  \\
        &\quad = E[W(X)(V-\pi^*(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^\dagger(0,X)) | S = 1] \\
        &\quad = E[E[W(X)(V-\pi^*(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^\dagger(0,X)) | S=1, Y^*, X] | S = 1]\\
        &\quad = E[W(X)(Y^* - \mu^\dagger(0,X))E[(V-\pi^*(0,X))\exp\{-\phi^*(X)VY^*\} | S=1, Y^*, X] | S = 1] \\
        &\quad = E[W(X)(Y^* - \mu^\dagger(0,X))\sum_v[(v-\pi^*(0,X))\exp\{-\phi^*(X)vY^*\} \Pr(V = v | S=1, Y^*, X)] | S = 1] \\
        &\quad = E[W(X)(Y^* - \mu^\dagger(0,X))c_v\sum_v\left[(v-\pi^*)\Pr(V = v | S = 1, Y^* = 0, X)\right] | S = 1] \\
        &\quad = E[W(X)(Y^* - \mu^\dagger(0,X))c_vE\left[V-\pi^*(0,X)| S = 1, Y^* = 0, X\right] | S = 1] \\
         &\quad = E[W(X)(Y^* - \mu^\dagger(0,X))c_v\underbrace{\{E\left[V| S = 1, Y^* = 0, X\right] -\pi^*(0,X)\}}_{=0}| S = 1] \\
        &\quad=0
    \end{align*}
     \vspace{1em} \\
    Similarly, note that 
    \begin{align*}
        f(Y^* = y | V, X) &= \dfrac{\Pr(Y^*=y | S = 1, V = 0, X)\exp\{\phi^*(X)Vy\}}{\sum_{y'}\Pr(Y^* = y' | S = 1, V = 0, X)\exp\{\phi^*(X)v'Y^*\}} \\
        &= c_y\Pr(Y^* = y | S = 1, V = 0, X)\exp\{\phi^*(X)Vy\}
    \end{align*}
    where $c_y = 1 / \Pr(Y^* = y' | S = 1, V = 0, X)\exp\{\phi^*(X)v'Y^*\}$ is a constant.
    \vspace{1em} \\
    Now consider $ \mu^\dagger(0,X) = \mu^*(0,X)$:
    \begin{align*}
        &E[W(X)(V-\pi^\dagger(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^\dagger(0,X)) | S = 1]  \\
        &\quad = E[W(X)(V-\pi^\dagger(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^*(0,X)) | S = 1] \\
        &\quad = E[E[W(X)(V-\pi^\dagger(0,X))\exp\{-\phi^*(X)VY^*\}(Y^* - \mu^*(0,X)) | S=1, V, X] | S = 1]\\
        &\quad = E[W(X)(V-\pi^\dagger(0,X))E[(Y^* - \mu^*(0,X))\exp\{-\phi^*(X)VY^*\} | S=1, V, X] | S = 1] \\
        &\quad = E[W(X)(V-\pi^\dagger(0,X))\sum_y[(y - \mu^*(0,X))\exp\{-\phi^*(X)Vy\} \Pr(Y^* = y | S=1, V, X)] | S = 1] \\
        &\quad = E[W(X)(V-\pi^\dagger(0,X))c_y\sum_y\left[(y - \mu^*(0,X))\Pr(Y^* = y | S=1, V=0, X)\right] | S = 1] \\
        &\quad = E[W(X)(V-\pi^\dagger(0,X))c_yE\left[Y^* - \mu^*(0,X)| S = 1, Y^* = 0, X\right] | S = 1] \\
         &\quad = E[W(X)(V-\pi^\dagger(0,X))c_y\underbrace{\{E\left[Y^*| S = 1, V = 0, X\right] -\mu^*(0,X)\}}_{=0}| S = 1] \\
        &\quad=0
    \end{align*}
    \newpage
    \subsection{Estimation}
\textbf{Steps:}
\begin{enumerate}
    \item Fit $\widehat{\mu}^*(0,X)$ e.g., via logistic regression of $Y^*$ on $(1,X)'$ among those with $V= 0$ using $O_{TND}$.
    \item Fit $\widehat{\pi}^*(0,X)$ e.g., via logistic regression of $V$ on $(1,X)'$ among those with $Y^* = 0$ using $O_{TND}$.
    \item  Estimate doubly-robust $\widehat{\phi}_{dr}^*(X)$ as solution to 
    \[\sum_{i=1}^n (1,X_i)'(V_i-\widehat{\pi}^*(0,X_i))\exp\{-\phi_{dr}^*(X_i)V_iY_i^*\}(Y^* - \widehat{\mu}^*(0,X_i)) = 0.\]
    \item Estimate the effect via
\begin{equation*}
    \widehat{\Psi}_{dr}^* = \dfrac{\sum_{i=1}^n V_i Y^*_i}{\sum_{i=1}^nV_iY^*_i \exp\{\widehat{\phi}_{dr}^*(X)\}}.
\end{equation*}
\end{enumerate}

\newpage

    \section{Direct effects of vaccination and identification of alternative estimands}\label{sec:de_testing}
    In the main text, we discuss the identification of the causal effect of vaccination on \textit{medically-attended illness} among the vaccinated, defined by the causal risk ratio:
    \[\Psi_{RRV} \equiv \dfrac{\Pr(Y^1 = 2| V = 1)}{\Pr(Y^0 = 2 | V = 1)} = \dfrac{\Pr(I^1 = 2, T^1 =1 | V = 1)}{\Pr(I^0 = 2, T^0 =1 | V = 1)}.\]
    This effect is assumed to be of substantive interest to investigators, as it is often the primary outcome in phase III vaccine trials. However, this outcome encompasses multiple component events downstream of exposure and infection, each of which may be influenced by vaccination, resulting in a non-null total effect on medically-attended illness. For example, consider a non-null effect, i.e. $\Psi_{RRV} \neq 1$ could occur if any one of the following scenarios were true:
    \begin{itemize}
        \item[(i)] The vaccine has an effect on exposure.
        \item[(ii)] The vaccine has no effect on exposure, but has an effect on infection.
        \item[(iii)] The vaccine has no effect on exposure or infection, but has an effect on the development of symptoms.
        \item[(iv)] The vaccine has no effect on exposure, infection, or symptoms, but has effect on another intermediate outcome, such as severity.
        \item[(v)] The vaccine has no effect on exposure, infection, symptoms, or severity, but has an effect on testing behavior when sick.
    \end{itemize}
    Depending on the research question, the effect on some of these components may not be clinically relevant. For instance, consider the case where the only effect of vaccination is to reduce care seeking among the vaccinated, i.e., a purely behavioral effect. This would lead to a real reduction in $\Psi_{RRV}$ even though there is no ``biological'' action of the vaccine.
    
    In this section, we explore the identification of alternative estimands that focus on specific components of the process and exclude others that are not relevant. In particular, we consider certain ``biological'' effects that exclude components where changes may stem from altered human behavior due to lack of blinding. 

    \subsection{Symptomatic infection}
    Consider first the effect of vaccination on symptomatic infection, i.e., removing the influence of any ``non-biological'' effects of unblinded vaccination on test or care-seeking behavior. Below we show that, if additional  conditions beyond Assumptions \ref{app_ass1} - \ref{app_ass4} hold, alternative estimands for the effect of vaccination on symptomatic infection are identifiable under the test-negative design. These conditions include:
    \begin{enumerate}[label=\upshape(A5\alph*), ref=A5\alph*]
        \item\label{app_ass5a}\textit{No effect of vaccination on testing for any individual, except through infection.} That is, $T^{v, i} = T^i$ for all $v$ in $\{0, 1\}$ and for all $i$ in $\{1, 2\}$. A consequence of this is 
        \[\Pr(T^1=1|I^1=1, V=1, X) = \Pr(T^0=1|I^0=1, V=1, X),\]
        and 
        \[\Pr(T^1=1|I^1=2, V=1, X) = \Pr(T^0=1|I^0=2, V=1, X).\]
         \item\label{app_ass5b}\textit{Equal effects of vaccination on testing for test-positive and test-negative illness.} That is, 
         \[\dfrac{\Pr(T^1 = 1 | I^1 = 1, V = 1)}{\Pr(T^0 = 1 | I^0 = 1, V = 1)} = \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1)}{\Pr(T^0 = 1 | I^0 = 2, V = 1)}\]
    \end{enumerate}
    \begin{enumerate}[label=\upshape(A\arabic*), ref=A\arabic*, start=6]
         \item\label{app_ass6}\textit{No effect modification for symptomatic infection among the tested on the ratio scale after conditioning on $X$} That is, 
          \[\dfrac{\Pr(I^1 = 2 | T = 1, V = 1, X)}{\Pr(I^0 = 2 | T = 1, V = 1, X)} = \dfrac{\Pr(I^1 = 2 | V = 1, X)}{\Pr(I^0 = 2 | V = 1, X)}\]
    \end{enumerate}
    The first and second conditions, \ref{app_ass5a} and \ref{app_ass5b}, are alternatives that place limits on the nature of effects on testing behavior. Assumption \ref{app_ass5a} requires no effect of vaccination on testing behavior regardless of infection source. It might be plausible if, as in phase III efficacy studies, participants were blind to whether they received vaccination or a placebo. However, in ``real world'' settings typical of test-negative studies, \ref{app_ass5b} may be more realistic. It only requires the effects be equal regardless of whether the source of symptomatic infection is the target pathogen or alternative test-negative pathogens. This may be plausible when participants do not have access to home testing and when they are unaware of, e.g. the testing status of others in their household or contact networks who may have recently been infected. Finally, Assumption \ref{app_ass6} requires no effect modification on the relative scale between the tested sample and the target population, that is either the covariates in $X$ include all possible modifiers or the distribution of unmeasured effect modifiers is equal between the tested sample and the target population. 

    As we show in Proposition \ref{prop3} under \ref{app_ass1} - \ref{app_ass4} and either of \ref{app_ass5a} or \ref{app_ass5b} the conditional risk ratio for symptomatic infection among the vaccinated is identified by the conditional odds ratio under test-negative sampling. Likewise, in Proposition \ref{prop4}, we show that the risk ratio for symptomatic infection among the vaccinated and tested is identified by $\Psi^*_{om}$ and $\Psi^*_{ipw}$, i.e., the novel TND estimands introduced in the main text.
    \newpage
    
    \begin{proposition}\label{prop3}
    The conditional risk ratio for symptomatic infection among the vaccinated, 
    \[\Theta_{RRV}(X) \equiv \dfrac{\Pr(I^1 = 2 | V = 1, X)}{\Pr(I^0 = 2| V = 1, X)},\]
    is identified by the conditional odds ratio, 
    \begin{equation}
    \Psi^*_{om}(X) = \dfrac{\Pr(Y = 2 | S = 1, V = 1, X)/\Pr(Y = 1 | S = 1, V = 1, X)}{\Pr(Y = 2 | S = 1, V = 0, X)/\Pr(Y = 1 | S = 1, V = 0, X)},
    \end{equation}  
    under test-negative sampling provided that \ref{app_ass1} - \ref{app_ass4} hold and either of \ref{app_ass5a} or \ref{app_ass5b} hold.
    
    \end{proposition}
    \begin{proof}
        Under Assumption \ref{app_ass3}, we have that 
        \[\dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)} = \dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 0, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 0, X)}.\]
        Likewise, Assumptions \ref{app_ass5a} and \ref{app_ass5b} both imply that 
        \[\dfrac{\Pr(T^0 = 1 | I^0 = 2, V = 1, X)}{\Pr(T^0 = 1 | I^0 = 1, V = 1, X)} = \dfrac{\Pr(T^1 = 1 | I^1 = 2, V = 1, X)}{\Pr(T^1 = 1 | I^1 = 1, V = 1, X)}.\]
        Substituting and applying consistency, this implies the following restriction on the observed data
        \[\dfrac{\Pr(T = 1 | I = 2, V = 1, X)}{\Pr(T = 1 | I = 2, V = 1, X)}= \dfrac{\Pr(T = 1 | I = 2, V = 0, X)}{\Pr(T = 1 | I = 1, V = 0, X)}\]        
        Now, we can show that
        \begin{align*}
            \Theta&_{RRV}(X) = \\
            &= \dfrac{\Pr(I^1 = 2 | V = 1, X)}{\Pr(I^0 = 2| V = 1, X)} \\
            &= \dfrac{\Pr(I^1 = 2 | V = 1, X)}{\dfrac{\Pr(I^0 = 1| V = 1, X)}{\Pr(I^0 = 1| V = 0, X)}\Pr(I^0 = 2| V = 0, X)} \\
            &= \dfrac{\Pr(I^1 = 2 | V = 1, X)}{\dfrac{\Pr(I^1 = 1| V = 1, X)}{\Pr(I^0 = 1| V = 0, X)}\Pr(I^0 = 2| V = 0, X)} \\
            &= \dfrac{\Pr(I = 2 | V = 1, X)\Pr(I = 1| V = 0, X)}{\Pr(I = 1| V = 1, X)\Pr(I = 2| V = 0, X)} \\
            &= \dfrac{\Pr(I = 2 | V = 1, X)\Pr(I = 1| V = 0, X)}{\Pr(I = 1| V = 1, X)\Pr(I = 2| V = 0, X)} \times \dfrac{\Pr(T = 1 | I = 2, V = 1)\Pr(T = 1 | I = 1, V = 0)}{\Pr(T = 1 | I = 1, V = 1)\Pr(T = 1 | I = 2, V = 0)} \\
            &= \dfrac{\Pr(I = 2 | T=1, V = 1, X)\Pr(I = 1| T=1, V = 0, X)}{\Pr(I = 1| T=1, V = 1, X)\Pr(I = 2| T = 1, V = 0, X)}  \\
             &= \dfrac{\Pr(Y = 2 | S=1, V = 1, X)\Pr(Y = 1| S=1, V = 0, X)}{\Pr(Y = 1| S=1, V = 1, X)\Pr(Y = 2| S = 1, V = 0, X)}  
        \end{align*}
        where the first line follows by definition, the second applies Assumption \ref{ass3a}, the third applies Asssumption \ref{app_ass2}, the fourth applies consistency, the fifth applies the result above, the sixth uses Bayes' rule, and the last applies the definitions of $Y$ and $S$.
    \end{proof}

    
    \newpage
    \begin{proposition}\label{prop4}
    The risk ratio for symptomatic infection among the vaccinated and tested, 
    \[\Theta_{RRVT} \equiv \dfrac{\Pr(I^1 = 2 | T = 1, V = 1)}{\Pr(I^0 = 2| T = 1, V = 1)},\]
    is identified under test-negative sampling by
   \begin{equation*}
        \Psi_{om}^* = \dfrac{\Pr(Y = 2 | S = 1, V = 1)}{E\left\{ \Pr(Y = 2 | S = 1, V = 0, X) \dfrac{\Pr(Y = 1 | S = 1, V = 1, X)}{\Pr(Y = 1| S = 1, V = 0, X)}\Big| S = 1, V = 1 \right\}}
    \end{equation*}
    and 
    \begin{equation*}
        \Psi_{ipw}^* = \dfrac{E\left\{V \mathbbm 1 (Y = 2)|S =1\right\}}{\E\left\{ (1 - V) \mathbbm 1 (Y = 2) \dfrac{\pi^*_1(X)}{1 - \pi^*_1(X)} \bigg| S = 1\right\}}
    \end{equation*}
    where $\pi^*_1(X) = \Pr(V = 1| S = 1, Y = 1, X)$, provided that assumptions \ref{app_ass1} - \ref{app_ass4} and \ref{app_ass6} hold and either of \ref{app_ass5a} or \ref{app_ass5b} hold.
    \end{proposition}
    \begin{proof}
        Applying same arguments in Proposition \ref{prop3}, we have that 
        \[\dfrac{\Pr(T = 1 | I = 2, V = 1, X)}{\Pr(T = 1 | I = 2, V = 1, X)}= \dfrac{\Pr(T = 1 | I = 2, V = 0, X)}{\Pr(T = 1 | I = 1, V = 0, X)}\]        
        Now, we can show that
        \begin{align*}
            \Theta&_{RRVT} \\
            &= \dfrac{\Pr(I^1 = 2 | T = 1, V = 1)}{\Pr(I^0 = 2| T = 1, V = 1)} \\
            &= \dfrac{\Pr(I^1 = 2 | T = 1, V = 1)}{E\left\{\Pr(I^0 = 2| T = 1, V = 1, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(I^1 = 2 | T = 1, V = 1)}{E\left\{\dfrac{\Pr(I^0 = 2 | V = 1, X)}{\Pr(I^1 = 2 | V = 1, X)}\Pr(I^1 = 2| T = 1, V = 1, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(I^1 = 2 | T = 1, V = 1)}{E\left\{\dfrac{\Pr(I^0 = 1| V = 1, X)}{\Pr(I^0 = 1| V = 0, X)}\dfrac{\Pr(I^0 = 2 | V = 0, X)}{\Pr(I^1 = 2 | V = 1, X)}\Pr(I^1 = 2| T = 1, V = 1, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(I^1 = 2 | T = 1, V = 1)}{E\left\{\dfrac{\Pr(I^1 = 1| V = 1, X)}{\Pr(I^0 = 1| V = 0, X)}\dfrac{\Pr(I^0 = 2 | V = 0, X)}{\Pr(I^1 = 2 | V = 1, X)}\Pr(I^1 = 2| T = 1, V = 1, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{E\left\{\dfrac{\Pr(I = 1| V = 1, X)}{\Pr(I = 1| V = 0, X)}\dfrac{\Pr(I = 2 | V = 0, X)}{\Pr(I = 2 | V = 1, X)}\Pr(I = 2| T = 1, V = 1, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{E\left\{\dfrac{\Pr(I = 1|T=1, V = 1, X)}{\Pr(I = 1|T = 1, V = 0, X)}\dfrac{\Pr(I = 2 | T = 1, V = 0, X)}{\Pr(I = 2 | T = 1, V = 1, X)}\Pr(I = 2| T = 1, V = 1, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(I = 2 | T = 1, V = 1)}{E\left\{\dfrac{\Pr(I = 1|T=1, V = 1, X)}{\Pr(I = 1|T = 1, V = 0, X)}\Pr(I = 2 | T = 1, V = 0, X)\Big| T = 1, V = 1\right\}} \\
            &= \dfrac{\Pr(Y = 2 | S = 1, V = 1)}{E\left\{ \Pr(Y = 2 | S = 1, V = 0, X) \dfrac{\Pr(Y = 1 | S = 1, V = 1, X)}{\Pr(Y = 1| S = 1, V = 0, X)}\Big| S = 1, V = 1 \right\}}
        \end{align*}
        where the first line follows by definition, the second applies Assumption \ref{ass3a}, the third applies Asssumption \ref{app_ass2}, the fourth applies consistency, the fifth applies the result above, the sixth uses Bayes' rule, and the last applies the definitions of $Y$ and $S$.
        
    \end{proof}
    \newpage

   %  \subsection{Effect among exposed}

    
   
    
   %  We assume that exposure is necessary for infection and infection is necessary for the development of symptoms or severe disease which can be formalized as:
   %  \begin{itemize}
   %      \item [(A5)] \textbf{Exposure necessity for infection}. That is, $E = 0 \implies I = 0$ and $I^{e=0} = I$ if $E = 0$.
   %      \item [(A6)] \textbf{Infection necessity for symptoms and severe sequelae}. That is, \\$I = 0 \implies (W = 0, Z = 0)$ and $W^{i=0} = W$ if $W = 0$ and $Z^i=Z$ if $Z=0$.
   %  \end{itemize}
   %  We also assume throughout
   %  \begin{itemize}
   %      \item [(A7)] \textbf{No effect of vaccination on exposure}. That is, $E^{v} = E$ for all $v$ in $\{0, 1\}$.
   %      \item [(A7*)] \textbf{Equal effects of vaccination on exposure}. That is, 
   %      \[\dfrac{\Pr[E^1=2 | V=1]}{\Pr[E^0=2 | V=1]} = \dfrac{\Pr[E^1=1 | V=1]}{\Pr[E^0=1 | V=1]}\]
   %  \end{itemize}

   %  \subsection{Effect on severe sequelae}

   %  \[\Phi_{RRV} \equiv \dfrac{\Pr[I^1 = 2 | V = 1]}{\Pr[I^0 = 2 | V = 1]}.\]

   % $k$ indexes the source (1: test-negative pathogen, 2: test-positive pathogen):

   %  We can then re-create the DAG in Figure \ref{fig:dag_split} 

    
    
    \newpage
    
\clearpage


% \subsection{What if test-positive and test-negative infections are not mutually exclusive?}
% Define $I_1$ as an indicator of a symptomatic test-negative infection and $I_2$ as an indicator of a symptomatic test-positive infection. We allow for the possibility that $\Pr(I_1 = 1, I_2 = 1) > 0$, but still assume that the symptom screen is effective such that, for any individual $i$, $\mathbbm 1(I_{i,1} + I_{i,2}) = 1$ when $S_i = 1$. We use the following modified assumption set:
% \begin{itemize}
%     \item[(A1$^\ddagger$)] Consistency of potential outcomes. For all individuals $i$ and for $v \in \{0, 1\}$, we have $I_{2i}^v = I_{2i}$, $I_{1i}^v = I_{1i}$, and $T_i^v = T_i$ when $V_i = v$.
%     \item[(A2$^\ddagger$)] No effect of vaccination on test-negative symptomatic illness  among the vaccinated. That is, $\Pr(I_1^0 = 1 | V = 1, X) = \Pr(I_1^1 = 1 | V = 1, X).$
%     \item[(A3$^\ddagger$)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for test-positive and test-negative illnesses, i.e. 
%         $$\beta_2(X) = \beta_1(X), $$
%         $$ \text{where } \beta_i(X) = \log \frac{\Pr(I^0_i = 1, T^0 = 1 | V = 1, X)\Pr(I^0_i = 0, T^0 = 1 | V = 0, X)}{\Pr(I^0_i = 0, T^0 = 1 | V = 1, X)\Pr(I^0_i = 1, T^0 = 1| V = 0, X)}.$$
%     \item[(A4$^\ddagger$)] Overlap of vaccination among test-positives and test-negatives. Define $\mathcal{S}_i(v)$ as the support of the law of $(I^v_i, T^v = 1, V = v, X)$, then for $v$ in $\{0,1\}$, then it must be that $\mathcal{S}_2(1) \subseteq \mathcal{S}_2(0)$ and $\mathcal{S}_2(v) \subseteq \mathcal{S}_1(v).$
%     \item[(A5$^\ddagger$)] No direct effect of vaccination on test-seeking behavior among the vaccinated. That is, for $i$ in $\{1,2\}$, $\Pr[T^1 = 1 | I^1_i = 1, V = 1, X] = \Pr[T^0 = 1 | I^0_i = 1, V = 1, X].$
% \end{itemize}

% Here, we show that, under assumptions A1$^\ddagger$-A5$^\ddagger$, the causal odds ratio among the vaccinated remains identified in the test-negative design.

% \begin{theorem}
%     If the test-negative and test-positive illnesses are not mutually exclusive, then, under assumptions A1$^\ddagger$-A5$^\ddagger$, the causal odds ratio among the vaccinated, 
%     \begin{equation*}
%         \Phi_{ORV} \equiv \dfrac{\Pr(I^1_2=1, T = 1|V=1)\Pr(I^1_2=0, T = 1|V=1)}{\Pr(I^0_2=0, T = 1|V=1)\Pr(I^0_2=1, T = 1|V=1)},
%     \end{equation*}
%     is identified by $\Psi^*_{om}$ and $\Psi^*_{ipw}$ (as defined in Lemma \ref{lemma2}) in the test-negative design.
%     \end{theorem}
    
    % \begin{proof}
    % \begin{align*}
    %     \Psi_{om}^* &= \dfrac{\Pr(Y^* = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(Y^* = 0 | S = 1, V = 1, X)}{\Pr(Y^* = 0 | S = 1, V = 0, X)} \Pr(Y^* = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} \\
    %     &= \dfrac{\Pr(I_2 = 1 | S = 1, V = 1)}{E\left\{ \dfrac{\Pr(I_1 = 1, I_2 = 0  | S = 1, V = 1, X)}{\Pr(I_1 = 1, I_2 = 0 | S = 1, V = 0, X)} \Pr(I_2 = 1 | S = 1, V = 0, X) \bigg| S = 1, V = 1\right\}} 
    % \end{align*}
    % \begin{align*}
    %     \Phi_{ORV} &= \dfrac{\Pr(I^1_2=1|V=1)\Pr(I^0_2=0|V=1)}{\Pr(I^1_2=0|V=1)\Pr(I^0_2=1|V=1)} \\
    %     &=\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)\Pr(I^0_2=0, T^0 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)\Pr(I^0_2=1, T^0 = 1|V=1)} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)}}{E\left\{\dfrac{\Pr(I^0_2=1, T^0 =1|V=1, X)}{\Pr(I^0_2=0, T^0 = 1|V=1, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1, T^1 = 1|V=1)}{\Pr(I^1_2=0, T^1 = 1|V=1)}}{E\left\{\dfrac{\dfrac{\Pr(I^0_1=1, T^0 = 1|V=1, X)}{\Pr(I^0_1=0, T^0 = 1|V=1, X)}}{\dfrac{\Pr(I^0_1=1, T^0 = 1|V=0, X)}{\Pr(I^0_1=0, T^0 = 1|V=0, X)}}\dfrac{\Pr(I^0_2=1, T^0 = 1|V=0, X)}{\Pr(I^0_2=0, T^0 = 1|V=0, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1| T^1 = 1, V=1)}{\Pr(I^1_2=0 | T^1 = 1, V=1)}}{E\left\{\dfrac{\dfrac{\Pr(I^0_1=1 | T^0 = 1, V=1, X)}{\Pr(I^0_1=0 | T^0 = 1, V=1, X)}}{\dfrac{\Pr(I^0_1=1 | T^0 = 1, V=0, X)}{\Pr(I^0_1=0 | T^0 = 1, V=0, X)}}\dfrac{\Pr(I^0_2=1 | T^0 = 1, V=0, X)}{\Pr(I^0_2=0 | T^0 = 1,   V=0, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I^1_2=1|V=1)}{\Pr(I^1_2=0|V=1)}}{E\left\{\dfrac{\Pr(I^1_1=1|V=1, X)\Pr(I^0_1=0|V=0, X)}{\Pr(I^1_1=0|V=1, X)\Pr(I^0_1=1|V=0, X)}\dfrac{\Pr(I^0_2=1|V=0, X)}{\Pr(I^0_2=0|V=0, X)}\bigg| V = 1\right\}} \\
    %     &=\dfrac{\dfrac{\Pr(I_2=1|V=1)}{\Pr(I_2=0|V=1)}}{E\left\{\dfrac{\Pr(I_1=1|V=1, X)\Pr(I_1=0|V=0, X)}{\Pr(I_1=0|V=1, X)\Pr(I_1=1|V=0, X)}\dfrac{\Pr(I_2=1|V=0, X)}{\Pr(I_2=0|V=0, X)}\bigg| V = 1\right\}}
    % \end{align*}
    % For the first expression, note that by Assumption A3 and exclusivity of test-negative and test-positive illnesses
    % \begin{align*}
    %     \dfrac{\Pr(I^0 = 2 | V = 1, X)}{\Pr(I^0 = 2 | V = 0, X)} &= \dfrac{\Pr(I^0 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
    %     &= \dfrac{\Pr(I^1 = 1 | V = 1, X)}{\Pr(I^0 = 1 | V = 0, X)} \\
    %     &= \dfrac{\Pr(I = 1 | V = 1, X)}{\Pr(I = 1 | V = 0, X)} \\
    % \end{align*}
    % where the first line is by Assumption A3 and exclusivity of test-negative and test-positive illnesses. The second line is by Assumption A2. And the last line applies consistency. This implies
    % \begin{equation*}
    %     \alpha_2^0(X) = \alpha_1(X)
    % \end{equation*}
    % and, therefore, combining with the derivation of Theorem 1, we have
    % \begin{align*}
    %     \Psi &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha^0_2(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}} \\
    %     &= \dfrac{\Pr(I = 2 | V = 1)}{E\left\{ \exp\{\alpha_1(X)\} \Pr(I = 2 | V = 0, X) \Big| V = 1 \right\}}.
    % \end{align*}
    
    % For the second expression, note first that, by the invariance of odds ratios, Assumptions A2 and A3 imply
    % \begin{equation*}
    %     \dfrac{\Pr(V = 1 | I^0 = 2, X)}{\Pr(V = 0 | I^0 = 2, X)} = \dfrac{\Pr(V = 1 | I = 1, X)}{\Pr(V = 0 | I = 1, X)}
    % \end{equation*}
    % and by consequence 
    % \begin{equation*}
    %     \dfrac{\pi^0_2(X)}{1 - \pi^0_2(X)} = \dfrac{\pi_1(X)}{1 - \pi_1(X)}.
    % \end{equation*}
    % Thus, again, continuing the derivation in Theorem 1, we have 
    % \begin{align*}
    %     \Psi &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi^0_2(X)}{1-\pi^0_2(X)}\right\}} \\
    %     &= \dfrac{E\{V \mathbbm 1 (I = 2)\}}{E\left\{(1 - V)\mathbbm 1 (I = 2) \dfrac{\pi_1(X)}{1-\pi_1(X)}\right\}}.
    % \end{align*}
    % \end{proof}
    \newpage

% \begin{align*}
%     \Pr[Y^* = 1 | S = 1, X, V] &= \Pr[I_2 = 1 | S = 1, X, V]\\
%     \Pr[Y^* = 0 | S = 1, X, V] &= \Pr[I_1 = 1 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% \begin{align*}
%     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[Y^* = 1 | S = 1, X, V] \\
%     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[Y^* = 0 | S = 1, X, V] \\
%     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[Y^* = 0 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
%     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[Y^* = 1 | S = 1, X, V]  - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% \begin{align*}
%     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[Y^* = 1 | S = 1, X, V] \\
%     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[Y^* = 0 | S = 1, X, V] \\
%     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[I_2 = 0 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
%     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[I_2      = 1 | S = 1, X, V]  - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% \end{align*}
% % \begin{align*}
% %     \Pr[I_1 = 1 | S = 1, X, V] &= \Pr[Y^* = 0 | S = 1, X, V] + \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] \\
% %     \Pr[I_1 = 0 | S = 1, X, V] &= \Pr[Y^* = 1 | S = 1, X, V] - \Pr[I_2 = 1, I_1 = 1 | S = 1, X, V] 
% % \end{align*}
% % and 
% % \begin{align*}
% %     \Pr[I_2 = 1 | S = 1, X, V] &= \Pr[Y^* = 1 | S = 1, X, V] \\
% %     \Pr[I_2 = 0 | S = 1, X, V] &= \Pr[Y^* = 0 | S = 1, X, V] 
% % \end{align*}
% In this case, Assumption (4) is 
% $$OR_2(X) = OR_1(X), $$
% $$ \text{where } OR_i(X) = \frac{\Pr[I^0_i = 1, T^0 = 1 | V = 1, X]\Pr[I^0_i = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_i = 0, T^0 = 1 | V = 1, X]\Pr[I^0_i = 1, T^0 = 1| V = 0, X]}.$$
% First, we show that, when $I_1$ and $I_2$ are not mutually exclusive the causal risk ratio is not identified. Following from our previous proof, we have that 
% \begin{align*}
%     \psi_{rrv}(X) &= \phi(X) \times \dfrac{\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 0, X]}\\
%     \Pr[I_2^0 = 2, T^0 = 1 | V = 1, X] &= \dfrac{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1 | V = 0, X]}{\Pr[I_1 = 0, T = 1 | V = 1, X] \Pr[I_1 = 1, T = 1 | V = 0, X]}\\
%     & \quad \quad \times  \dfrac{\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 0, X]} \Pr[I_2 = 1, T = 1 | V = 0, X]
% \end{align*}

% Here we show that, under these conditions, the conditional odds ratio in the TND identifies the conditional causal odds ratio, i.e.
% \begin{equation*}
%      \psi_{orv}(X) &\equiv \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]}. 
% \end{equation*}

% \begin{align*}
%         \psi_{orv}(X) &= \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I^0_1 = 0, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 1, T^0 = 1 | V = 0, X]}{\Pr[I^0_1 = 1, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 0, T^0 = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I_2 = 0, T = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I_2 = 1, T = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0, T = 1 | V = 1, X]\Pr[I_1 = 1, T = 1 | V = 0, X]}{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1 | I \neq 0, T = 1, V = 1, X]\Pr[I_2 =0 |  I \neq 0, T = 1, V = 0, X]}{\Pr[I_2 = 0 |  I \neq 0, T = 1, V = 1, X]\Pr[I_2 = 1 |  I \neq 0, T = 1, V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0 |  I \neq 0, T = 1, V = 1, X]\Pr[I_1 = 1 |  I \neq 0, T = 1, V = 0, X]}{\Pr[I_1 = 1 |  I \neq 0, T = 1, V = 1, X]\Pr[I_1 = 0 |  I \neq 0, T = 1, V = 0, X]}\\
%         &=  \dfrac{\Pr[I_2 = 1 | S = 1, V = 1, X]\Pr[I_2 =0 |  S = 1, V = 0, X]}{\Pr[I_2 = 0 |  S = 1, V = 1, X]\Pr[I_2 = 1 | S = 1, V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 0 |  S = 1, V = 1, X]\Pr[I_1 = 1 |  S = 1, V = 0, X]}{\Pr[I_1 = 1 |  S = 1, V = 1, X]\Pr[I_1 = 0 |  S = 1, V = 0, X]}\\
%         &=  \dfrac{\Pr[Y^* = 1 | S = 1, V = 1, X]}{\Pr[Y^* = 0|  S = 1, V = 1, X]} \dfrac{\Pr[Y^* = 0 |  S = 1, V = 0, X]}{\Pr[Y^* = 1 | S = 1, V = 0, X]}\\
% \end{align*}  
    
% Note that in this case, Assumption (4) becomes 
% \begin{itemize}
%      \item[(A4)] Odds ratio equi-confounding. Degree of unmeasured confounding bias on the odds ratio scale is the same for all symptomatic illness regardless if $I_1$ or $I_2$ is cause, i.e. 
%     $$OR_2(X) = OR_1(X), $$
%     $$ \text{where } OR_i(X) = \frac{\Pr[I^0_i = 1, T^0 = 1 | V = 1, X]\Pr[I^0_i = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_i = 0, T^0 = 1 | V = 1, X]\Pr[I^0_i = 1, T^0 = 1| V = 0, X]}.$$
% \end{itemize}
% Consider the conditional odds ratio for the effect of vaccination among the vaccinated, i.e.
%     \begin{align*}
%         \psi_{orv}(X) &\equiv \dfrac{\Pr[I^1_2 = 1, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I^1_2 = 0, T^1 = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 1, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 1, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I^0_2 = 1, T^0 = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I^0_2 = 0, T^0 = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I^0_1 = 1, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 0, T^0 = 1 | V = 0, X]}{\Pr[I^0_1 = 0, T^0 = 1 | V = 1, X]\Pr[I^0_1 = 1, T^0 = 1| V = 0, X]} \\
%         &=  \dfrac{\Pr[I_2 = 1, T = 1 | V = 1, X]\Pr[I_2 = 1, T = 1 | V = 0, X]}{\Pr[I_2 = 0, T = 1 | V = 1, X]\Pr[I_2 = 0, T = 1 | V = 0, X]} \\ & \quad \quad \times \dfrac{\Pr[I_1 = 1, T = 1 | V = 1, X]\Pr[I_1 = 0, T = 1 | V = 0, X]}{\Pr[I_1 = 0, T = 1 | V = 1, X]\Pr[I_1 = 1, T = 1| V = 0, X]}
%     \end{align*}        
% Under the consistency assumption (A1) the numerator is equal to $\Pr[I = 2, T = 1 | V = 1, X]$. Focusing on the denominator, under equi-confounding (A3)
% \begin{equation*}
% \Pr[I^0 = 2, T^0 = 1  | V = 1, X] = \frac{\Pr[I^0 = 1, T^0 = 1  | V = 1, X]}{\Pr[I^0 = 1, T^0 = 1  | V = 0, X]}\Pr[I^0 = 2, T^0 = 1 | V = 0, X]
% \end{equation*}
% and then by (A1) and (A2) with (A4) ensuring overlap
%     \begin{equation*}
%      \Pr[I^0 = 2, T^0 = 1  | V = 1, X] = \frac{\Pr[I = 1, T = 1  | V = 1, X]}{\Pr[I = 1, T = 1  | V = 0, X]}\Pr[I = 2, T = 1 | V = 0, X]
%     \end{equation*}
% Plugging back into the expression for $\psi_{rrv}(X)$, we find the following identifying expression 
%     \begin{equation*}
%          \phi(X) \equiv \dfrac{\dfrac{\Pr[I = 2, T = 1 | V = 1, X]}{\Pr[I = 1, T = 1 | V = 1, X]}}{\dfrac{\Pr[I = 2, T = 1 | V = 0, X]}{\Pr[I = 1, T = 1 | V = 0, X]}}
%     \end{equation*}
% which is the ratio of the odds of symptomatic infection with the vaccine pathogen versus symptomatic infection with another pathogen in the vaccinated and unvaccinated. It is also strictly written in terms of the observables. A key insight is that $\frac{\Pr[I = 1, T =1  | V = 0, X]}{\Pr[I = 1, T = 1 | V = 1, X]}$ acts as a proxy for $\frac{\Pr[I^0 = 2, T =1  | V = 0, X]}{\Pr[I^0 = 2, T = 1 | V = 1, X]}$ essentially a ``parallel trend'' for $I=2$ in absence of vaccination.

% Recall that, in a test-negative study, we only observe test results among the symptomatic and tested, i.e. samples $\{(X_i, V_i, S_i = 1, Y^*_i) : i = 1, \ldots, n\}$ where $S = \mathbb{I}(I \neq 0, T = 1)$. However, we can show that 
%     \begin{align*}
%          \phi(X) &= \dfrac{\dfrac{\Pr[Y^* = 1 | S = 1, V = 1, X]}{\Pr[Y^* = 0 | S = 1, V = 1, X]}}{\dfrac{\Pr[Y^* = 1 | S = 1, V = 0, X]}{\Pr[Y^* = 0 | S = 1, V = 0, X]}}
%     \end{align*}    
% which is the odds ratio comparing odds of testing positive for vaccinated and unvaccinated among the tested only.

\newpage 

\section{Identification when test is imperfect}\label{sec:testing}
In the main text, we assumed the availability of a perfect test such that $\widetilde{Y}_i = Y_i$ for all individuals where $\widetilde{Y}$ is the test result and $Y$ is the true value. In the absence of such a test, here we describe the potential for bias due to misclassification of case status which has also been described previously \cite{sullivan_theoretical_2016}. Because most studies employ real-time reverse-transcription polymerase chain reaction (RT-PCR), false positives are unlikely as most RT-PCR tests have specificity approaching 100\%. Therefore, misclassification of test-positives is probably rare, perhaps due to sample contamination or data entry errors. False negatives may be more likely as test sensitivity is generally lower due to the fact that (1) some of those infected with test-positive illness may not shed detectable virus or viral RNA; (2) some may seek care after shedding has ceased; or (3) sample quality may be compromised due to swab quality or inadequate storage. If sample collection is sufficient such that the source of the test-negative infection can be identified via RT-PCR, then sensitivity with respect to the test-positive illness can be improved by limiting to test-negative controls with an identified cause. 

In the ideal case that we have a test with perfect specificity and there is no effect of vaccination on the measurement error, i.e. when
 \begin{enumerate}[label=\upshape(G\arabic*), ref=G\arabic*]
        \item\label{app_assg1}\textit{No false-positives (perfect specificity).} This implies that for the true and tested values, $Y$ and $\widetilde{Y}$, and for every individual $i$, we have
        \[Y_i \neq 2 \implies \widetilde{Y}_i \neq 2.\]
        \item\label{app_assg2}\textit{No effect of vaccination on test result} That is, for any $v$ and $v'$, we have
         \[\Pr(\widetilde{Y}^{1} = 2 | V = 1, Y^1=2) = \Pr(\widetilde{Y}^{0} = 2 | V = 1, Y^{0}=2).\]
    \end{enumerate}
we can actually show that the causal risk ratio using the mismeasured outcome $Y^*$ is the same as the one we would obtain with the true outcome $Y$. 
\begin{proof}
    \begin{align*}
         \dfrac{\Pr(\widetilde{Y}^{1} = 2 | V = 1)}{\Pr(\widetilde{Y}^0 = 2 | V = 1)} &= \dfrac{\sum_y \Pr(\widetilde{Y}^1 = 2, Y^1 = y | V = 1)}{\sum_y \Pr(\widetilde{Y}^0 = 2, Y^0 = y| V = 1)} \\
        &= \dfrac{\Pr(\widetilde{Y}^1 = 2, Y^1 = 2 | V = 1) + \Pr(\widetilde{Y}^1 = 2, Y^1 \neq 2 | V = 1)}{\Pr(\widetilde{Y}^0 = 2, Y^0 = 2 | V = 1) + \Pr(\widetilde{Y}^0 = 2, Y^0 \neq 2 | V = 1)} \\
        &= \dfrac{\Pr(\widetilde{Y}^1 = 2, Y^1 = 2 | V = 1)}{\Pr(\widetilde{Y}^0 = 2, Y^0 = 2 | V = 1)} \\
        &= \dfrac{\Pr(\widetilde{Y}^1 = 2 | V = 1, Y^1 = 2)\Pr(Y^1 = 2 | V = 1) }{\Pr(\widetilde{Y}^0 = 2 | V = 1, Y^0 = 2) \Pr(Y^0 = 2 | V = 1)} \\
         &= \dfrac{\Pr(Y^1 = 2 | V = 1) }{\Pr(Y^0 = 2 | V = 1) }
    \end{align*}
    The first line applies the law of total probability, the second expands the sum, the third applies \ref{app_assg1}, the fourth factors the joint probability, and the last applies \ref{app_assg2}.
\end{proof}

\newpage

\section{Simulation details}\label{sec:moresim}
Here, we provide additional details on the data generation process and estimation methods for the simulation study described in the main text. We also present additional results. 

Our data generation process is a special case of the nonparametric structural equation model implied by the single world intervention graph (SWIG) in eFigure \ref{fig:simswig} representing the data as observed in the test negative design under a hypothetical intervention that sets $V$ to $v$. More specifically, we generate data according to  
\begin{align*}
    X, U &\sim \text{Unif}(0,1)\\
    V\mid X, U & \sim \text{Bernoulli}(\expit(\alpha_0 + \alpha_X X + \alpha_U U))\\
    I^v \mid V, X, U &\sim \text{Multinomial}(1-p_1(v, X, U) - p_2(v, X, U), p_1(v, X, U), p_2(v, X, U))\\
    T^v\mid I^v=i, V, X, U &\sim \text{Bernoulli}(\mathbbm 1(i>0)\exp\{(\tau_{1} + \tau_{1V}v) \mathbbm 1(i=1) + (\tau_{2} + \tau_{2V} v + \tau_{2U} U ) \mathbbm 1(i=2) \\&\qquad \qquad\qquad + \tau_X X + \tau_U U \})
\end{align*}
where $U$ is an unmeasured confounder and
\begin{align*}
    p_1(v, X, U) & = \Pr(I^v = 1 | V, X, U) = \exp(\beta_{10} + \beta_{1V}v + \beta_{1X}X + \beta_{1VX}vX + \beta_{1U}U) \\
    p_2(v, X, U) & = \Pr(I^v = 2 | V, X, U) = \exp(\beta_{20} + \beta_{2V}v + \beta_{2X}X + \beta_{2VX}vX + \beta_{2U}U).
\end{align*}
This implies the conditional independence $Y^v \indep V\mid X, U$ holds for $v=0,1$, meaning that conditioning on $U$ and $X$ is sufficient to control confounding. Under this setup, the causal risk ratio for medically-attended illness is 
\begin{align*}
    \Psi &= \dfrac{\Pr(Y^1=2 \mid V=1)}{\Pr(Y^0=2\mid V=1)}\\
    &= \dfrac{E\{\Pr(Y^1=2 \mid V=1, U, X)\mid V=1\}}{E\{\Pr(Y^0=2\mid V=1, U, X)\mid V=1\}}\\
   % &= \dfrac{E\{\exp(\beta_{20} + \beta_{2V} + \beta_{2X}X + \beta_{2VX}X + \beta_{2U}U + \tau_2 + \tau_{2V} + \tau_X X + \tau_U U + \tau_{2U} U)\mid V=1\}}{E\{\exp(\beta_{20} +  \beta_{2X}X  + \beta_{2U}U + \tau_2 + \tau_X X + \tau_U U + \tau_{2U} U)\mid V=1\}}\\
    &= \exp(\beta_{2V} + \tau_{2V}) \dfrac{E[\exp\{(\beta_{2X} + \beta_{2VX} + \tau_X) X + (\beta_{2U} + \tau_U + \tau_{2U}) U\}\mid V=1]}{E[\exp\{ (\beta_{2X} + \tau_X)X  + (\beta_{2U} + \tau_U + \tau_{2U}) U\}\mid V=1]}
    % &= \exp(\beta_{2V} + \tau_{2V}) \dfrac{E[\exp\{(\beta_{2X} + \beta_{2VX} + \tau_X) X\}\mid V=1]}{E[\exp\{ (\beta_{2X} + \tau_X)X\}\mid V=1]}.
\end{align*}
When there is no effect modification by X ($\beta_{2VX} = 0$), this reduces to 
\begin{align*}
    \Psi &= \exp(\beta_{2V} + \tau_{2V}),
\end{align*}
and when there is no no direct effect of vaccination on testing ($\tau_{2V} = 0$), this simplifies further to $\exp(\beta_{2V})$ which we note is the same as the effect on symptomatic infection. Finally, under TND sampling with $S=\mathbbm{1}(Y \neq 0)$, it can also be shown that
\begin{align*}
    \Pr[Y=2 \mid  S=1, V, X, U] &= \expit\{(\beta_{20}-\beta_{10}+\tau_2 - \tau_1) + (\beta_{2V} - \beta_{1V} + \tau_{2V} - \tau_{1V}) V\\ &\qquad\qquad+(\beta_{2X}-\beta_{1X})X+ (\beta_{2VX} - \beta_{1VX})VX + (\beta_{2U} - \beta_{1U} + \tau_{2U})U\},
\end{align*}
which follows a logistic regression model.

Violations of Assumption \ref{ass3} are controlled via $\beta_{2U}$, $\beta_{1U}$ and $\tau_{2U}$. Under our set up, we have:
\begin{align*}
    \dfrac{\Pr[Y^0=2 \mid V=1,X]}{\Pr[Y^0=2\mid V=0,X]}=\dfrac{\E[\exp\{(\tau_U  + \tau_{2U}+\beta_{2U})U\}\mid V=1, X]}{\E[\exp\{(\tau_U  + \tau_{2U}+\beta_{2U})U\}\mid V=0, X]},
\end{align*}
and 
\begin{align*}
    \dfrac{\Pr[Y^0=1 \mid V=1,X]}{\Pr[Y^0=1\mid V=0,X]}=\dfrac{\E[\exp\{(\tau_U  +\beta_{1U})U\}\mid V=1, X]}{\E[\exp\{(\tau_U +\beta_{1U})U\}\mid V=0, X]},
\end{align*}
implying that that Assumption \ref{ass3} holds when $\beta_{2U}=\beta_{1U}$ and $\tau_{2U}=0$ (Note that under these conditions the logistic model above similarly does not depend on $U$). Following the previous discussion, Assumption \ref{ass3} can be viewed as composed of two sub conditions (\ref{ass3a} and \ref{ass3b}): First, the unmeasured confounder exerts an equivalent effect on $I=1$ and $I=2$ (i.e. $\beta_{1U}=\beta_{2U}$), and second, there is no interaction between the unmeasured confounder and the source of illness for the probability of testing on the multiplicative scale (i.e. $\tau_{2U}=0$). 

Likewise, violations of Assumption \ref{ass2} are controlled by $\beta_{1V}$ and $\tau_{1V}$. When $\beta_{1V} \neq 0$ there is a direct effect of vaccination on test-negative illness $I = 1$ and when $\tau_{1V} \neq 0$ there is a direct effect of vaccination on testing among those with test-negative illness. When this occurs, we can at best identify the ratio of vaccine effects on the two illness outcomes, i.e. $\exp(\beta_{2V} + \tau_{2V})/\exp(\beta_{1V} + \tau_{1V})$, provided there is no effect modification by $X$ or $U$. As we show in Appendix \ref{sec:de_testing} and illustrate further via simulation below, under the special case of equal effects on testing, i.e. $\tau_{2V} = \tau_{1V}$, certain effects on symptomatic illness may still be recovered.



\begin{figure}[t]
    \centering
    \begin{tikzpicture}
        
        \tikzset{line width=1.25pt,
            swig vsplit={inner line width right = 0.5pt, line width right = 2pt},
            ell/.style={draw, inner sep=3pt,line width=1.25pt}}
        
        \node[name=V, shape=swig vsplit]{ \nodepart{left}{$V$} \nodepart{right}{$v$} };

        \node (Vname) at (0, 0.5-1.75) {Vaccination};
        
        
        \node[shape=ellipse,ell] (I) at (2.5,0) {$I^v$};
        \node (Iname) at (2.5,0.5-1.75) {Symptomatic};
        \node (Iname2) at (2.5,0.5-2.25) {Illness};
        
        
        \node[shape=ellipse,ell] (T) at (5,0) {$T^v$};
        \node (Tname) at (5,0.5-1.75) {Testing};
        
        
        
        \node[shape=ellipse,ell] (U) at (2.5,1.5+1) {$U,X$};
        %			\node (HSname) at (0,1.5+0.5+0.5) {Unmeasured};
        \node (Uname) at (2.5,1.5+0.5+0.75+1) {Measured \& unmeasured};
        \node (Uname2) at (2.5,1.5+0.75+1) {confounding};
        \draw[-stealth, line width = 1.25pt] (U) to (-0.2, 0.31);
            \draw[-stealth, line width=1.25pt, bend right](V) to (T);


        \foreach \from/\to in {V/I, I/T, U/I, U/T}
        \draw[-stealth, line width = 1.25pt] (\from) -- (\to);
        %%%NCs
    
                
    \end{tikzpicture}
    \caption{A Single-World Intervention Graph (SWIG) for causal relationship between variables of a test-negative design in the simulation.\label{fig:simswig}}
\end{figure} 

 We generate a target population of $N = 15,000$ resulting in TND samples of test-positive cases and test-negative controls of between $2000$ and $3000$. We consider nine scenarios: 
 \begin{enumerate}
    \item No unmeasured confounding.
    \item Unmeasured confounding, but assumptions \ref{ass1}-\ref{ass4} hold.
    \item Direct effect of vaccination on $I=1$.
    \item Equi-confounding is violated.
    \item Equi-selection is violated.
    \item Equal effects of vaccination on testing.
    \item Unequal effects of vaccination on testing.
    %\item Infections $I=1$ and $I=2$ are not mutually exclusive.
    \item All assumptions hold but there is effect modification by covariates $X$.
 \end{enumerate}
 Parameter values for all scenarios are shown in eTable \ref{tab:simparams}. For each scenario, we generate 1000 replicates and estimate $\Psi$ using the proposed estimators $\widehat{\Psi}_{om}^*$, $\widehat{\Psi}_{ipw}^*$, and $\widehat{\Psi}_{dr}^*$ as well as the conventional logistic regression estimator and calculate the bias and coverage of 95\% confidence intervals based on the true value. For comparison, we also estimate $\Psi$  assuming one had access to the full sample, as in cohort study, using the following estimators
 \begin{equation}\label{eqn:cohort_estimator}
    \widehat{\Psi}_{cohort} = \dfrac{\sum_{i=1}^N\widehat{\mu}_1(X_i)}{\sum_{i=1}^N\widehat{\mu}_0(X_i)}
 \end{equation}
 and 
 \begin{equation}\label{eqn:cohort_u_estimator}
    \widehat{\Psi}_{cohort,U} = \dfrac{\sum_{i=1}^N\widehat{\mu}_1(X_i, U_i)}{\sum_{i=1}^N\widehat{\mu}_0(X_i, U_i)}
 \end{equation}
 where $\mu_v(X) = \Pr(Y=2 | X, V=v)$ and $\mu_v(X,U) = \Pr(Y=2 | X, U, V=v)$
 The first estimator represents the typical case that the covariate $U$ is unmeasured, and the second represents the hypothetical case where $U$ is available, allowing for conditioning on a sufficient adjustment set. We consider both the typical case that the covariate $U$ is unmeasured, and the hypothetical case where $U$ is available, allowing for conditioning on a sufficient adjustment set. Finally, we include estimates from and $\widehat{\Psi}_{om}$ based on $\Psi_{om}$ from Proposition \ref{prop1} and discussed further in \ref{sec:app_estimation} that uses the full sample instead of restricting to those tested. Finally, in scenario 8, we demonstrate the double robustness property of $\widehat{\Psi}_{dr}^*$ by adjusting the data generation process for $V$ to be
 \begin{align*}
    V\mid X, U & \sim \text{Bernoulli}(\exp\{\alpha_0 + \alpha_X \mathbbm 1 (X > 0.5) + \alpha_U U\})
\end{align*}
when simulating under misspecification of the extended propensity score model and adjusting the process for $I^v$ and $T^v$ to be
\begin{align*}
    \Pr(I^v = 2 | V, X, U) = \exp(\beta_{20} + \beta_{2V}v + \beta_{2X}(X - 0.5)^2 + \beta_{2VX}vX + \beta_{2U}U). \\
    \Pr(T^v = 1 | I^v = 2, V, X, U) = \exp(\beta_{20} + \beta_{2V}v + \beta_{2X}(X - 0.5)^2 + \beta_{2VX}vX + \beta_{2U}U). \\
\end{align*}
 More details on naming convention and model specification for each estimation method are provided in eTable \ref{tab:methods}. For all estimators other than TND, we estimate 95\% confidence intervals using stacked estimating equations.

\begin{table}[p]
    \centering
    \caption{Parameter values for the data generation processes in each scenario for the simulation study. Parameter values that change across simulations are highlighted in \textbf{bold}.}\label{tab:simparams}
    \begin{tabular}{ccccccccc}
        \toprule
        & \multicolumn{8}{c}{Scenario} \\
        \cmidrule{2-9}
        Parameter & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8  \\
        \midrule
        $\alpha_0$ & -0.9 & -0.9 & -0.9 & -0.9 & -0.9 & -0.9 & -0.9 & -0.9 \\
        $\alpha_X$ & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 \\
        $\alpha_U$ & 0 & \textbf{2} & \textbf{2} & \textbf{2} & \textbf{2} & \textbf{2} & \textbf{2} & \textbf{2} \\
        $\beta_{10}$ & -2.1 & -2.1 & -2.1 & -2.1 & -2.1 & -2.1 & -2.1 &  -2.1 \\
        $\beta_{1V}$ & 0 & 0 & \textbf{0.1} & 0 & 0 & 0 & 0 & 0 \\
        $\beta_{1X}$ & -0.5 & -0.5 & -0.5 & -0.5 & -0.5 & -0.5 & -0.5 & -0.5 \\
        $\beta_{1VX}$ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        $\beta_{1U}$ & 1 & 1 & 1 & \textbf{0.25} & 1 & 1 & 1 & 1 \\
        $\beta_{20}$ & -2.4 & -2.4 & -2.4 & -2.4 & -2.4 & -2.4 & -2.4 &  -2.4 \\
        $\beta_{2V}$ & -1 & -1 & -1 & -1 & -1 & -1 & -1 &  \textbf{-0.25} \\
        $\beta_{2X}$ & -0.625 & -0.625 & -0.625 & -0.625 & -0.625 & -0.625 & -0.625 & -0.625 \\
        $\beta_{2VX}$ & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{-1.5} \\
        $\beta_{2U}$ & 1 & 1 & 1 & 1 & 1 & 1 & 1 &  1 \\
        $\tau_1$ & -1.1 & -1.1 & -1.1 & -1.1 & -1.1 & -1.1 & -1.1 &  -1.1 \\
        $\tau_2$ & -0.6 & -0.6 & -0.6 & -0.6 & -0.6 & -0.6 & -0.6 &  -0.6 \\
        $\tau_{1V}$ & 0 & 0 & 0 & 0 & 0 & \textbf{-0.25} & \textbf{-0.25} & 0 \\
        $\tau_{2V}$ & 0 & 0 & 0 & 0 & 0 & 0 & \textbf{-0.25} & 0 \\
        $\tau_X$ & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 \\
        $\tau_U$ & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 & 0.25 \\
        $\tau_{2U}$ & 0 & 0  & 0  & 0 & \textbf{-2}  & 0  & 0   & 0  \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[p]
    \centering
    \caption{Estimation methods for simulation study.}\label{tab:methods}
    \begin{tabular}{llp{3in}}
        \toprule
        Name & Estimator & Methods \\
        \midrule
        TND, logit & $\widehat{\Psi}_{om}^*(X)$ & conventional approach to TND based on coefficient from correctly-specified logistic regression of $Y^*$ on $(1, V, X)'$. \\
        \addlinespace[1em]
        TND, om & $\widehat{\Psi}^*_{om}$ & Proposed estimator in expression \ref{eqn:om_estimator}. Nuisance terms $\mu_v^*(X)$ estimated via logistic regression of $Y^*$ on $(1, V, X, VX)'$.  \\
        \addlinespace[1em]
        TND, ipw & $\widehat{\Psi}^*_{ipw}$ & Proposed estimator in expression \ref{eqn:ipw_estimator}. Nuisance term $\pi_0^*(X)$ estimated via logistic regression of $V$ on $(1, X)'$ among those with $Y^*=0$. \\
        \addlinespace[1em]
        TND, dr & $\widehat{\Psi}^*_{dr}$ & Proposed estimator in expression \ref{eqn:dr_estimator}. Nuisance terms $\mu_v^*(X)$ and $\pi_0^*(X)$ estimated as above. \\
        \addlinespace[1em]
        DiD & $\widehat{\Psi}_{om}$ &  Alternative estimator introduced in expression \ref{eqn:om_estimator_cohort}. Nuisance terms $\mu_v(X)$ estimated via logistic regression of $Y$ on $(1, V, X, VX)'$ in the full sample. \\
        \addlinespace[1em]
        cohort, U unmeasured & $\widehat{\Psi}_{cohort}$ & Standardization estimator in expression \ref{eqn:cohort_estimator} assuming $U$ is unavailable. Nuisance terms $\mu_v(X)$ estimated via logistic regression of $Y$ on $(1, V, X, VX)'$ in the full sample. \\ 
        \addlinespace[1em]
        cohort, U measured & $\widehat{\Psi}_{cohort,U}$ & Standardization estimator in expression \ref{eqn:cohort_u_estimator} assuming $U$ is available. Nuisance terms $\mu_v(X,U)$ estimated via logistic regression of $Y$ on $(1, V, X, VX, U)'$ in the full sample. \\ 
        \bottomrule
    \end{tabular}
\end{table}

\newcommand*{\TableHead}[1]{\multicolumn{1}{p{3em}}{\centering\hskip0pt#1}}
\newcommand*{\TableHeadd}[1]{\multicolumn{1}{p{4em}}{\centering\hskip0pt#1}}
\newcommand*{\TableHeaddd}[1]{\multicolumn{1}{p{7em}}{\centering\hskip0pt#1}}

\input{results/sims}

\begin{table}[p]
    \centering
    \caption{Robustness of estimators to misspecification of nuisance functions: 1000 simulations of sample size $n=$15,000 from modified processes based on scenario 8 in eTable 3. Bias is estimated mean bias, SE is estimated monte carlo standard error, Coverage is estimated coverage probability of 95\% CI}\label{tab:sims2}
    \begin{tabular}{lddddddd}
    \toprule
    Statistic & \TableHeadd{$\widehat{\Psi}_{om}^*(X)$}  & \TableHeadd{$\widehat{\Psi}_{om}^*$}  & \TableHeadd{$\widehat{\Psi}_{ipw}^*$}  & \TableHeadd{$\widehat{\Psi}_{dr}^*$} & \TableHeadd{$\widehat{\Psi}_{om}$} & \TableHeadd{$\widehat{\Psi}_{cohort,U}$} & \TableHeadd{$\widehat{\Psi}_{cohort}$} \\
    \midrule
    \addlinespace[0.3em]
\multicolumn{8}{l}{\textit{scenario 8: both correctly specified}}\\
\hspace{1em}Bias & -0.042 & -0.002 & -0.002 & -0.002 & -0.039 & -0.001 & 0.226\\
\hspace{1em}SE & 0.097 & 0.097 & 0.097 & 0.097 & 0.095 & 0.074 & 0.073\\
\hspace{1em}Coverage & 0.923 & 0.944 & 0.944 & 0.944 & 0.923 & 0.938 & 0.133\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textit{scenario 8: propensity score model misspecified}}\\
\hspace{1em}Bias & -0.037 & -0.001 & -0.012 & -0.001 & -0.038 & 0.000 & 0.215\\
\hspace{1em}SE & 0.102 & 0.101 & 0.101 & 0.101 & 0.098 & 0.076 & 0.075\\
\hspace{1em}Coverage & 0.941 & 0.954 & 0.954 & 0.954 & 0.942 & 0.953 & 0.198\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textit{scenario 8: outcome model misspecified}}\\
\hspace{1em}Bias & -0.062 & 0.022 & -0.004 & 0.004 & -0.023 & 0.028 & 0.256\\
\hspace{1em}SE & 0.093 & 0.093 & 0.095 & 0.094 & 0.090 & 0.071 & 0.069\\
\hspace{1em}Coverage & 0.911 & 0.949 & 0.957 & 0.958 & 0.944 & 0.942 & 0.052\\
\addlinespace[0.3em]
\multicolumn{8}{l}{\textit{scenario 8: both misspecified}}\\
\hspace{1em}Bias & -0.097 & 0.018 & -0.091 & -0.023 & -0.046 & 0.025 & 0.285\\
\hspace{1em}SE & 0.096 & 0.096 & 0.113 & 0.099 & 0.090 & 0.075 & 0.070\\
\hspace{1em}Coverage & 0.829 & 0.947 & 0.894 & 0.950 & 0.930 & 0.933 & 0.028\\
\bottomrule
\end{tabular}
\end{table}
\newpage
\clearpage
% $\varphi$
 \printbibliography[heading=subbibliography]
\end{refsection}
\end{appendix}

